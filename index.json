[{"content":"Watcher PTG Etherpad\nOverview The 2025.2 PTG was the Second PTG i have attended for watcher. The last PTG had one session to cover all the technical debt required to revive the project. This time we extended the ptg to 2 days.\nDay 1 Tech Debt Croniter The PTG started with a disscussion about technical debt that emerged during the epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule continuous audits based on cron strings. The Croniter libaray maintainer has decided to stop maintaining this library and it is transitioning to a new maintainer. While that gives watcher some breathing space, our usage is minimal, so we agreed to drop the dependency by leveraging the existing functionality of apscheduler.\nSupport status of datasources The second topic of the ptg focused on the support status of datasources and features of watcher in general.\nAs part of this discussion we touched on the idea of multiple levels of support:\nCore features (supported, tested in ci and ready for production use) Experimental features (supported but not tested in CI or may not be suitable for production use) Deprecated features (features that are no longer maintained and not suitable for production use) Unsupported features (features that are not supported by the project, but may work) During this session we agreed to deprecate the support for the following features:\nMonasca Grafana Noisy Neighbour Strategy (l3 cache) Monasca support was dropped because it is no longer maintained and is officially being retired.\nGrafana support was dropped because it is no longer maintained and has no existing ci coverage under our new support levels it could be considered experimental as it may work under certain conditions however since there is no ci and no active development it was agreed that we should drop support for this feature.\nThe Current Noisy Neighbour Strategy (l3 cache) was deprected because it depends on metrics that are nolnger avialable for several years. a dedicated session on how to replace it was held later in the ptg to discuss how to replace it going forward.\nFinally we wrapped up this topic by agreeing that we should create a support and testing matrix for watcher takeing inspiration from https://docs.openstack.org/cinder/latest/reference/support-matrix.html or https://docs.openstack.org/nova/latest/user/support-matrix.html The intent is to provide a way for users to see which features are supported, tested and suitable for production use cases.\nEventlet Removal The final technical debt topic was Eventlet removal. Watcher like many other OpenStack projects have been using eventlet since its inception, and it has become clear that eventlet is no longer sustainable. While no concrete proposal was made for this topic, to start evaluating the removal this cycle with some initial POCs and aim to remove eventlet in 2026.1\nWorkflow/API Improvements SKIPPED status for actions This discusstion focused on the introduction of a new Skipped status for actions. Today actions can be in one of the following states:\npending ongoing succeeded failed When constructing and executing an action plan it would be useful to extend the action states to introduced a skipped state, which would allow users to mark an action as skipped, preventing it from being executed by the engine. The idea is that this can be used to skip actions that are known to fail or be undesirable to execute. Additionally if an actions preconditions cannot be met, the action can be marked as skipped instead of failed.\nThis lead to a larger discussion about the meaning of SUCCEEDED and FAILED for action plans in general. https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108 We discussed that the current behavior where an action plan is considered succeeded when all actions are attempted regardless of the success of the action is unintuitive and not in line with the state machine docs.\nit was agree that if any action fails the overall action plan should be reported as failed. Additionally in this context we agreed that if the SKIPPED state is added it should be considered as Succeeded, not Failed.\nwatcher-dashboard This sessions was relatively short and focused on 2 areas testing and missing functionality\nTesting We opened the session discussing the fact that the horizon plugin as almost no testing. the net effect of this is every change need to be manually verified by the reviewer. We have two options with regard to testing we can build on django\u0026rsquo;s unittests integration to validate the core logic, or we could build out selenium testing. no general conclusion was arrived at but we resolved to follow up with the horizon core team in the future for guidance.\nExposing parameter in audits This was simple and non controversial, today when creating a audit its not possible to set the parameters filed via the horizon ui. we agreed this should be addresses as a speechless blueprint.\nSKIPPED status finally we discussed that if we extend the action status to model the new SKIPPED state we will need to enhance the action plan dashboard to allow skipping an action and to display the skipped state when a precondition fails.\nWatcher cluster model collector improvement ideas This was arguably another tech debt discussion in the guise of an operator pain point. For performance reasons watcher is build with a cached data model that periodically refreshes, combined with integration with notification for near real-time updates.\nDuring the session we reviewed https://bugs.launchpad.net/watcher/+bug/2104220 and resolved to investigate if we are properly consuming the notifications form nova. While we are consuming the notifications bug #2104220 asserts that we are perhaps using the wrong fields to update the instance host on live migration resulting in the source host being updated instead of the destination host.\nother mitigation were discussed such as adding the ability to force refresh the model when executing an audit, creating a new audit type to just run the refresh, or adjusting the collector interval.\nDay 1 Summary Day one was pretty packed but we resolved to finish early when we reached the end of the agenda.\nWe resolved to proceed with 2 new specs for the SKIPPED sate and model collector plugins and to treat the parameter enhancement to watcher-dashboard as a specless blueprint. croniter will be replaced as a bug fix and we need to review the impact of bug: #2104220\nDay 2 Watcher and Nova\u0026rsquo;s visible constraints Day 2 kicked off where day one ended on the topic of how watcher models nova instances and the visibility of scheduler constraints. In the epoxy cycle we added a number of attributes to the server show response that may be of interest to watcher, namely the scheduler hint and image properties. additionally i noted that in a prior release we added the pinned AZ as a separate filed. while extending the data model to provide this info would not be an api change we discussed that a spec could be nice to have to document the changes but a specless blueprint could also be valid.\nNoisy neighbour strategy This discussion happend as a sperate session on the second day but we touched on it on day 1 as part of the tech debt session. The TL;dr is since the exisitng noisy neighbour policy is non functional and since we want to aovid upgrade impacts a new SLA goal with 1 or multiple strateies shoudl be created. A spec to define the semantics should be created but at a high level the open questions are\nshould we have one stagey per metric or a single parmaterised stagey which metrics shoudl we use as the key performance indicators to comptue if the sla is treshold is exceeded cpu_steal io_wait cache_misses others? finally we agreed to deprecate the existing strategy this cycle for removal in 2026.2.\nHost Maintenance strategy new use case The last feature session discussed the limitation with the current host maintenance strategy the main limitations resolved around how to express if a instance should be stopped, cold migrated or live migrated. we agreed to continue with the spec https://review.opendev.org/c/openstack/watcher-specs/+/943873 and that it was ok to add new parameters to allow specifying which actions are allowed for the audit.\nThis topic was reprised in the nova room on day 4 TL;DR in the nova session we agreed to try to use server metadata to encode policies like live migratable. this can be incorporated into the spec design as a follow up if there is time. i.e. instance can be annotated by adding lifecycle:livemigratable=True|False and similar lifeccyle: metadata keys to express which actions may be taken by an external orchestrator like watcher.\nPTG wrappup contributor docs The second to last session was a quick discussion of some of the document that watcher is missing namely the scope of the project doc and the chronological release guide. these are both important document for use to create going forward, as is ensuring our existing contributor docs are up to date regarding when to file a bug vs blueprint vs spec.\nretro and adhoc planning To wrap up the ptg we had a short retro on how the last cycle went and any last minute topics. traditionally one start the PTG with the retro rather then finishes but its better late then never. we realised that one topic had not been raised which is the future of the core team we briefly discussed that going forward we will review the core team member ship at the start of each slurp release (i.e 2026.1) and remove inactive cores that have not participated in review, contribution or ptgs since the prior slurp. As such no removal form the core team will be made for 2025.2 although new addition may be made based on review activity over the cycle.\nWith that we we wrapped up the ptg with a reminder of our two cross project sessions with telemetry/horizon on day 3 and nova on day 4\nTelemetry/Horizon Context This ptg unlike many others i attended a combined telemetry/horizon cross project session lead by Victoria Martinez de la Cruz (vkmc). There we discussed a number of topics related to observability and how to provide tenant and admin static dashboards to visualise metrics.\nInitially we started by discussion why observability and metrics are distinct form the existing views provided by the admin hypervisors dashboard and how they are distinct form showback/chargeback provide by cloud-kitty\nshowback/change-back focus on billing and the ratings are base not on the utilisation of the provisioned resource but rather then allocation. i.e. you are billed equally for an idle vm that uses 4 cpus and 16G of ram vs a fully loaded vm. cloud kitty can help tell you how large your bill will be but it will not tell you if your application can run with half the resources. A observability view based on near real-time metrics aims to solve the latter problem.\nThe hypervisor dashboard is also related but distinct. the hypervisor view is intended to provide a semi real-time view of the capacity of a could for admin to be able to plan if they need more capacity. Again this view looks more at the available resources and the allocated resource but does not provide an overview of the utilisation.\nPOC An initial poc of what the dashboard could looks like is available here https://github.com/SeanMooney/grian-horizon-plugin proposal create a new horizon plugin as an official deliverable of the telemetry project in the governance repos this will be hosted in the https://opendev.org/openstack namespace and will be released as an official deliverable via the release repo we need to spike on d3.js vs rickshaw vs chart.js for charting and assess the version compatible and tech debt initially the new metrics dashboard will integrate with Prometheus as a metrics store. a fake backend may be created for testing, other backends are out of scope but could be added later. a combination of new panels and tabs in existing panels will be added. initially all dashboards will be static and direct quires to the backend will not be supported this will allow the plugin to provide multi tenancy as required in the absence of multi tenancy support in Prometheus. the plugin will be stateless with the time span that can be viewed determined by Prometheus. the design goal would be to support short term metrics of up to 7 days. the python-observability client will be used to query Prometheus. Nova TODO\n","permalink":"https://www.seanmooney.info/blog/2025.2-ptg/","summary":"\u003ch1 id=\"watcher\"\u003eWatcher\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4\"\u003ePTG Etherpad\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThe 2025.2 PTG was the Second PTG i have attended for watcher.\nThe last PTG had one session to cover all the technical debt required\nto revive the project. This time we extended the ptg to 2 days.\u003c/p\u003e\n\u003ch2 id=\"day-1\"\u003eDay 1\u003c/h2\u003e\n\u003ch3 id=\"tech-debt\"\u003eTech Debt\u003c/h3\u003e\n\u003ch4 id=\"croniter\"\u003e\u003ca href=\"https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L39\"\u003eCroniter\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eThe PTG started with a disscussion about technical debt that emerged during the\nepxoy cycle. This focused mainly on croniter, which is used by watcher to schedule\ncontinuous audits based on cron strings. The Croniter libaray maintainer has decided\nto stop maintaining this library and it is transitioning to a new maintainer.\nWhile that gives watcher some breathing space, our usage is minimal, so we agreed\nto drop the dependency by leveraging the existing functionality of apscheduler.\u003c/p\u003e","title":"2025.2 Ptg"},{"content":"What Is OpenStack? For those of you who have not heard of \u0026ldquo;OpenStack\u0026rdquo; or the OpenStack Foundation, you might be wondering what it is. Wiki defines it as an a free and open-source software platform for cloud computing, mostly deployed as an infrastructure-as-a-service (IaaS).\nWhile that is true, it is incomplete and unless you know what infrastructure as a service is it\u0026rsquo;s not very helpful.Granted the wiki page does actually explain what OpenStack and what OpenStack does but ill have a go anyway. So OpenStack as software, is a collection of composable interrelated projects primarily written in python, for creating private and public clouds environments.\nOpenStack provides an abstraction layer that allows a collection of servers to be managed as pools of compute, storage and networking services. compute, storage and networking are the 3 pillars of IT service that make up a standard IT infrastructure. As OpenStack provides a software interface to manage these 3 pillars of IT infrastructure it is an Infrastructure as a service offering.\nThe project that makes up OpenStack IaaS offering are what used to be know as the OpenStack core projects.\nCore Projects: Nova: Compute Project Neutron: Networking Project Keystone: Authentication Project Glance: Images as a service Project Cinder: Block Storage Project Swift: Object Storage Project Non-Core projects?: But if there is a Core then are there more non-core projects? For OpenStack that is a resounding YES!!! in fact since the creation of the big tent and arguably before, the compendium of OpenStack and OpenStack related projects has been expanding beyond IaaS to Orchestration, Platform as a service, advanced services such as container orchestration and support services such as workflow engines, database as a service, backup as a service and many other micro services that can be composed to build your own OpenStack-powered application.\nA listing of offical project teams can be found in the openstack governance repo.\nHow mature are all test projects? With all these core and non-core project to choose for do I need them all? How mature are they really? Are they all ready to use in my production application?\nWell, the answer to the first question is simple, OpenStack is a composition of micro services that work together to provide a could of your own design. if you don\u0026rsquo;t need object storage then you can deploy without swift, already have ceph deployed? no problem you can have cinder,nova and glance use that as your storage backend. do you want DNS as a service by not orchestration? you can deploy designate but leave heat out of your cloud deployment.\nThe second question of how mature these projects are is a little harder to answer. The OpenStack Core project have matured over many releases to be stable and production ready, the advanced services are at a differing level of maturity but luckily the OpenStack foundation has been developing a tool to help you make up your own mind.\nThe OpenStack Project Navigator is a great place to start and if you still have question the reaching out to the comunity is as easy as jumping on IRC or sending a mail to the mailing list.\nFuture Reading Openstack homepage Openstack software overview Openstack architecture 10,000 feet Intro to neutron Openstack Foundation youtube ","permalink":"https://www.seanmooney.info/blog/what-is-openstack/","summary":"\u003ch1 id=\"what-is-openstack\"\u003eWhat Is OpenStack?\u003c/h1\u003e\n\u003cp\u003eFor those of you who have not heard of \u0026ldquo;OpenStack\u0026rdquo; or the \u003ca href=\"https://www.openstack.org\"\u003eOpenStack Foundation\u003c/a\u003e,\nyou might be wondering what it is. Wiki defines it as an a free and open-source\nsoftware platform for cloud computing, mostly deployed as an\ninfrastructure-as-a-service (IaaS).\u003c/p\u003e\n\u003cp\u003eWhile that is true, it is incomplete and unless you know what infrastructure\nas a service is it\u0026rsquo;s not very helpful.Granted the wiki page does actually\nexplain what OpenStack and what OpenStack does but ill have a go anyway.\nSo OpenStack as software, is a collection of composable interrelated projects\nprimarily written in python, for creating private and public clouds environments.\u003c/p\u003e","title":"what is openstack"}]