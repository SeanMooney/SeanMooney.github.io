[{"content":"Three Months with AI: From Experiment to Daily Workflow Over the past three months, I\u0026rsquo;ve been intentionally exploring how AI can fit into my software development workflow. Here\u0026rsquo;s what I learned.\nBuilding with AI: The Prototype Experiment My initial foray into AI tooling was building a prototype to understand the fundamentals. I spent about 6 hours creating ca-bhfuil, a simple experiment in combining multiple AI components:\nLangChain and LangGraph as the agent framework Ollama for local LLM hosting sqlite-vec as a basic vector database for RAG experimentation The result was a very simple prototype using hardcoded data to demonstrate storing commit messages in a vector database, prompting an LLM to use RAG to retrieve relevant data, and generating responses. You can see an example response in the commit message.\nThis is far from being a useful tool, but it effectively demonstrated the basics of building an AI agent. The project is on hold for now, but I may return to the concept in the future.\nAI in Daily Development The IDE Evolution For several years, I\u0026rsquo;ve had AI available in my IDE through both VS Code and Emacs. Last quarter, I was using Continue with local models hosted on Ollama. In fact, that was how I mainly used AI for the last 12 months due to changes in my company\u0026rsquo;s AI policy. This quarter, I briefly experimented with the new Granite Code extension, but it was a regression from Continue for my workflow since I run Ollama remotely â€” a use case they don\u0026rsquo;t focus on.\nExploring AI Coding Agents In Q2, I took time to experiment with other AI coding agents, specifically RooCode, Aider, and Claude Code.\nRooCode: The Power User\u0026rsquo;s Dream RooCode is a powerful agentic open-source agent that extends VS Code. One of the AI YouTubers I follow released custom modes for Roo called Micromanager mode, which prompted me to explore using local models for routine AI code changes while reserving larger models for orchestration and complex tasks.\nThe idea was to have a local-first approach to AI tooling. Unfortunately, the hardware I currently have for running LLMs doesn\u0026rsquo;t quite have enough memory to work effectively with RooCode. Models advanced enough to handle large amounts of context and support tool calling, like the latest release of Devstral, are tuned to run on systems with 32GB of RAM. My M4 Mac Mini has 16GB, and quantized models that do fit proved unreliable or too slow to be effective.\nHowever, I did use RooCode with hosted models via OpenRouter and Google Cloud to develop a Nova specification, and I wrote a detailed blog post about that process, focusing on how I used AI to work around my dyslexia and produce higher-quality initial versions of specifications.\nAider and Gemini Deep Research: The Eventlet Challenge TL;DR: Used AI to research and debug OpenStack\u0026rsquo;s Python 3.13 compatibility issues, exploring everything from high-level analysis to experimental code solutions. While the technical solutions weren\u0026rsquo;t production-ready, AI proved valuable for rapid prototyping and problem exploration.\nOpenStack has a significant problem: it uses a concurrency framework called Eventlet (a Python library that enables cooperative multitasking) which is incompatible with Python 3.13, preventing OpenStack from running on the latest Python version. This became an excellent test case for AI-assisted debugging and research.\nUnderstanding the Problem I used Gemini\u0026rsquo;s deep research capabilities to explore the background and potential paths forward. After spending an hour or two working with AI to better understand how changes in Python\u0026rsquo;s threading model due to PEP 703 might cause the errors we\u0026rsquo;re observing, I produced a comprehensive analysis document.\nUnfortunately, the research made a strong argument for why it\u0026rsquo;s more important to move off Eventlet quickly rather than trying to extend it to newer Python versions. The conclusion was clear: this isn\u0026rsquo;t a problem we should solve by patching Eventlet.\nExploring Technical Solutions Despite the research conclusions, I wanted to explore potential workarounds. After 3 hours of trying to use AI to develop a simple reproducer, I used Gemini to brainstorm several different possible fixes in Nova, producing another technical analysis document.\nThe brainstorming session explored several approaches:\nArena Allocation Strategy: Originally I wanted to explore if we could take an arena allocation strategy (a memory management technique where objects are allocated in large blocks) to try and keep the eventlet stack frame alive, but since the underlying bug is with liveness tracking of the frame and the objects in them, that likely won\u0026rsquo;t work and it\u0026rsquo;s not easy to do in Python anyway.\nThread Local Storage: My next idea was to use thread local storage (data that\u0026rsquo;s unique to each thread) or a module level WeakKeyDict to store strong references to the HTTP clients, keyed off the eventlet greenthread (Eventlet\u0026rsquo;s lightweight thread equivalent). While clever, it\u0026rsquo;s also too clever and not something we should sprinkle over the code base.\nContext Manager Approach: In the end we arrived at possibly using a context manager to extend the lifecycle, so I iterated with Ollama and some local models to create a hold context manager and then refined it with more powerful models.\nThe Hold Context Manager Experiment The most interesting experiment was developing a context manager to extend object lifecycles:\nimport threading from typing import Any, Optional, TypeVar, Dict, Union, cast T = TypeVar(\u0026#39;T\u0026#39;) class Hold: \u0026#34;\u0026#34;\u0026#34; Context manager that holds an object in thread-local storage to prevent it from being collected. The context manager creates a namespace in thread-local storage keyed by the context manager\u0026#39;s identity (using its memory address) and stores the object there. This ensures the object remains accessible even if the garbage collector attempts to deallocate it during context switches in eventlet green threads. \u0026#34;\u0026#34;\u0026#34; def __init__(self, obj: T) -\u0026gt; None: self.obj = obj self._key = (id(self), hash(obj)) # Precompute key once def __enter__(self) -\u0026gt; T: threading_locals = threading.local() setattr(threading_locals, self._key, self.obj) return self.obj def __exit__(self, exc_type, exc_value, traceback) -\u0026gt; None: threading_locals = threading.local() try: delattr(threading_locals, self._key) except Exception: pass return None I have not proved to myself that this is a good approach either, so for now I\u0026rsquo;m capturing this experiment in this blog post for future reference.\nAI vs Human Solutions The most fascinating part of this exercise was comparing AI-generated solutions with human expertise. For one of the reported issues, we had a concrete reproducer, so I used Aider with OpenRouter to generate a possible solution.\nWhat was particularly interesting: both DeepSeek and a human developer (HervÃ©\u0026rsquo;s approach) independently identified the same problematic function. When I asked AI to generate a solution without mentioning HervÃ©\u0026rsquo;s work, it focused on the same code area.\nThe AI solution probably isn\u0026rsquo;t correct, but this convergence suggests AI can effectively identify problem areas, even when the proposed fixes need significant human refinement.\nCreating an AI Style Guide for OpenStack TL;DR: Developed a comprehensive style guide for using AI with OpenStack development, iterating through multiple AI tools to create both human-readable guidelines and tool-optimized versions.\nMoving from debugging to documentation, my next major AI experiment focused on creating reusable guidelines for AI assistance in open source development.\nAs I used AI more with open source and OpenStack in particular, I wanted to create an AI style guide to encode important information for AI to follow. One thing that became apparent quickly is that paid models are still much more powerful than local models, even though local models are rapidly catching up.\nRegardless of whether it\u0026rsquo;s hosted or local, open or closed, any usage of AI with open source has critical requirements that must be followed.\nI created the OpenStack AI Style Guide through an iterative process:\nInitial Draft: Used Aider with DeepSeek-R1 to summarize style guidance from the OpenStack hacking repo and its style checks.\nEnhancement: Asked Aider to add non-conflicting enhancements from Google\u0026rsquo;s latest Python style guide and PEP 8.\nPolicy Integration: Uploaded to Claude.ai and iterated with Claude Sonnet 4 to incorporate guidance from:\nOpenInfra AI Policy DCO replacement resolution OpenStack Conventions: Refined the style to better reflect how OpenStack commit messages are written.\nTool Optimization: Used Claude to distill the guide into a tool-focused version that minimizes context length and token usage alongside the comprehensive guide.\nAfter about 20 iterations with Claude and several earlier versions, I used Claude Code to create a repository that would be easy for AI tools to consume. The end result is the OpenStack AI Style Guide.\nBeyond Code: AI-Enhanced Development Environment TL;DR: Used AI to modernize and maintain my Emacs configuration, turning a procrastinated maintenance task into a streamlined, accessibility-focused development environment.\nWhile most of my AI experiments focused on coding and documentation, I also explored how AI could improve my daily development environment. This led to a significant overhaul of my Emacs configurationâ€”an exercise that perfectly demonstrated AI\u0026rsquo;s potential for maintenance tasks I\u0026rsquo;d been avoiding.\nThe Emacs Modernization Project For years I had put off creating a proper Emacs config, preferring minimal customization. But when VS Code started getting on my nerves, I decided to give building out a sophisticated Emacs environment a proper shot. Rather than using Doom or Spacemacs, I stripped everything back to basics and started a literate config from scratch.\nThe transformation was dramatic: AI helped me consolidate scattered settings into proper use-package blocks, fix critical startup errors, and embrace true literate programming principles. The result was nearly 300 lines of reduction while dramatically improving maintainability.\nKey AI-Assisted Improvements:\nProper use-package conventions for both third-party packages and built-in settings Accessibility features including dyslexia-friendly font presets with OpenDyslexic Distraction-free writing environments via olivetti Comprehensive spell-checking workflows with writegood-mode and codespell integration Direct Claude Code integration for AI-assisted configuration management This experiment highlighted an important insight: AI excels at maintenance tasks we tend to procrastinate on. The maintenance overhead that had kept me from building a custom environment became manageable with agentic coding tools like Claude Code and Aider.\nThe Accessibility Connection The Emacs modernization tied directly back to my earlier AI writing experiments. By integrating AI assistance directly into my editing environment, I created a seamless workflow for the kind of writing enhancement that proved so valuable for technical documentation.\nLessons Learned: Three Months of AI Integration After experimenting with AI across prototyping, debugging, documentation, and environment configuration, several clear patterns emerged about where AI genuinely helps versus where it falls short.\nAI Excels At: Structured writing and editing: Transforming rough technical concepts into polished documentation Research synthesis: Combining information from multiple sources into coherent analysis Iterative refinement: Improving drafts through focused feedback loops Accessibility: Helping work around learning disabilities like dyslexia Maintenance tasks: Handling tedious configuration and cleanup work that humans procrastinate on Pattern recognition: Identifying similar code structures or problem areas across different contexts AI Struggles With: Domain expertise: Deep institutional knowledge about OpenStack\u0026rsquo;s architecture and conventions Community context: Understanding the social and political aspects of open-source development Hardware limitations: Local models require significant resources for complex tasks Tool integration: Many AI coding tools aren\u0026rsquo;t optimized for remote or self-hosted setups Production readiness: Generated solutions often need significant human refinement for real-world use The Sweet Spot The most effective approach isn\u0026rsquo;t having AI replace human expertise, but using it as a collaborative partner. AI handles the mechanical aspects of writing and research while human expertise guides architectural decisions, technical trade-offs, and community dynamics.\nIt\u0026rsquo;s worth noting that AI won\u0026rsquo;t replace your writing entirely. Your initial draft and ideas remain essential. AI excels at enhancing the skeleton you create, making content more readable and coherent.\nLooking Forward This three-month experiment convinced me that AI can be a powerful tool for OpenStack development when used thoughtfully. Rather than replacing human expertise, AI augments it by helping experienced contributors work more efficiently and potentially helping newer contributors learn faster.\nUsing AI to learn is a double-edged sword. When I first used AI (Copilot) to help me work in Go, it made me feel productive in the new language environment much faster than traditional learning methods.\nHowever, that ease of onboarding comes at a cost. Without struggling through challenges, you won\u0026rsquo;t embed knowledge as deeply. I don\u0026rsquo;t recommend new programmers rely heavily on AI until they\u0026rsquo;ve built critical thinking skills and can reason about the effects of their changes. AI will confidently lead you down the wrong path, and as tools become more powerful, analyzing their suggestions becomes more important, not less.\nThe real success isn\u0026rsquo;t in any individual tool or technique, but in discovering how AI can lower barriers to high-quality technical contribution. My writing workflow isn\u0026rsquo;t perfect yet, but it\u0026rsquo;s significantly better than before and continues to evolve.\nThe future I\u0026rsquo;m excited about isn\u0026rsquo;t one where AI writes code for us. Instead, I envision AI helping us express our original intent more clearly and providing tools that enable us to write better code ourselves.\nIf you\u0026rsquo;re interested in experimenting with AI in your OpenStack work, check out the OpenStack AI Style Guide and feel free to reach out on IRC (sean-k-mooney) or through the usual OpenStack channels.\n","permalink":"https://www.seanmooney.info/blog/three-months-with-ai/","summary":"\u003ch1 id=\"three-months-with-ai-from-experiment-to-daily-workflow\"\u003eThree Months with AI: From Experiment to Daily Workflow\u003c/h1\u003e\n\u003cp\u003eOver the past three months, I\u0026rsquo;ve been intentionally exploring how AI can fit into my software development workflow. Here\u0026rsquo;s what I learned.\u003c/p\u003e\n\u003ch2 id=\"building-with-ai-the-prototype-experiment\"\u003eBuilding with AI: The Prototype Experiment\u003c/h2\u003e\n\u003cp\u003eMy initial foray into AI tooling was building a prototype to understand the fundamentals. I spent about 6 hours creating \u003ca href=\"https://github.com/SeanMooney/ca-bhfuil\"\u003eca-bhfuil\u003c/a\u003e, a simple experiment in combining multiple AI components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLangChain and LangGraph\u003c/strong\u003e as the agent framework\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOllama\u003c/strong\u003e for local LLM hosting\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esqlite-vec\u003c/strong\u003e as a basic vector database for RAG experimentation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe result was a very simple prototype using hardcoded data to demonstrate\nstoring commit messages in a vector database, prompting an LLM to use RAG\nto retrieve relevant data, and generating responses. You can see an \u003ca href=\"https://github.com/SeanMooney/ca-bhfuil/commit/c1df8b24a50098e6a27946d103d962349f40c2a2\"\u003eexample response in the commit message\u003c/a\u003e.\u003c/p\u003e","title":"Three Months with AI: Building, Coding, and Contributing"},{"content":"Using AI to Write OpenStack Nova Specifications: A Real-World Experiment. Background and Motivation After attending the 2025.2 PTG and seeing the extensive backlog of specs that need to be written, reviewed, and implemented, I decided to run an experiment: can AI meaningfully help with the OpenStack specification process? Not just the writing, but the actual design thinking and technical architecture work that goes into a good Nova spec.\nThe motivation was simple - we have more good ideas than we have time to properly document them. Between my day job, core review responsibilities, and the general time constraints we all face, turning a concept into a well-structured, technically sound specification can take days or weeks. What if AI could help accelerate that process?\nThe Personal Challenge As many of ye may know, i am severely severely dyslexic. While the English dialect i speak is technically Hiberno-English, or Irish English, calling the raw output of my writing that is a stretch. Core reviewers for years have been kind enough to help translate the sean-speak that leaves my fingertips back into English.\nOver the years i have developed several strategies to deal with this, but there have been times where creating a document with all the technical rigor required in something like a spec or blog post may take me 8-10 hours just for it to take me an additional 3-4 hours just to proof read and spell check it with multiple tools like Grammarly, my browser spell checker and editors spellcheck just for there to still be issues when a human reads it.\nSo when on a good day it takes me only half as long to correct a document as to write the base content and on a bad day it can literally take 2-3 times as long to do so, there is potentially a lot to be gained by using AI to aid in my writing.\nThe Broader Impact This isn\u0026rsquo;t just about me though - there are likely other contributors in the OpenStack community and beyond who face similar challenges. Learning disabilities, non-native English speakers, contributors who are brilliant engineers but struggle with written communication - AI assistance could be a game changer for inclusive participation in open source communities. If this experiment proves successful, it might open doors for more diverse voices to contribute to technical discussions that have traditionally required strong written English skills.\nThe Timing Well why now is a good question but with a pretty simple answer:\nLike many companies, Red Hat would like us all to explore AI and what we can use it for day to day. Until recently AI tools were auto-complete on steroids and kind of terrible at this, but in the last 3-6 months with release of LLMs like DeepSeek-R1 and Google Gemini Flash 2.5, coupled with tools like RooCode, the landscape has changed a lot. I was asked to review a technical solution proposal drafted by one of our very talented Solution Architects Greg Procunier. What better time than when you are provided a problem statement, use cases and even a potential approach to apply my knowledge of nova to how I would address the problem? After all, the best way to learn how to use a tool is to try and use it to address a real world problem. The Challenge: CPU Performance Tiers As is disappointingly common with nova the concept that Greg outlined in openstack-cgroup-tiering is something that we have discussed tangentially many times over the years. In fact in his technical plan Greg explains how a combination of old and very old openstack features can be combined to achieve the usecase without any code change.\nSo why a new spec? simple it should be possible for mere mortals to achieve the use case without over a decade of contributing to nova or cloud solution architecture experience to define cpu performance level in OpenStack!\nTechnical Background: The existing approach requires deep knowledge of Nova\u0026rsquo;s scheduler filters, Placement service resource classes, libvirt domain XML generation, cgroups configuration, and the interactions between all these components. Greg\u0026rsquo;s proposal aimed to make CPU performance tiers as simple as selecting a flavor.\nThe Experiment Setup TL;DR: Used RooCode with Google Gemini Flash 2.5 and DeepSeek-R1 to iteratively develop a comprehensive Nova specification, transforming AI\u0026rsquo;s initial rough attempt into a production-ready design through systematic refinement.\nThere is a standard in writing often used in U.S. military communication known as BLUF (Bottom line up front). Well I\u0026rsquo;m Irish so Better late than never, the intent of this blog is not to discuss The details of how to modify nova to support \u0026ldquo;CPU Performance Service Levels using CPU Shares and Placement Resource Classes\u0026rdquo;. Instead the goal is to discuss the experience and process of using ai to write the spec. By necessity some of the details of the spec will also be addressed here but as a framework for the conversation rather than as the focus.\nThe Failed First Attempts This is actually my 3rd attempt as initially i tried to use Gemini pro and the web chat interface to do this followed by using google docs gemini integration and both failed spectacularly.\nThe web chat interface couldn\u0026rsquo;t maintain context across the multiple reference documents I needed - it would forget about the nova-specs template when I was asking about Greg\u0026rsquo;s technical plan, or lose track of OpenStack Placement concepts when discussing scheduler changes. Google Docs integration was even worse - it kept trying to \u0026ldquo;help\u0026rdquo; by reformatting my RST syntax and couldn\u0026rsquo;t handle the complexity of cross-referencing multiple technical documents.\nFinding the Right Tool: RooCode So for this experiment i wanted to provide a more powerful agentic coding environment where i could group reference content in folders locally for LLMs to use.\nTo do this i selected RooCode, an agentic AI coding environment that allows you to work with AI models in a more interactive way than simple chat interfaces. Unlike basic AI assistants, RooCode can maintain context across multiple files, execute code, and work with local repositories. It provides different \u0026ldquo;modes\u0026rdquo; - essentially different system prompts and capabilities - for different types of work.\nThe week before i had been playing with Adam Larson\u0026rsquo;s MicroManager mode. That\u0026rsquo;s a topic for a different blog but most of my interaction were done in the Senior Engineer mode with some usage of roo\u0026rsquo;s default Architect mode. The key advantage was being able to provide the AI with access to the entire nova-specs repository, placement documentation, and Greg\u0026rsquo;s reference material simultaneously.\nFor this blog you can effectively assume that the delta between RooCodes default system prompt and my setup was\nYou are my expert programmer named Roo Sr. You are an expert programmer, that is free to implement functionality across multiple files. You take general guidelines about what needs to be done, and solve the toughest problems. You will look at the context around the problem to see the bigger picture of the problem you are working on, even if this means reading multiple files to identify the breadth of the problem before coding. I also created a custom prompt enhancer prompt to improve my initial requests:\nYou are Promptly, an AI assistant specializing in refining prompts for LLMs. If any aspect of the task is unclear, ask clarifying questions before proceeding. # Output Format - reply with only the enhanced prompt, no conversation, explanations, lead-in, bullet points, placeholders, or surrounding quotes - Keep content concise and easy to read # Context/Inputs - You are an LLM assisting in crafting prompts for advanced AI systems - Enhance clarity and precision of user-submitted prompts - Balance brevity with accuracy (avoid excessive token usage while maintaining precision) # Constraints/Guidelines - Correct obvious typos/spelling errors - Use plain language without technical jargon - Do not provide solutions to the original prompt - Do not include instructions for handling ambiguous input or edge cases - It\u0026#39;s very important that you focus on the question the user has. When using tools, always pass required parameters - Think about your responce carefully, use the sequentialthinking tool to help reason about the prompt you are enhancing. # Priorities 1. **Accuracy** 2. **Efficiency** 3. **Clarity** Generate an enhanced version of this prompt: Is this a good prompt enhancer prompt? Probably not, but it worked well enough with my semi-local LLMs that I decided to use it for this experiment.\nModel Selection: Local vs. Hosted This is as good a time as any to talk about models. Originally I wanted to try and do this using models that I ran locally on a 16GB M4 Mac Mini with Ollama. I tried a number of different models to do this but RooCode\u0026rsquo;s system prompt uses too much context to work with models that fit on this hardware. So while I did use Gemma 3:12B-IT-QAT as the local model for the prompt enhancer, I selected Google Gemini Flash 2.5 and DeepSeek-R1:free.\nWhy deepseek? DeepSeek-R1 is specifically designed for reasoning tasks and includes a \u0026ldquo;thinking\u0026rdquo; process where you can see the model\u0026rsquo;s internal reasoning before it responds. For complex technical architecture decisions like designing a Nova scheduler weigher, having a model that can work through the problem step-by-step seemed valuable. The reasoning traces also help you understand why the AI made certain design choices.\nWhy gemini flash? Beyond being very fast and Google\u0026rsquo;s $300 free credits, Gemini Flash 2.5 is particularly good at following complex instructions and working with structured documents like RST files. It\u0026rsquo;s also more reliable at tool calling and file manipulation than many other models, which was crucial for actually editing the specification document. Originally i wanted to have the Architecture mode use the long thinking deepseek and programmer modes use gemini flash to apply the changes to the spec file.\nData Organization What about data management? Well, it may be old-fashioned but just use Git. In this case I created an empty Git repo and added my reference material as Git submodules.\nThe Test Case So what was I actually trying to build? The core idea was to let users pick CPU performance tiers - think Gold, Silver, Bronze - where each tier gets guaranteed proportional CPU time through Linux cgroups and the CFS scheduler. All integrated with OpenStack Placement so the scheduler actually knows what\u0026rsquo;s available.\nThis seemed like a perfect test case for AI assistance because:\nIt\u0026rsquo;s complex enough to be interesting but not so complex it would take months It touches multiple OpenStack services (Nova, Placement, potentially Watcher) You need to understand both OpenStack\u0026rsquo;s scheduler architecture and Linux kernel features There are clear use cases but the implementation needs careful design work Basically, it\u0026rsquo;s the kind of thing that sounds simple when you describe it to someone but turns into a 700-line specification when you actually think through all the details.\nPre-AI Design Process: The Human Foundation Before diving into AI assistance, I want to emphasize that this wasn\u0026rsquo;t a case of asking AI to design something from scratch. The human expertise and preparation phase was crucial.\nNormally I gather use case and requirement before sitting down to design something like this and ruminate on it for a bit before writing anything down.\nThis time was only slightly different as Greg had done much of that in writing up their solution plan. As I had a personal engagement taking my mother to the doctor for a checkup i took the, unusual for me, step of printing out a dead tree edition of Greg\u0026rsquo;s document and marked it up with a Sharpe as i waited.\nThe reason i bring this up is to make it clear that before i started trying to use ai to design a solution, i read the relevant reference material in multiple media(online and physically), i reflected on the content assessing how it i would use my nova knowledge to refactor the solution to be more applicable upstream, and just left the ideas percolate in my subconscious for about a week to 10 days before involving AI.\nThat mean that when i did start writing i had a direction and a sense for how i wanted to use the reference material to express my intent.\nThe AI-Assisted Process TL;DR: Evolved from AI\u0026rsquo;s basic initial attempt to a comprehensive specification through seven iterations, using targeted prompts to address specific technical gaps and architectural requirements.\nGetting Started: Specification Structure and the Path to v0 One of the most time-consuming parts of writing a Nova spec is getting the structure right and ensuring you\u0026rsquo;ve covered all the necessary sections.\nContext for Non-OpenStack Readers: Nova is OpenStack\u0026rsquo;s compute service that manages virtual machines across thousands of physical servers. Nova specs are comprehensive technical design documents (similar to RFCs or ADRs in other projects) that must be written and approved before any significant new feature can be implemented. They typically run 300-800 lines and require deep understanding of distributed systems, virtualization, and OpenStack\u0026rsquo;s complex service interactions.\nThe 3 most important sections of a Nova spec are as follows\nProblem Description: This is the what, expressing what you want to build\nUse Cases: This is the why, the motivation for why this problem is important\nProposed Changes: This is the How, Breaking down the implementation into logical components\nTechnical Context: Nova specs must also address complex concerns like database migrations, API versioning, upgrade compatibility across thousands of servers, integration with other OpenStack services (Placement, Neutron, Cinder), hypervisor-specific implementations, and detailed testing strategies.\nGetting this right without deep expertise is not something that anyone does correctly the first time. This is the foundation of the spec, so that\u0026rsquo;s where I started.\nMy initial prompt where I first used AI in this process was:\ni would like to create a new nova-spec to describe how to support cpu teiring. some refernce matilal is aviable in @/reference/ and a local copy of the nova-specs git repo is cloned at @/nova-specs . the spec template for the current release is @/nova-specs/specs/2025.2-template.rst : i would like to analsys the refence mataila and using the existing spec as as example create a new spec to describe how to implemente \u0026#34;cpu performance service level using cpu shares\u0026#34; But why write a prompt by hand when AI can improve it?\nDevelop a Nova specification document detailing the implementation of CPU performance service levels using CPU shares. Utilize the existing Nova specification template located at `/nova-specs/specs/2025.2-template.rst` as a guide. Analyze the reference material available in `/reference/` and leverage the template structure and style. The specification should comprehensively describe the design, implementation details, operational considerations, and testing procedures for a system that allows users to request and guarantee CPU performance tiers based on CPU share allocation. Specifically address how CPU shares will be managed, how performance guarantees are enforced, how users interact with the system (API, UI), and how the system integrates with existing Nova components. Include diagrams and examples to illustrate key concepts and workflows. The target audience is Nova developers and operators. This shows what RooCode\u0026rsquo;s built-in enhancer can do, but I also wrote my own. Can it do better?\nCreate a Nova specification document detailing CPU performance service levels using CPU shares, for Nova developers and operators. Use `/nova-specs/specs/2025.2-template.rst` as the template and `/reference/` for style and structure. The specification should comprehensively cover design, implementation, operational considerations, and testing for a system enabling users to request and guarantee CPU performance tiers via CPU share allocation. **Focus areas:** * **CPU Share Management:** Detail allocation, tracking, and adjustment mechanisms. Address conflicts and scaling. * **Performance Guarantee Enforcement:** Describe how requested performance levels are consistently met, considering hypervisor limitations, oversubscription, and workload characteristics. Include mitigation strategies for performance degradation. * **User Interaction:** Define RESTful API and UI interfaces for requesting, monitoring, and managing CPU share allocations. Provide example API requests and responses. * **Nova Integration:** Explain integration with existing Nova components (scheduler, compute, API, etc.). Identify impacts and required modifications. * **Oversubscription Handling:** Detail the strategy for managing oversubscription, including share prioritization and performance impact under high load. * **Testing Procedures:** Outline unit, integration, and performance tests to validate functionality and performance. * **Diagrams and Examples:** Include diagrams illustrating key concepts (e.g., share allocation workflow, performance guarantee enforcement) and practical examples demonstrating API usage and configuration. Adhere to the template\u0026#39;s structure and style, providing a detailed and actionable guide. Consider future extensibility and integration with other performance management features. That\u0026rsquo;s much better.\nI should note that there are some aspects of this prompt that will steer the AI in slightly the wrong direction, like \u0026ldquo;monitoring\u0026rdquo; and \u0026ldquo;UI interfaces\u0026rdquo;, but I wanted to have something concrete to start iterating on.\nV0: AI\u0026rsquo;s First Attempt - A Reasonable Start The AI\u0026rsquo;s initial specification was a reasonable starting point that captured the basic concept and use cases. You can view the complete v0 specification here:\nðŸ“„ View AI\u0026rsquo;s Initial Specification (v0)\nKey characteristics of the v0 draft:\nBasic problem statement and use cases Proposed direct API parameters for user control Simple cgroups enforcement approach Database schema changes (new cpu_shares column) High-level implementation concepts with placeholders Was this a good spec? Well no, it didn\u0026rsquo;t properly follow the spec format, but more importantly it missed a lot of the nuance that makes a Nova spec actually implementable.\nIt did capture the problem statement and use-cases reasonably well, but it made some fundamental mistakes. For example, it suggested adding new API parameters that users could set directly - which sounds reasonable until you realize that Nova doesn\u0026rsquo;t work that way. Users interact with Nova through flavors and images, not by setting arbitrary parameters on server creation. It also proposed a new database column without considering upgrade impacts, and didn\u0026rsquo;t understand how Nova\u0026rsquo;s scheduler actually works with Placement.\nA good Nova spec needs to show understanding of existing patterns, explain how the new feature integrates with current workflows, and demonstrate that the author understands the operational complexity of deploying changes across thousands of compute nodes. The v0 draft read like someone who understood the general concept but had never actually worked with Nova\u0026rsquo;s codebase.\nBut was any of that a show stopper? No. It was actually a pretty good starting point to build from - the core technical concept was sound, and having something concrete to iterate on is better than staring at a blank page.\nThe Transformation Process: From v0 to v7 Before diving into the final result, let me explain the refinement process that transformed AI\u0026rsquo;s basic first attempt into a comprehensive, production-ready specification.\nSetting Up the Collaboration First an overview of how i used Senior engineer mode.\nOne thing about AI is they are good at role-play, so first I set up a new chat with the following prompt, clearing all previous context.\nAs Nova project maintainer, i want to revise this design proposal to align with Nova\u0026#39;s goals and architecture. Provide insights on its technical accuracy, clarity, and completeness, to ensure the proposed change, fulfils the use cases and addresses the problem descriptions. I will iteratively provide modifications; collaborate to apply them. I will provide additional documentation describing Nova and the proposal as context. The design document is in reStructuredText format and we wil produce a new revision that will be stored in a new file. initally do not take any action until i ask you do, just acknowledge this prompt and wait for my request. This at least I hoped would put the AI in the right \u0026ldquo;mindset\u0026rdquo; to help me correct the issues with the v0 draft.\nBefore I go over the main prompt I used to revise the spec, I\u0026rsquo;ll also note that I did manually edit the spec in between prompts to address issues that were quicker to fix by hand and also to write some additional prose.\nTo make revisions to the spec, I first created a prompt which I iterated on once or twice with the prompt enhancer. I saved the major prompts to a file for later reference, but I also wrote smaller prompts to make minor tweaks between each revision. For example, when I manually added content I ran:\nReview the document at @/nova-specs/specs/cpu-performance-service-level.rst for technical accuracy, spelling, grammar, and consistency between the problem statement, use cases, and proposed solution. The final output is as follows, but I don\u0026rsquo;t expect you to read it in detail. I\u0026rsquo;ve included it to show how the spec evolved from AI\u0026rsquo;s first attempt to what I considered acceptable for a first draft to submit for review.\nV7: The Final Implementation-Ready Specification After seven iterations of refinement, the specification evolved into a comprehensive, production-ready design. You can view the complete final specification here:\nðŸ“„ View Final Specification (v7)\nKey improvements in the v7 specification:\nComprehensive technical definitions and algorithmic descriptions Placement service integration with standardized VCPU_SHARES resource class Detailed ResourceProviderWeigher with complete pseudocode implementation Flavor-driven configuration (no API changes required) Production deployment considerations and upgrade compatibility Mathematical formulations with practical examples Summary of v0 -\u0026gt; v7 Evolution TL;DR: Transformed from a basic API-centric approach with database changes to a sophisticated Placement-integrated solution using flavors, standardized resource classes, and a custom scheduler weigher.\nFor those who don\u0026rsquo;t want to read the full spec, here is an AI-generated summary of what changed.\nThe specification evolved significantly through seven iterations, transforming from a basic concept to a comprehensive technical design.\nTitle and Scope Changes v0: \u0026ldquo;CPU Performance Service Levels using CPU Shares\u0026rdquo; v7: \u0026ldquo;CPU Performance Service Levels using CPU Shares and Placement Resource Classes\u0026rdquo;\nThe scope expanded to include full OpenStack Placement service integration.\nMajor Technical Changes Resource Management Approach:\nv0: Direct API parameters (cpu_shares) with basic cgroups enforcement v7: Flavor-driven approach using quota:cpu_shares_multiplier with VCPU_SHARES resource class and Placement integration Scheduler Integration:\nv0: Vague mention of \u0026ldquo;a new scheduler filter or weigher might be required\u0026rdquo; v7: Detailed ResourceProviderWeigher specification with complete algorithm, pseudocode, and \u0026ldquo;most boring host\u0026rdquo; selection strategy User Interface:\nv0: New API parameters for direct user control v7: No API changes - flavor-based configuration with administrative controls New Sections Added The final version includes several sections absent from v0:\nDefinitions: Comprehensive technical glossary ResourceProviderWeigher: Complete algorithm specification with pseudocode Configuration Examples: Practical deployment guidance Detailed Diagrams: ASCII art showing resource contention scenarios Comprehensive Alternatives: Analysis of rejected approaches with reasoning Implementation Details: Specific work items and dependencies Technical Depth Improvements v0 Specification:\nBasic problem statement and use cases High-level implementation concepts Placeholders for key sections Simple API-centric approach v7 Specification:\nDetailed technical definitions and algorithms Complete configuration management strategy Comprehensive upgrade compatibility mechanisms Production-ready implementation plan Mathematical formulations and example calculations Architectural Sophistication The final version demonstrates significantly more architectural sophistication:\nMulti-layered enforcement (Placement + CFS scheduler) Integration with existing OpenStack patterns (flavor validation framework) Configuration-driven feature activation for upgrade compatibility Resource provider tree processing algorithms Sophisticated weighing mechanisms for intelligent host selection The evolution represents a transformation from conceptual proposal to implementation-ready specification with comprehensive technical detail and production deployment considerations.\nSean: ^ While not wrong, AIs can be a bit of a suck-up.\nThe Iterative Refinement Process Right, so back to the actual work - what prompts did I use to get from the AI\u0026rsquo;s first rough attempt to something that didn\u0026rsquo;t make me cringe when I read it?\nI started with something relatively simple, because if there\u0026rsquo;s one thing I\u0026rsquo;ve learned about working with AI, it\u0026rsquo;s that you don\u0026rsquo;t dump your entire design vision on it at once.\nthe first change i woudl like to make is summerise the current proposal as a short paragaph in the Alternatives seation with a title of \u0026#34;cpu service level via custom resources classes\u0026#34; after that we will work togehter to rework the spec to use a single standard resouces class called \u0026#34;VCPU_SHARES\u0026#34; In other words, I started by summarizing the initial spec as an alternative and then made the first big change: using a single resource class instead of one per tier.\nThis was one of the core simplifications needed to make this feature easy to use.\nNext, I got a little more ambitious and the next improvement I identified was to add automatic reporting instead of using provider.yaml files.\nSummarize the currnt proposal to report available inventories in the Alternatives section using the `provider.yaml` file, titled \u0026#34;Reporting VCPU_SHARE via provider.yaml.\u0026#34; Then update the proposal to state that the libvirt driver will automatically report a `VCPU_SHARES` inventory on the root resource provider for each hypervisor. The allowed range for CPU shares in the cgroups v2 API is [1, 10000]. A new `vcpu_share_multiplier` configuration option will be added, with a minimum value of 1 and a maximum value of 10000, the default will be 1000. Reporting of `VCPU_SHARES` will be controlled by a `report_vcpu_shares` boolean option in the libvirt section of the `nova.conf`. When enabled, the libvirt driver will report a quantity of `vcpu_share_multiplier * count(cpu_share_set)` as `VCPU_SHARES`. So at this point, the spec described the problem and use cases, introduced a new standard resource class, and described updating the libvirt driver to report that automatically.\nAdding Scheduler Intelligence Next step: scheduling.\nDocument the design and functionality of the `ResourceProviderWeigher` in @/nova-specs/specs/cpu-performance-service-level.rst. The `ResourceProviderWeigher` calculates a weight by comparing instance resource and trait requests to placement provider summaries. Enhance the `HostState` object in @/nova/nova/scheduler/host_manager.py with the provider summary dictionary. This dictionary is obtained from the placement API (see @/placement/api-ref/source/allocation_candidates.inc and @/placement/api-ref/source/samples/allocation_candidates/ for examples) when Nova requests allocation candidates. The weighting algorithm should adhere to these guidelines: 1. **Input:** The algorithm takes the `provider_summaries` data structure, which represents a forest of trees encoded as a map. Node relationships are defined by `parent_provider_uuid` (with the root having `null`) and `root_provider_uuid`. Resource providers are modeled as a mapping of UUIDs to resource class inventories (capacity and usage). 2. **Resource Aggregation:** Transform the tree into a map of resource class to free capacity (capacity - used), aggregating capacity and usage for identical resource classes across multiple providers within the tree. 3. **Trait Aggregation:** Create a flattened set of all traits reported by resource providers in a given tree, indexed by the root provider UUID. 4. **Weight Calculation:** Based on requested resources and traits, produce a weight between 0 and 1. 5. **Weighting Formula:** For each available resource, calculate the arithmetic mean of `(1 - (requested amount of resource class) / (free capacity of resource class))` across all resource classes in the provider tree. Add to this the count of requested traits divided by available traits. Normalize the result by dividing by 2. Reference existing Nova weighers (e.g., @/nova/nova/scheduler/weights/) for implementation guidance, especially the `MetricsWeigher` and `ImagePropertiesWeigher` for configuration options. The `ResourceProviderWeigher` should support the following configurable options: * `ResourceProviderWeigher` multiplier * List of resource classes to use for weighing (default: all) * List of traits to include when weighing (default: all) This prompt represented a significant step up in complexity. The main elements of note are that i provided it the paths to documentation for the placement allocation_candidates api and the api samples, named the relevant Classes in the nova scheduler HostState, and provided the host_manager.py as context.\nWith that context i described how i wanted the ResourceProviderWeigher to work.\nThis got pretty close but needed several refinements to actually express my intent. This and the prior prompts were the largest changes to the spec, and it was here that I took the time to make most of my manual changes.\nFinal Polish and Technical Context Form this point my prompts were more polishing rather then large changes.\nReview @/nova-specs/specs/cpu-performance-service-level.rst for consistency in style, information presentation, and section content, using @/nova-specs/specs/train/implemented/cpu-resources.rst as a detailed example. The CPU resources spec is one of our most complex specs and as a result one of the better written.\ncan you add a short introduction to @/nova-specs/specs/cpu-performance-service-level.rst describing how the linux CFS scheduler works in the context fo cpu_shares and how vms VCPU kvm threads are schdule to host cores and how time is allcoated to each process. Results and Evaluation TL;DR: AI didn\u0026rsquo;t make the process faster, but significantly improved quality by helping structure complex technical concepts and eliminating spelling/grammar issues before human review.\nThe Punch Line Did Ai make faster or easier to write the spec\u0026hellip; No not with this setup but it did result in a higher quality spec with most if not all of the spelling issues address before humans other then my self had to read them.\nThe time spent setting up and debugging this setup especially the time spent trying to use local models with the limited ram availability negated any speed up that could otherwise have been achieved.\nIf or when I do this again, I think I will change the agentic agent choice to either use Aider or Claude Code. RooCode could also be very effective if I choose a more powerful LLM that better supports RooCode\u0026rsquo;s complex system prompt and tool calling conventions.\nThe Final Result TL;DR: Produced a 700-line, production-ready Nova specification covering problem definition, technical architecture, implementation plan, testing strategy, and alternatives analysis.\nAfter about a week of iterative work, spread out as an hour or two in the evening we produced a comprehensive specification that includes:\nComplete problem definition with concrete use cases Detailed technical architecture using Placement resource classes Implementation plan broken down by component (scheduler, libvirt driver, API) Configuration options with sensible defaults Testing strategy including functional and integration tests Documentation requirements for operators and users Alternative approaches and why they were rejected The specification ended up being about 700 lines and a little over 3500 words and covers all the sections you\u0026rsquo;d expect in a Nova spec.\nWhat Worked Well AI Strengths in Technical Documentation Design Iteration and Refinement Iteration works well when working with LLMs and AI agents. With enough context and prompting you can get it to do some impressive refactoring but it still need your guidance. If you provide it with high quality examples it can provide high quality output.\nTechnical Writing and Clarity The AI was great at helping structure complex technical concepts in a way that would be accessible to reviewers who might not be deeply familiar with cgroups or the placement service internals.\nAn example of this is the ascii diagram of the vm vcpu to host cpus mappings.\nIn the past I have drawn them using https://asciiflow.com/#/ which is great but I think Gemini did a great job. Initially I used DeepSeek but Gemini I think captured it better.\nWhat Was Challenging AI Limitations in Complex Domains Domain Expertise Limitations Here\u0026rsquo;s the thing - while the AI knows a lot about OpenStack in general, it doesn\u0026rsquo;t have the kind of deep institutional knowledge you get from years of actually working on Nova. I found myself constantly having to correct assumptions about how features actually work, or provide context about why we made certain design decisions years ago.\nFor example, the AI initially suggested approaches that would have completely broken CPU pinning, or didn\u0026rsquo;t account for the fact that the libvirt driver has some\u0026hellip; let\u0026rsquo;s call them \u0026ldquo;quirks\u0026rdquo; in how it handles certain configurations.\nCommunity and Process Knowledge The AI doesn\u0026rsquo;t understand the social aspects of the OpenStack development process - what kinds of changes are likely to be controversial, which core reviewers have strong opinions about certain areas, or how to phrase things to avoid bikeshedding in reviews.\nIt also doesn\u0026rsquo;t know about informal agreements or \u0026ldquo;soft\u0026rdquo; rules that aren\u0026rsquo;t documented anywhere but are understood by the community. You know, the kind of things you learn from being in IRC discussions at 2am when someone\u0026rsquo;s trying to debug a weird corner case.\nUnderstanding Upgrade Complexity Upgrade impact is very important to nova. we avoid designs that require db changes or api changes when they are not strictly required. That does not mean we do not add new apis or db tables but we treat such changes as a high bar.\nIn Nova specifically, database schema changes require all the nova controller services (API, scheduler, conductor) that have access to the database to be upgraded simultaneously. This makes all the compute agents and the workloads that reside on them unmanageable for the duration of the controller upgrades. API changes have backward compatibility implications that can affect users for years, and must be supported across multiple OpenStack releases.\nThe original approach the AI suggested had a new db column to store the cpu_shares and allowed end users to request them directly. While neither of these are technically required to achieve the goal, the AI lacked the context to know that those are expensive things to do in something like nova, whereas db changes and api changes may be relatively cheap in a typical web application.\nTechnical Tool Limitations TL;DR: RestructuredText\u0026rsquo;s use of dashes conflicted with git diff syntax, causing AI tool confusion and edit failures when making cross-section changes.\nRestructured Text Format Confusion Here\u0026rsquo;s something that caught me completely off guard, though it\u0026rsquo;s obvious in hindsight.\nRestructuredText uses ------- everywhere - literally under every heading. You know what else uses -------? Git diffs.\nSo when I was working within a single section, everything was fine. But as soon as I tried to make changes across larger parts of the spec, the AI would get confused by all the dashes and start treating section dividers as diff markers. Tool call failures everywhere, and poor Roo couldn\u0026rsquo;t figure out what I actually wanted it to change.\nThis is partly down to the RST format but more a limitation of both Roo and the models I selected: google-gemini-flash-2.5 and deepseek/deepseek-r1:free. Gemini is better at applying diffs and tool calls than deepseek but neither are really trained for this. If I used a model that was specifically trained for this it would have gone more smoothly.\nAider and Claude Code, or other tools like Zed, Windsurf, or Cursor may do better.\nLessons Learned The Collaborative Model Works Best AI as a Collaborative Partner The most effective approach wasn\u0026rsquo;t having the AI write the spec for me, but rather using it as a collaborative partner.\nThe AI was excellent at:\nHelping structure and organize complex information generating summaries and diagrams comparing refernce information to determine how to improve the existing proposal to incorporate that context. Human Expertise Remains Essential My years of experience with Nova and OpenStack were essential for:\nKnowing what\u0026rsquo;s actually implementable vs. what sounds good on paper Understanding the community and review process Catching subtle interactions with existing features Making pragmatic trade-offs between ideal solutions and mergeable code Process Insights Iteration and Refinement The best results came from multiple rounds of iteration. The first draft was pretty rough, but each iteration I ask for specific problems to be addressed and incrementally made the specification significantly better.\nWould I Do This Again? Evaluating Success Against Goals This is an interesting question.\nAs I alluded to before, if my goal was to get things done faster then I don\u0026rsquo;t think that was a success. Fortunately that was not my goal - my goal was to provide a higher quality initial version of a spec for upstream review.\nI wanted to try and mitigate some of the mental load of written communications and mask the impacts of my dyslexia. That goal was definitely achieved.\nWas it easy? No, this still took a lot of effort to drive. Would I do it again with AI? Yes, on balance I think I would.\nNext Steps and Future Applications Community Review The specification is now ready for community review. I am curious to see how the community responds to an AI-assisted specification. Will reviewers be able to tell? Will the quality be noticeably different?\nExpanding the Approach I\u0026rsquo;m also planning to use this approach for other specifications I\u0026rsquo;ve been putting off due to time constraints. If the community review goes well, I might write up some guidelines for others who want to experiment with AI-assisted spec writing once i gain more experience with it.\nFor now I have created https://github.com/SeanMooney/openstack-ai-style-guide to help with future AI usage in OpenStack.\nImplications for the OpenStack Community TL;DR: AI assistance could democratize technical contribution, accelerate complex projects like Eventlet removal, and help preserve institutional knowledge while raising questions about review processes.\nThis experiment raises some interesting questions for our community:\nAccelerating Innovation If AI can help experienced contributors produce higher quality specifications faster, can it help use implement them.\nEventlet removal is a very complex undertaking. open api schema definition are another community goal with a lot of detailed work. Could AI help achieve those goals faster?\nDemocratizing Contribution Could AI assistance help newer contributors write better specifications by helping them understand OpenStack architecture patterns and community conventions?\nWhat about those with learning disabilities like dyslexia? visual disabilities via enhanced TTS and voice interaction?\nReview Process Evolution How should reviewers approach AI-assisted specifications? What new review criteria might we need to consider?\nKnowledge Preservation One unexpected benefit was that working with the AI to explain OpenStack concepts helped me articulate institutional knowledge that might not be well documented elsewhere. Could AI assistance be a tool for better knowledge transfer?\nFinal Thoughts This experiment convinced me that AI can be a powerful tool for OpenStack development when used thoughtfully. It\u0026rsquo;s not a replacement for human expertise, but it can be guided by it - helping experienced contributors work more efficiently and potentially helping newer contributors learn faster.\nWhat started as a personal accessibility challenge became something bigger: a proof of concept that AI-assisted technical writing can produce genuinely useful results. The specification that emerged from this process isn\u0026rsquo;t just \u0026ldquo;good enough\u0026rdquo; - it\u0026rsquo;s comprehensive, technically sound, and ready for serious community review.\nBut perhaps more importantly, this experiment points toward a future where the barriers to contributing high-quality technical documentation are significantly lower. If AI can help someone with severe dyslexia produce a 700-line Nova specification, imagine what it could do for brilliant engineers who struggle with English as a second language, or newcomers who understand the technology but don\u0026rsquo;t yet know the community conventions.\nThe success here wasn\u0026rsquo;t in replacing human judgment - it was in augmenting human expertise. Every architectural decision, every technical trade-off, every understanding of community dynamics came from years of OpenStack experience. The AI simply helped translate that knowledge into clear, well-structured prose.\nI\u0026rsquo;m excited to see how this specification is received by the community and whether others will experiment with similar approaches. More importantly, I\u0026rsquo;m curious to see if this opens doors for contributions from voices we might not have heard otherwise.\nThe real test isn\u0026rsquo;t whether the AI wrote a good spec - it\u0026rsquo;s whether this approach can help more people contribute to the technical discussions that shape OpenStack\u0026rsquo;s future.\nThe specification is now under community review. I\u0026rsquo;m interested in feedback on both the specification itself and the AI-assisted development process. Feel free to reach out on IRC (sean-k-mooney) or through the usual OpenStack channels.\n","permalink":"https://www.seanmooney.info/blog/ai-to-spec/","summary":"\u003ch1 id=\"using-ai-to-write-openstack-nova-specifications-a-real-world-experiment\"\u003eUsing AI to Write OpenStack Nova Specifications: A Real-World Experiment.\u003c/h1\u003e\n\u003ch2 id=\"background-and-motivation\"\u003eBackground and Motivation\u003c/h2\u003e\n\u003cp\u003eAfter attending the 2025.2 PTG and seeing the extensive backlog of specs that\nneed to be written, reviewed, and implemented, I decided to run an experiment:\ncan AI meaningfully help with the OpenStack specification process? Not just the\nwriting, but the actual design thinking and technical architecture work that\ngoes into a good Nova spec.\u003c/p\u003e\n\u003cp\u003eThe motivation was simple - we have more good ideas than we have time to\nproperly document them. Between my day job, core review responsibilities, and\nthe general time constraints we all face, turning a concept into a\nwell-structured, technically sound specification can take days or weeks. What\nif AI could help accelerate that process?\u003c/p\u003e","title":"Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance"},{"content":"Watcher PTG Etherpad\nOverview The 2025.2 PTG was the Second PTG i have attended for watcher. The last PTG had one session to cover all the technical debt required to revive the project. This time we extended the ptg to 2 days.\nDay 1 Tech Debt Croniter The PTG started with a disscussion about technical debt that emerged during the epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule continuous audits based on cron strings. The Croniter libaray maintainer has decided to stop maintaining this library and it is transitioning to a new maintainer. While that gives watcher some breathing space, our usage is minimal, so we agreed to drop the dependency by leveraging the existing functionality of apscheduler.\nSupport status of datasources The second topic of the ptg focused on the support status of datasources and features of watcher in general.\nAs part of this discussion we touched on the idea of multiple levels of support:\nCore features (supported, tested in ci and ready for production use) Experimental features (supported but not tested in CI or may not be suitable for production use) Deprecated features (features that are no longer maintained and not suitable for production use) Unsupported features (features that are not supported by the project, but may work) During this session we agreed to deprecate the support for the following features:\nMonasca Grafana Noisy Neighbour Strategy (l3 cache) Monasca support was dropped because it is no longer maintained and is officially being retired.\nGrafana support was dropped because it is no longer maintained and has no existing ci coverage under our new support levels it could be considered experimental as it may work under certain conditions however since there is no ci and no active development it was agreed that we should drop support for this feature.\nThe Current Noisy Neighbour Strategy (l3 cache) was deprected because it depends on metrics that are nolnger avialable for several years. a dedicated session on how to replace it was held later in the ptg to discuss how to replace it going forward.\nFinally we wrapped up this topic by agreeing that we should create a support and testing matrix for watcher takeing inspiration from https://docs.openstack.org/cinder/latest/reference/support-matrix.html or https://docs.openstack.org/nova/latest/user/support-matrix.html The intent is to provide a way for users to see which features are supported, tested and suitable for production use cases.\nEventlet Removal The final technical debt topic was Eventlet removal. Watcher like many other OpenStack projects have been using eventlet since its inception, and it has become clear that eventlet is no longer sustainable. While no concrete proposal was made for this topic, to start evaluating the removal this cycle with some initial POCs and aim to remove eventlet in 2026.1\nWorkflow/API Improvements SKIPPED status for actions This discusstion focused on the introduction of a new Skipped status for actions. Today actions can be in one of the following states:\npending ongoing succeeded failed When constructing and executing an action plan it would be useful to extend the action states to introduced a skipped state, which would allow users to mark an action as skipped, preventing it from being executed by the engine. The idea is that this can be used to skip actions that are known to fail or be undesirable to execute. Additionally if an actions preconditions cannot be met, the action can be marked as skipped instead of failed.\nThis lead to a larger discussion about the meaning of SUCCEEDED and FAILED for action plans in general. https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108 We discussed that the current behavior where an action plan is considered succeeded when all actions are attempted regardless of the success of the action is unintuitive and not in line with the state machine docs.\nit was agree that if any action fails the overall action plan should be reported as failed. Additionally in this context we agreed that if the SKIPPED state is added it should be considered as Succeeded, not Failed.\nwatcher-dashboard This sessions was relatively short and focused on 2 areas testing and missing functionality\nTesting We opened the session discussing the fact that the horizon plugin as almost no testing. the net effect of this is every change need to be manually verified by the reviewer. We have two options with regard to testing we can build on django\u0026rsquo;s unittests integration to validate the core logic, or we could build out selenium testing. no general conclusion was arrived at but we resolved to follow up with the horizon core team in the future for guidance.\nExposing parameter in audits This was simple and non controversial, today when creating a audit its not possible to set the parameters filed via the horizon ui. we agreed this should be addresses as a speechless blueprint.\nSKIPPED status finally we discussed that if we extend the action status to model the new SKIPPED state we will need to enhance the action plan dashboard to allow skipping an action and to display the skipped state when a precondition fails.\nWatcher cluster model collector improvement ideas This was arguably another tech debt discussion in the guise of an operator pain point. For performance reasons watcher is build with a cached data model that periodically refreshes, combined with integration with notification for near real-time updates.\nDuring the session we reviewed https://bugs.launchpad.net/watcher/+bug/2104220 and resolved to investigate if we are properly consuming the notifications form nova. While we are consuming the notifications bug #2104220 asserts that we are perhaps using the wrong fields to update the instance host on live migration resulting in the source host being updated instead of the destination host.\nother mitigation were discussed such as adding the ability to force refresh the model when executing an audit, creating a new audit type to just run the refresh, or adjusting the collector interval.\nDay 1 Summary Day one was pretty packed but we resolved to finish early when we reached the end of the agenda.\nWe resolved to proceed with 2 new specs for the SKIPPED sate and model collector plugins and to treat the parameter enhancement to watcher-dashboard as a specless blueprint. croniter will be replaced as a bug fix and we need to review the impact of bug: #2104220\nDay 2 Watcher and Nova\u0026rsquo;s visible constraints Day 2 kicked off where day one ended on the topic of how watcher models nova instances and the visibility of scheduler constraints. In the epoxy cycle we added a number of attributes to the server show response that may be of interest to watcher, namely the scheduler hint and image properties. additionally i noted that in a prior release we added the pinned AZ as a separate filed. while extending the data model to provide this info would not be an api change we discussed that a spec could be nice to have to document the changes but a specless blueprint could also be valid.\nNoisy neighbour strategy This discussion happend as a sperate session on the second day but we touched on it on day 1 as part of the tech debt session. The TL;dr is since the exisitng noisy neighbour policy is non functional and since we want to aovid upgrade impacts a new SLA goal with 1 or multiple strateies shoudl be created. A spec to define the semantics should be created but at a high level the open questions are\nshould we have one stagey per metric or a single parmaterised stagey which metrics shoudl we use as the key performance indicators to comptue if the sla is treshold is exceeded cpu_steal io_wait cache_misses others? finally we agreed to deprecate the existing strategy this cycle for removal in 2026.2.\nHost Maintenance strategy new use case The last feature session discussed the limitation with the current host maintenance strategy the main limitations resolved around how to express if a instance should be stopped, cold migrated or live migrated. we agreed to continue with the spec https://review.opendev.org/c/openstack/watcher-specs/+/943873 and that it was ok to add new parameters to allow specifying which actions are allowed for the audit.\nThis topic was reprised in the nova room on day 4 TL;DR in the nova session we agreed to try to use server metadata to encode policies like live migratable. this can be incorporated into the spec design as a follow up if there is time. i.e. instance can be annotated by adding lifecycle:livemigratable=True|False and similar lifeccyle: metadata keys to express which actions may be taken by an external orchestrator like watcher.\nPTG wrappup contributor docs The second to last session was a quick discussion of some of the document that watcher is missing namely the scope of the project doc and the chronological release guide. these are both important document for use to create going forward, as is ensuring our existing contributor docs are up to date regarding when to file a bug vs blueprint vs spec.\nretro and adhoc planning To wrap up the ptg we had a short retro on how the last cycle went and any last minute topics. traditionally one start the PTG with the retro rather then finishes but its better late then never. we realised that one topic had not been raised which is the future of the core team we briefly discussed that going forward we will review the core team member ship at the start of each slurp release (i.e 2026.1) and remove inactive cores that have not participated in review, contribution or ptgs since the prior slurp. As such no removal form the core team will be made for 2025.2 although new addition may be made based on review activity over the cycle.\nWith that we we wrapped up the ptg with a reminder of our two cross project sessions with telemetry/horizon on day 3 and nova on day 4\nTelemetry/Horizon Context This ptg unlike many others i attended a combined telemetry/horizon cross project session lead by Victoria Martinez de la Cruz (vkmc). There we discussed a number of topics related to observability and how to provide tenant and admin static dashboards to visualise metrics.\nInitially we started by discussion why observability and metrics are distinct form the existing views provided by the admin hypervisors dashboard and how they are distinct form showback/chargeback provide by cloud-kitty\nshowback/change-back focus on billing and the ratings are base not on the utilisation of the provisioned resource but rather then allocation. i.e. you are billed equally for an idle vm that uses 4 cpus and 16G of ram vs a fully loaded vm. cloud kitty can help tell you how large your bill will be but it will not tell you if your application can run with half the resources. A observability view based on near real-time metrics aims to solve the latter problem.\nThe hypervisor dashboard is also related but distinct. the hypervisor view is intended to provide a semi real-time view of the capacity of a could for admin to be able to plan if they need more capacity. Again this view looks more at the available resources and the allocated resource but does not provide an overview of the utilisation.\nPOC An initial poc of what the dashboard could looks like is available here https://github.com/SeanMooney/grian-horizon-plugin proposal create a new horizon plugin as an official deliverable of the telemetry project in the governance repos this will be hosted in the https://opendev.org/openstack namespace and will be released as an official deliverable via the release repo we need to spike on d3.js vs rickshaw vs chart.js for charting and assess the version compatible and tech debt initially the new metrics dashboard will integrate with Prometheus as a metrics store. a fake backend may be created for testing, other backends are out of scope but could be added later. a combination of new panels and tabs in existing panels will be added. initially all dashboards will be static and direct quires to the backend will not be supported this will allow the plugin to provide multi tenancy as required in the absence of multi tenancy support in Prometheus. the plugin will be stateless with the time span that can be viewed determined by Prometheus. the design goal would be to support short term metrics of up to 7 days. the python-observability client will be used to query Prometheus. Nova The nova ptg was busy as always, i was also sick all week and had conflict with watcher/horizon so i was not able to attended all the sessions so ill focus on some of important topics form my perspective instead of trying to do a full summery. for those looking for a full summery here is a excellent write up Rene\u0026rsquo;s Official PTG Summary, i will also not cover the eventlet removal topic in detail as gibi has a write up of that here Eventlet removal - Flamingo PTG\nDAY 1 day 1 was a short day to allow for some cross project sessions to happen before nova officially started. as is normal we started the ptg with a retrospective on the past cycle, what worked well and how we wanted to proceed.\nproject planning In general not much has changed for this cycle but ill just highlight some important dates Timeline:\nSoft spec freeze (no new specs): June 1st Hard spec freeze (M2): July 3rd Feature Freeze (FF): August 28th Final release: late September / early October For those not familiar with the difference between the soft and hard spec freeze the soft freeze is the last date we ask contributors to propose large pieces of work. it serves as a signal that if your topic is complex and has not already been socialised and reviewed by that point it likely wont be mergeable by the hard freeze. This basically creates a buffer before people tend to disappear for PTO or others factors that might make getting a quorum difficult.\nThis worked well for us in Epoxy when we set the soft spec freeze in early December to acknowledge that we would loose quorum between late December and early January.\npython 3.13 and eventlet This are related but distinct topics. This cycle we would like to start early testing with python 3.13 as Ubuntu 25.04 has now moved to 3.13 and i believe Debian 13 may also. unfortunately monkey_patching the tread lib breaks on 3.13 so eventlet based service cant actually run on 3.13 under devstack unit test jobs seam to pass so that a start. we agreed to add a functional-py313 job if we can get it to pass however we do rely on eventlet there so that may or may not be possible.\nEventlet removal will be one of nova\u0026rsquo;s priories for the 2025.2 cycle as we acknowledged we are behind were we should be at this point in time. for those wanting to follow this progress we are going to track it as a recurring slot in our IRC meeting and gibi is planning to document the progress in there blog, the first post is already live!\nDay 2 day 2 kicked off with some security related topics and finished up with live migration discussions\nSEV TDX and arm CCA Confidential computing is not new by any means even in nova with support for SEV being available for many years. Last cycle tkajinam had started to make improvement in SEV-ES support in advance of the enablement of SEV-SNP, unfortunately we did not have review bandwidth to land there enhancements. We resolved to prioritise the review of there work and also discussed the new development that are happening on intel and arm platforms. TL;DR the kernel, qemu and libvirt work is not complete for both TDX and CCA at this time and in the arm case there still isn\u0026rsquo;t hardware in production. effectively its too early to actually add support for arm CCA support and its likely to be a 2027.1 topic based on Ubuntu 26.04+ intel TDX is closer but also likely to early to supprot in 2025.2\nPQOS (platform quality of service) This is and old one. so like 7 years ago when i was still at intel there was an effort to support intel cache and memory bandwidth both statically in nova (pushed by myself) and using an external resource management daemon call RDT. both effort die out but in the last month or two people have started asking about it again.\ni was not able to attend this session but it seam like the new contributors are going to pickup my old spec and re-propose it for 2025.2 with updates. Since i wrote that in 2019 the way intel CMT and MBT work has been simplified to some degree so they will update the spec to reflect that and we will review.\nWhile this may be useful for some real-time workloads that are latency sensitive, its non trivial to add, complex to configure and likely not a generically useful outside a select number of workloads. This will be on to monitor but likely not a feature that will be used in many clouds. for those that need it however it could be a big improvement.\nlive migration topics vtpm TL;DR vtpm live migration is our largest gap in this area we briefly discussed later in the ptg that the pattern for vTPM secret handling will also inform how we address the same challenges for cinder encrypted volume live migration and local nova storage encryption when we eventually get back to that. The big change in our plan for vTPM is that this work will be taken over by another contributor and artom will work with them to hand off this work early in the cycle.\npci/vGPU live migration follow ups In epoxy nova added support for vfio variant drivers and with that support for pci devices that\nEnable VFIO devices with kernel variant drivers Live migrate VFIO devices using kernel variant drivers During the ptg we discussed what could not be completed and the divergence of the code paths between flavor based pci pass-though live migration and neutron SR-IOV port live migration.\nThe big takeaway form this session was we will address https://bugs.launchpad.net/nova/+bug/2103631 in two steps. First we will move the check before the conductor calls the scheduler. this is a backporable fix and will provide most of the optimization benefit by entirely eliminating the placement and scheduler work. Second we will move the check to the api and return a 409. this will fully optimize the early exit but it will not be as backportable. While the api can already return a 409 https://docs.openstack.org/api-ref/compute/#live-migrate-server-os-migratelive-action chaning a 202 to a 409 is not always considered valid for backports.\nDAY 3 day 3 was perhaps the most eclitic of topics with everything form confirming we will complete previous work started in epoxy to discussion of eventlet removal and cross project session with glance/cinder .\nVDI enhancements, os-traits and one-time-use devices VDI and one time use devices were short as the first was a carry over form work that was approved form epoxy but didn\u0026rsquo;t merge in time and one time use devices merged the week before the ptg :) VDI in openstack is a somewhat complex topic. on one hand we know that there are a number of gaming companies that host there server side infrastructure on openstack but more recently with the rise fo cloud game streaming companies there are now a new type of nova user that use both GPU pass-though and spice fundamentally to delver high performance vdi solutions to there customers. Spice and to a similar degree RDP are both better positioned to meet the semi real-time demands of cloud gaming then VNC ever will be but its also an area of nova that until recently has not had much development in a very long time.\nboth however hit a common pain point, how we use and release os-traits so we spent a while discussing possible path forward to streamline the work flow. fundamentally we did not resolve to actually make those changes but absorbing os-traits and os-resource-class back into placement but we generally agreed that we could do that if we happen to find the time.\none time use devices also sparked a discussion on a related placement bug and its fix this could be seen as controversial but we all agreed that the existing behaviour is wrong and that we shoudl fix it. Dan presented there solution and we generally agreed that it seam like a reasonable approach and that we should proceed with the review. For operators resolving the current limitation that placement Allocations cannot be adjusted when provider is over capacity will transitively fix 3 separate nova bugs all rooted in this underlying limitation.\nThis session was also very useful as John Garbutt reminded us that we used the set reserved=total trick to fix a race condition with cleaning in ironic. i.e. for ironic nodes we set reserved=total when we provision a node and reserved is only reset after cleaning is complete. So while this is undocumented behaviour its not undefined and we now have 2 users of it so we probably shoudl test this in placement properly and update the api ref to document it.\ntech debt oenstack client support last cycle was a departure form previous cycles when we actually had multiple api changes. unfortunately all 4 api change merged late and none of them landed the sdk and osc change required to fully complete the features. manila share support spice direct Show Scheduler Hints in Server Details image properties in server show\nWhile this is a low-light all of the above have patches in flight and should land early this cycle. This topic was actually split between this session on Wednesday with the main topic discussion happening on Thursday with the client/sdk teams and QA teams for the tempest impact.\nglance/nova/cinder corss project unfortunately i could not attend this due to a conflict but the main topics covered were captured in the glance ptg summery the two most impoant topics in the nova context were\nnew location API (Cinder \u0026amp; Nova) Glance has introduced two new Location APIs in the Dalmatian cycle. We can use these APIs to address OSSN-0090 and OSSN-0065. Patches for Nova and Cinder must still be merged, hopefully during the Flamingo cycle.\nfreeze glance client development Cinder and Nova will need to use the OpenStack SDK instead of the glanceclient. There is no need to complete this work during this cycle, but we should at least have a good idea of what APIs are currently being used, so that we can have a plan for the next cycles.\nHow nova store image properties in our db you may wonder why this is not relevant to the glance discussion it is but this is really internal to nova usage of image properties specifically not storing them if we don\u0026rsquo;t need them. There is a lot of history to this topic but the clips note version is nova stop supporting custom image properties more than a decade ago but our persistence of them in our db was a little leaky see: https://bugs.launchpad.net/nova/+bug/2098384 for more details. We agreed that moving forward we should not store image metadata keys in our db that are not part of the standard set and we should plan to provide a way to clean up the stale entries that already exist.\nFinalizing the secure RBAC goal the TL;DR of this is the implementing of the manager role and service roles in our policies started a while ago but unfortunately gmann did not have time to work on it in Epoxy until late in the cycle and we didn\u0026rsquo;t have time to review it when they got unblocked. so in 2025.2 we would like to prioritise completing this work which is the final work remaining for nova to complete the SRBAC community goal.\nEventlet removal i wont go into this in detail because gibi already wrote up a better summery but This was the second session on eventlet in the context of nova (the first being the community one lead by oslo). Here we discussed the concrete next steps for nova both in terms fo what to do for 2025.2 and beyond. The important take always are we will track the progress of this goal weekly in our meetings, gibi and kamil will work to incrementally remove our direct eventlet imports throughout the cycle and try to re-implement scatter gather in the api so that it can be the first nova service to run without eventlet. This is probably the largest change in novas implementation since we added python3 support or the initial move form twisted to eventlet in the very early days. As a community we have 12-18 months to complete this transition and the nova core team committed to prioritise this work going forward.\nDay 4 as with day 3 there were many different topic raised mainly in the context of new features combined with cross project session with neutron, tempest and client/sdk teams.\nA QOS api for nova This was and is likely the most contoverial topic that was raised during the entire ptg bar providing hooks for the XML which was a straight -3 as that is never going to happen. during this session we discussed a proposal that is documented here\nthis topic not only involved nova but also cinder and neutron changes. in essence this requires fundamentally changes to how nova neutron and cinder work internally and together.\nNova does not supprot changing the QOS applied to neutron Port or cinder volume while the resource is attached to nova instance. on the neutron size if the QoS policy is a min bandwidth or PPS policy, changing the QOS on a port can invalidate the placement of the VMs breaking how those features work.\nFrom a nova point of view if you allow the qos policy on a neutron port to change in neutron while its attached to a nova instance it is therefor a bug.\nCinder and nova have a similar impedance mismatch in that cinder defines it qos policies on the volume type not on the volume. that means that changing the qos on a volume type would impact many volumes nova has not way to support that today. To make matters worse operators already abuse the fact that the fronted qos is updated on live migration to apply these changes live to existing instance even though nova has no idea this is happening and that might fail.\nThis was a somewhat challenging session as the topic was brought by an operator that already designed how they wanted it to work and were asking for use to just accept the changes even though there design is not really compatible with how self service multi tenant cloud should work.\nwe spend a lot of the session discussing a possible future nova api for QoS similar to neutrons i.e. where a operator can define a set of QoS policies (effectively a sub set fo flavor extra specs) which an end user could select form and that an operator could apply to a project by default.\nwhile that may be a valid evolution of novas api it touch on previous landmines like composable flavors. we will also need to carefully think about the RBAC implication of who should be able to set a QoS policy on a Project or instance level and if it shoudl be settable via a Flavor. For example the manager role likely shoudl be able to set it on a project level if and only if an admin has not set it. admin likely should be able to set a qos policy in a flavor but when not set a person with the member role should be able to select form any of the QoS policies defined by an admin as they can in neutron today.\nwhile we tired to convey some of this feedback i feel like they did not internalise that well. we also provide context to them on previous discussion form 2019 https://etherpad.opendev.org/p/r.23a243a5860c7406917f86f48cfb4491#L389 during the shanghai ptg.\nthe proposer resolved to follow up with each team separately and file a spec for the works but we will have to wait and see if took on board our feedback or not.\nOVN live migration who would have guessed that its still a pain\u0026hellip; we discussed https://bugs.launchpad.net/nova/+bug/2073254 in Thursday and then again on Friday the TL;DR is neutron now send the event at the right time so we can remove the hacks we added 5 years ago so i have proposed https://review.opendev.org/c/openstack/nova/+/946950 to address that.\nWe also discussed that we may want to move the creation of the tap to os-vif instead of libvirt. https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1174 This likely should only be done if neutron asked us too but we can make progress even without this.\nWatcher topics This was the main topic i added this PTG to discuss how we could evolve nova to better support watcher use-cases. In this session we discussed how we can annotate instance with metadata to inform policy. i.e. can this instance be live migrated. the second topic was how can nova tell us if a vm can be migrated or if other lifecycle operations are valid like cold migration or shelve. the third topic was slightly different on the usefulness of weighing host by traits with two different use cases, preferred/avoided traits via flavor and automatically avoiding host with *_pressure traits.\nannotating instances with metadata The proposal to add a new api for instance annotation was deferred/rejected in favor of using the existing instance metadata for this use-cases. the proposed set of metadata pairs are\nlifecyle:evacuatable=true|false lifecyle:cold-migratable=true|false lifecyle:shelvable=true|false lifecycle:premetable=true|false ha:maintance-stragey:in_place|power_off|migrate ha:role=primary|secondary ha:priority:[string|number] these can be documented by the service that uses them but we may want to maintain a list of useful metadata keys like the glance useful image properties doc in nova if they are used. We can/should document that nova considers this a valid use-case for metadata provided the user applies the metadata themselves.\nnote: on Friday we had related discussion about admin metadata which ill cover later.\ninstance capabilities we might work on the name because i will never spell that properly :) this we said we should do and i should write a spec during the session artom mention that in addition to the lifecycle annotations above another capability could be resume on host reboot. after the discussion i realized there are some other lifecycle operation like suspend, pause and snapshot that might make sense to report.\nso the full set would be\nlifecyle:evacuatable=true|false lifecyle:cold-migratable=true|false lifecyle:shelvable=true|false lifecycle:premetable=true|false lifecyle:resume_on_host_reboot=Ture|False lifecyle:pause=true|false lifecyle:suspend=true|false lifecyle:rescue=true|false lifecyle:snapshot=true|false the idea of instance capabilitiesis that nova should know what operations are valid to perform on an instance based on the virt driver, flavor, image and other project resources like manilla shares, cinder volumes, neutron ports, and cyborg accelerators.\nThis will need a spec to proceed but the general direction is seen as positive.\n*_PRESSURE traits and trait based weighing Firstly we started this conversation by asserting that if the Trait is custom nova will not touch it. So any human or service could annotate nova resource providers with CUSTOM_ traits\nsecond i asserted i would like to have Standard traits for CPU RAM and DISK pressure\nPRESSURE_CPU PRESSURE_MEMORY PRESSURE_DISK network pressure could be added late btu its not one of the one reported by the kernel today https://docs.kernel.org/accounting/psi.html#pressure-interface\none of the open question i had for this session is would nova be ok with the libvirt driver reporting these traits or should watcher annotate host with them based on a periodic audit. the core team was generally ok with the libvirt driver doing this but we will confirm that as part of a spec.\nThe second half of this topic was focused on how these newly reported traits when placing vms\nThe proposal create 1-2 new weightier to way based on traits.\nfor the *pressure traits we likely want to aovid them by default however there may be other traits that an operator may want to avoid or prefer. so that implies that there should be a config option. we decide to punt to the spec/implementation if we wat to include the pressure traits by default or not.\nthe second weigher would be a flavor based weigher that would extend the existing required/forbidden syntax to supprot preferred and avoided\nboth weigher will rely on a common change to the host state object to pass the provider summaries this will allow filters and weighers consider traits and available resource classes.\nnote i did not discuss this in the session but this would also allow me to introduce what i have called in the past a boring host weigher. the concept of the boring host weigher is simple prefer host with fewer traits and resource class or generally a simpler provider tree. The logic, simpler the provider tree the less fancy the hardware available on the host and there for its less likely that placing a random vm on the host will prevent a future vm that need one of the special aspect of the host form fitting.\nWe agreed that while these are all nice to have and in scope of nova we will propose concrete next steps in a spec if/when we work on them.\nOpenApi Stephen made massive progress last cycle in nova but we ran out of review bandwidth they summerised there progress in general across openstack in a recent blog post An Update on OpenAPI in Openstack during the ptg session we resolved to try and continue making progress on this as nova is in a good position to complete this work. we also discussed that once this work is complete and nova has both request and response schema for all apis we could remove the tempest schema validation in a later release. This would be a nice reduction in technical debt for tempest but will need to wait until all stable/* release of nova support open API validation.\nclient changes we covered this earlier but Stephen also lead a discussion on how can we work better between the nova and client/sdk teams to make sure we complete the client work when we make api changes other then not merging them really really late which we should avoid anyway we mainly just reiterated that direct pings are ok and actually welcome when we thing a change is ready to merge. in other word reach out when we are close and use depend-on ectra to call out the deps on the api changes.\nTesting, testing and more testing i wont spend much time on this but interleaved with other topics we discussed how to improve testing in tempest, and can we finish the efforts to []emulate hardware in ci](https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L859) to test OTU devices/pci passthough, ndd generic mdevs as well as maybe test sr-iov via IGB down the road.\nthe answer is yes, we can and should update the spec template to note that at a minimum we shoudl update the tempest schema tests when we add a new api. dan has started a effort to test pci passhtough as part fo testing OTU device using the virtio-RNG. sylvain Will revive there mtty series to create a proxy for generic mdevs.\npower management next steps the final session of day 4 was a deep dive on where power management is, the original design goals and what shoudl we do to fix it. Actually that a lie we did this out of order and talked about this before the OpenAPI and client topics but I\u0026rsquo;m going to bottom on the etherpad so we will pretend this was the last sessions :)\nthe over all outcome fo this session was that we shoudl not rush to power off the cores on startup and should wait for the resource tracker to be populated with instance so we can ensure we never power off cores related to an instance that is running. second while we could modify the virt driver interface to allow the comptue manager to defer the power off we should explore if we can just internalise this entirely in the libvirt driver first. if we cant then virt driver change it is. in other words don\u0026rsquo;t rush to power off the core until we are sure we dont need to power them back up and that they are not in use.\nDay 5 live migration with cinder encrypted volumes This does not work today for exactly the same reason as vtpm migration. admins do not have access to secrets owned by users. The fix, learn lessons form how we fix this for vtpm and apply the same to cinder volumes. specifically the only way we can fix this is the host policy https://specs.openstack.org/openstack/nova-specs/specs/2025.1/approved/vtpm-live-migration.html#id2\ncinder volume secrets unlike vtpm are not delete form libvirt when we boot the vm. so we could copy them between hosts once we implement that for vtpm. we already depend on the fact the secret is available in libvirt for resuming guest with encrypted volumes on host reboot.\nManila cross project session now that you can attach a share to a nova instance we took time in this ptg to reflect on the next steps. There are many but in priority order they are the following:\nfinish the osc support and backport to epoxy testing (tempest, functional testing elsewhere) memfd memory type in nova (possible by default) online attach/detach attaching a share during server create live migration (still need to have this fully supproted in libvirt/qemu) that is a lot of future work but each is incrementally doable and we resolved to have specs as need for each item\nprovider.yaml enhancments The main topic of this session is simple. today you can add custom traits via provider.yaml if you later remove them from the file now will not remove it form the resource provider\u0026hellip; it should do that.\nso we discuss a few ways to address that and i suggested that nova could copy the file to its state Dir .i.e. /var/lib/nova/last_applied_provider.yaml on the next restart nova could then compare the traits in the last_applied_provider.yaml to the current provider.yaml and compute if any trait shoudl be removed. nova would then up copy the new provider.yaml to last_appled_provider.yaml if it successfully updated placement.\nthe proposer will do a poc and write up a spec or specless blueprint for this. in the future we could support other changes like the ability to remove resource provider created via provider.yaml but that will initially be out of scope.\ninstance metadata protection This was another very productive session, the use case that was presented is as follows, I as a cloud operator have some knowledge of my infrasture that i want to apply to an instance, i would like to make this query able via the rest api, to that end i would like to annotate the instance with this metadata but i would like to be able to prevent a normal user form removing this metadata.\nthe discussion started with there proposal to have a config option to define metadata protection sort of like how you can define protect image properties that only an admin can set in glance.\nthe main problem with this approach is interoperability between clouds. we then discussed the idea of defining an admin prefix and perhaps a manager too allow user with those roles to set keys the member or reader users cannot modify. while this would work it woudl require any exiting metadata that an operator might be setting to be updated.\nfinally we discussed how the lock api and the fact that we record the role of who locked the instance. our general recommendation was to create a spec to allow recording the role of the user that set metadata and to prevent lower privileged roles from removing keys set by higher privileged roles. that would define the following prescience relationship admin \u0026gt; manager \u0026gt; member\nwe said we woudl be happy to review a spec to that effect if they write one.\nupdate/set delete on terminate for neuton port This was short, we should supprot it but we just have not had a contributor to work on it so patches are welcome. we suggested starting with a poc and filing a spec once they are happy they have something working\n[Graceful shutdown] (https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089) this was another rapid fire topic. we explained the previous attempt and the fact we currently don\u0026rsquo;t have capacity to work on this.\njohn raised an interesting alternative which would be to add a maintenance mode concept. this could be implemented as a block in the api of all write operations alternatively we could decorate all rpc functions that start an instance action or move op with a decorator that would reject the rpc with a 409 if the compute service was set to maintenance mode either as a new sate for disabled or as a new filed.\nwe all agree that a more graceful shutdown would be nice to have and a poc of different approaches might be a good next step currently the core team does not have time to work on it but we welcome other to propose solutions. The main call to action is to report any bugs for problem we know of when there is not a graceful shutdown while we may or may not be able to fix them that will provide a list of edge cases that need to be solved for.\niothread and block device queue Again this is a topic that was started on in the past and stalled out. there has been previous resarch done by redhat and other on how virt-blk scales with io vs iothread vs queues https://developers.redhat.com/articles/2024/09/05/scaling-virtio-blk-disk-io-iothread-virtqueue-mapping\nthis session basily was quick and after some discussion fo the history and previous recommendations of our virt team to always have at least 1 iothread we agreed that the propose should write a spec to introduce 2 new flavor extra specs/image properties to be able to request the number of iothread and block device queues. .e.g hw:io_threads=4 hw:blk_multiqueue:2\nthe detail will be covered in the the spec but when unset we should likely always allocate 1 iothread and preserve the libvirt default of creating one virtio-blk queue per vcpu.\nfiltering server list by neutron network the final large topic of the ptg was a proposal to filter server list by neutron network (possibly also subnet in the future)\nthe general problem to day is while you can make a single query to get all port device-id values (the nova instance the port is attached too) for all ports on a neutron network, you cant then call nova with 100s of uuids in one query to list the server details. the proposer uses this information to determine the owner of the vm so that when they need to do maintenance or a particular network runs out of ips they can reach out to there customer and ask them to move a port or release an ip ectra.\nmost of this session revolved around is this in scope of nova to do or not.\nin the end we resolved that if the data is in the nova db and easy to filter on or we can make a efficient query to another service that a user cant replicate because they cant pass the resultant uuid to nova for list, then its ok to extend server list to do this.\nwe will only add filter on a case by case basis. as a result we agreed that if the proposer writes a spec and works on a poc we will review them.\nPTG summary. a lot happened in nova and watcher this PTG and more happened in the TC and other project session that i have not covered at all. hopefully someone will find this useful. i have tried to summarize my understanding of all the sessions i attended and provide links to the sources where possible.\nwe got a lot done in what was a total of maybe 20 hour of real-time meetings but we equally now have alot of work to do to achieve all we discussed including many specs to read and write.\nI woudl personally like to thank rene for running the nova sessions and all the rest that contributed there time. it was a long, tiring and stressful week for many but it was also good to see us disucss these topics as a community in the open.\n","permalink":"https://www.seanmooney.info/blog/2025.2-ptg/","summary":"\u003ch1 id=\"watcher\"\u003eWatcher\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4\"\u003ePTG Etherpad\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThe 2025.2 PTG was the Second PTG i have attended for watcher.\nThe last PTG had one session to cover all the technical debt required\nto revive the project. This time we extended the ptg to 2 days.\u003c/p\u003e\n\u003ch2 id=\"day-1\"\u003eDay 1\u003c/h2\u003e\n\u003ch3 id=\"tech-debt\"\u003eTech Debt\u003c/h3\u003e\n\u003ch4 id=\"croniter\"\u003e\u003ca href=\"https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L39\"\u003eCroniter\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eThe PTG started with a disscussion about technical debt that emerged during the\nepxoy cycle. This focused mainly on croniter, which is used by watcher to schedule\ncontinuous audits based on cron strings. The Croniter libaray maintainer has decided\nto stop maintaining this library and it is transitioning to a new maintainer.\nWhile that gives watcher some breathing space, our usage is minimal, so we agreed\nto drop the dependency by leveraging the existing functionality of apscheduler.\u003c/p\u003e","title":"2025.2 Ptg"},{"content":"What Is OpenStack? For those of you who have not heard of \u0026ldquo;OpenStack\u0026rdquo; or the OpenStack Foundation, you might be wondering what it is. Wiki defines it as an a free and open-source software platform for cloud computing, mostly deployed as an infrastructure-as-a-service (IaaS).\nWhile that is true, it is incomplete and unless you know what infrastructure as a service is it\u0026rsquo;s not very helpful.Granted the wiki page does actually explain what OpenStack and what OpenStack does but ill have a go anyway. So OpenStack as software, is a collection of composable interrelated projects primarily written in python, for creating private and public clouds environments.\nOpenStack provides an abstraction layer that allows a collection of servers to be managed as pools of compute, storage and networking services. compute, storage and networking are the 3 pillars of IT service that make up a standard IT infrastructure. As OpenStack provides a software interface to manage these 3 pillars of IT infrastructure it is an Infrastructure as a service offering.\nThe project that makes up OpenStack IaaS offering are what used to be know as the OpenStack core projects.\nCore Projects: Nova: Compute Project Neutron: Networking Project Keystone: Authentication Project Glance: Images as a service Project Cinder: Block Storage Project Swift: Object Storage Project Non-Core projects?: But if there is a Core then are there more non-core projects? For OpenStack that is a resounding YES!!! in fact since the creation of the big tent and arguably before, the compendium of OpenStack and OpenStack related projects has been expanding beyond IaaS to Orchestration, Platform as a service, advanced services such as container orchestration and support services such as workflow engines, database as a service, backup as a service and many other micro services that can be composed to build your own OpenStack-powered application.\nA listing of offical project teams can be found in the openstack governance repo.\nHow mature are all test projects? With all these core and non-core project to choose for do I need them all? How mature are they really? Are they all ready to use in my production application?\nWell, the answer to the first question is simple, OpenStack is a composition of micro services that work together to provide a could of your own design. if you don\u0026rsquo;t need object storage then you can deploy without swift, already have ceph deployed? no problem you can have cinder,nova and glance use that as your storage backend. do you want DNS as a service by not orchestration? you can deploy designate but leave heat out of your cloud deployment.\nThe second question of how mature these projects are is a little harder to answer. The OpenStack Core project have matured over many releases to be stable and production ready, the advanced services are at a differing level of maturity but luckily the OpenStack foundation has been developing a tool to help you make up your own mind.\nThe OpenStack Project Navigator is a great place to start and if you still have question the reaching out to the comunity is as easy as jumping on IRC or sending a mail to the mailing list.\nFuture Reading Openstack homepage Openstack software overview Openstack architecture 10,000 feet Intro to neutron Openstack Foundation youtube ","permalink":"https://www.seanmooney.info/blog/what-is-openstack/","summary":"\u003ch1 id=\"what-is-openstack\"\u003eWhat Is OpenStack?\u003c/h1\u003e\n\u003cp\u003eFor those of you who have not heard of \u0026ldquo;OpenStack\u0026rdquo; or the \u003ca href=\"https://www.openstack.org\"\u003eOpenStack Foundation\u003c/a\u003e,\nyou might be wondering what it is. Wiki defines it as an a free and open-source\nsoftware platform for cloud computing, mostly deployed as an\ninfrastructure-as-a-service (IaaS).\u003c/p\u003e\n\u003cp\u003eWhile that is true, it is incomplete and unless you know what infrastructure\nas a service is it\u0026rsquo;s not very helpful.Granted the wiki page does actually\nexplain what OpenStack and what OpenStack does but ill have a go anyway.\nSo OpenStack as software, is a collection of composable interrelated projects\nprimarily written in python, for creating private and public clouds environments.\u003c/p\u003e","title":"what is openstack"}]