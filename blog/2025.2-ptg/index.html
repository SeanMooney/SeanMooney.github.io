<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>2025.2 Ptg | Today I learned</title>
<meta name="keywords" content="openstack, ptg">
<meta name="description" content="Watcher
PTG Etherpad
Overview
The 2025.2 PTG was the Second PTG i have attended for watcher.
The last PTG had one session to cover all the technical debt required
to revive the project. This time we extended the ptg to 2 days.
Day 1
Tech Debt
Croniter
The PTG started with a disscussion about technical debt that emerged during the
epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule
continuous audits based on cron strings. The Croniter libaray maintainer has decided
to stop maintaining this library and it is transitioning to a new maintainer.
While that gives watcher some breathing space, our usage is minimal, so we agreed
to drop the dependency by leveraging the existing functionality of apscheduler.">
<meta name="author" content="Sean Mooney">
<link rel="canonical" href="https://www.seanmooney.info/blog/2025.2-ptg/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.seanmooney.info/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.seanmooney.info/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.seanmooney.info/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.seanmooney.info/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.seanmooney.info/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.seanmooney.info/blog/2025.2-ptg/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="/css/custom.css"><meta property="og:url" content="https://www.seanmooney.info/blog/2025.2-ptg/">
  <meta property="og:site_name" content="Today I learned">
  <meta property="og:title" content="2025.2 Ptg">
  <meta property="og:description" content="Watcher PTG Etherpad
Overview The 2025.2 PTG was the Second PTG i have attended for watcher. The last PTG had one session to cover all the technical debt required to revive the project. This time we extended the ptg to 2 days.
Day 1 Tech Debt Croniter The PTG started with a disscussion about technical debt that emerged during the epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule continuous audits based on cron strings. The Croniter libaray maintainer has decided to stop maintaining this library and it is transitioning to a new maintainer. While that gives watcher some breathing space, our usage is minimal, so we agreed to drop the dependency by leveraging the existing functionality of apscheduler.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-04-15T09:26:03+01:00">
    <meta property="article:modified_time" content="2025-04-15T09:26:03+01:00">
    <meta property="article:tag" content="Openstack">
    <meta property="article:tag" content="Ptg">
      <meta property="og:image" content="https://www.seanmooney.info/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://www.seanmooney.info/images/papermod-cover.png">
<meta name="twitter:title" content="2025.2 Ptg">
<meta name="twitter:description" content="Watcher
PTG Etherpad
Overview
The 2025.2 PTG was the Second PTG i have attended for watcher.
The last PTG had one session to cover all the technical debt required
to revive the project. This time we extended the ptg to 2 days.
Day 1
Tech Debt
Croniter
The PTG started with a disscussion about technical debt that emerged during the
epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule
continuous audits based on cron strings. The Croniter libaray maintainer has decided
to stop maintaining this library and it is transitioning to a new maintainer.
While that gives watcher some breathing space, our usage is minimal, so we agreed
to drop the dependency by leveraging the existing functionality of apscheduler.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://www.seanmooney.info/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "2025.2 Ptg",
      "item": "https://www.seanmooney.info/blog/2025.2-ptg/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "2025.2 Ptg",
  "name": "2025.2 Ptg",
  "description": "Watcher PTG Etherpad\nOverview The 2025.2 PTG was the Second PTG i have attended for watcher. The last PTG had one session to cover all the technical debt required to revive the project. This time we extended the ptg to 2 days.\nDay 1 Tech Debt Croniter The PTG started with a disscussion about technical debt that emerged during the epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule continuous audits based on cron strings. The Croniter libaray maintainer has decided to stop maintaining this library and it is transitioning to a new maintainer. While that gives watcher some breathing space, our usage is minimal, so we agreed to drop the dependency by leveraging the existing functionality of apscheduler.\n",
  "keywords": [
    "openstack", "ptg"
  ],
  "articleBody": "Watcher PTG Etherpad\nOverview The 2025.2 PTG was the Second PTG i have attended for watcher. The last PTG had one session to cover all the technical debt required to revive the project. This time we extended the ptg to 2 days.\nDay 1 Tech Debt Croniter The PTG started with a disscussion about technical debt that emerged during the epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule continuous audits based on cron strings. The Croniter libaray maintainer has decided to stop maintaining this library and it is transitioning to a new maintainer. While that gives watcher some breathing space, our usage is minimal, so we agreed to drop the dependency by leveraging the existing functionality of apscheduler.\nSupport status of datasources The second topic of the ptg focused on the support status of datasources and features of watcher in general.\nAs part of this discussion we touched on the idea of multiple levels of support:\nCore features (supported, tested in ci and ready for production use) Experimental features (supported but not tested in CI or may not be suitable for production use) Deprecated features (features that are no longer maintained and not suitable for production use) Unsupported features (features that are not supported by the project, but may work) During this session we agreed to deprecate the support for the following features:\nMonasca Grafana Noisy Neighbour Strategy (l3 cache) Monasca support was dropped because it is no longer maintained and is officially being retired.\nGrafana support was dropped because it is no longer maintained and has no existing ci coverage under our new support levels it could be considered experimental as it may work under certain conditions however since there is no ci and no active development it was agreed that we should drop support for this feature.\nThe Current Noisy Neighbour Strategy (l3 cache) was deprected because it depends on metrics that are nolnger avialable for several years. a dedicated session on how to replace it was held later in the ptg to discuss how to replace it going forward.\nFinally we wrapped up this topic by agreeing that we should create a support and testing matrix for watcher takeing inspiration from https://docs.openstack.org/cinder/latest/reference/support-matrix.html or https://docs.openstack.org/nova/latest/user/support-matrix.html The intent is to provide a way for users to see which features are supported, tested and suitable for production use cases.\nEventlet Removal The final technical debt topic was Eventlet removal. Watcher like many other OpenStack projects have been using eventlet since its inception, and it has become clear that eventlet is no longer sustainable. While no concrete proposal was made for this topic, to start evaluating the removal this cycle with some initial POCs and aim to remove eventlet in 2026.1\nWorkflow/API Improvements SKIPPED status for actions This discusstion focused on the introduction of a new Skipped status for actions. Today actions can be in one of the following states:\npending ongoing succeeded failed When constructing and executing an action plan it would be useful to extend the action states to introduced a skipped state, which would allow users to mark an action as skipped, preventing it from being executed by the engine. The idea is that this can be used to skip actions that are known to fail or be undesirable to execute. Additionally if an actions preconditions cannot be met, the action can be marked as skipped instead of failed.\nThis lead to a larger discussion about the meaning of SUCCEEDED and FAILED for action plans in general. https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108 We discussed that the current behavior where an action plan is considered succeeded when all actions are attempted regardless of the success of the action is unintuitive and not in line with the state machine docs.\nit was agree that if any action fails the overall action plan should be reported as failed. Additionally in this context we agreed that if the SKIPPED state is added it should be considered as Succeeded, not Failed.\nwatcher-dashboard This sessions was relatively short and focused on 2 areas testing and missing functionality\nTesting We opened the session discussing the fact that the horizon plugin as almost no testing. the net effect of this is every change need to be manually verified by the reviewer. We have two options with regard to testing we can build on django’s unittests integration to validate the core logic, or we could build out selenium testing. no general conclusion was arrived at but we resolved to follow up with the horizon core team in the future for guidance.\nExposing parameter in audits This was simple and non controversial, today when creating a audit its not possible to set the parameters filed via the horizon ui. we agreed this should be addresses as a speechless blueprint.\nSKIPPED status finally we discussed that if we extend the action status to model the new SKIPPED state we will need to enhance the action plan dashboard to allow skipping an action and to display the skipped state when a precondition fails.\nWatcher cluster model collector improvement ideas This was arguably another tech debt discussion in the guise of an operator pain point. For performance reasons watcher is build with a cached data model that periodically refreshes, combined with integration with notification for near real-time updates.\nDuring the session we reviewed https://bugs.launchpad.net/watcher/+bug/2104220 and resolved to investigate if we are properly consuming the notifications form nova. While we are consuming the notifications bug #2104220 asserts that we are perhaps using the wrong fields to update the instance host on live migration resulting in the source host being updated instead of the destination host.\nother mitigation were discussed such as adding the ability to force refresh the model when executing an audit, creating a new audit type to just run the refresh, or adjusting the collector interval.\nDay 1 Summary Day one was pretty packed but we resolved to finish early when we reached the end of the agenda.\nWe resolved to proceed with 2 new specs for the SKIPPED sate and model collector plugins and to treat the parameter enhancement to watcher-dashboard as a specless blueprint. croniter will be replaced as a bug fix and we need to review the impact of bug: #2104220\nDay 2 Watcher and Nova’s visible constraints Day 2 kicked off where day one ended on the topic of how watcher models nova instances and the visibility of scheduler constraints. In the epoxy cycle we added a number of attributes to the server show response that may be of interest to watcher, namely the scheduler hint and image properties. additionally i noted that in a prior release we added the pinned AZ as a separate filed. while extending the data model to provide this info would not be an api change we discussed that a spec could be nice to have to document the changes but a specless blueprint could also be valid.\nNoisy neighbour strategy This discussion happend as a sperate session on the second day but we touched on it on day 1 as part of the tech debt session. The TL;dr is since the exisitng noisy neighbour policy is non functional and since we want to aovid upgrade impacts a new SLA goal with 1 or multiple strateies shoudl be created. A spec to define the semantics should be created but at a high level the open questions are\nshould we have one stagey per metric or a single parmaterised stagey which metrics shoudl we use as the key performance indicators to comptue if the sla is treshold is exceeded cpu_steal io_wait cache_misses others? finally we agreed to deprecate the existing strategy this cycle for removal in 2026.2.\nHost Maintenance strategy new use case The last feature session discussed the limitation with the current host maintenance strategy the main limitations resolved around how to express if a instance should be stopped, cold migrated or live migrated. we agreed to continue with the spec https://review.opendev.org/c/openstack/watcher-specs/+/943873 and that it was ok to add new parameters to allow specifying which actions are allowed for the audit.\nThis topic was reprised in the nova room on day 4 TL;DR in the nova session we agreed to try to use server metadata to encode policies like live migratable. this can be incorporated into the spec design as a follow up if there is time. i.e. instance can be annotated by adding lifecycle:livemigratable=True|False and similar lifeccyle: metadata keys to express which actions may be taken by an external orchestrator like watcher.\nPTG wrappup contributor docs The second to last session was a quick discussion of some of the document that watcher is missing namely the scope of the project doc and the chronological release guide. these are both important document for use to create going forward, as is ensuring our existing contributor docs are up to date regarding when to file a bug vs blueprint vs spec.\nretro and adhoc planning To wrap up the ptg we had a short retro on how the last cycle went and any last minute topics. traditionally one start the PTG with the retro rather then finishes but its better late then never. we realised that one topic had not been raised which is the future of the core team we briefly discussed that going forward we will review the core team member ship at the start of each slurp release (i.e 2026.1) and remove inactive cores that have not participated in review, contribution or ptgs since the prior slurp. As such no removal form the core team will be made for 2025.2 although new addition may be made based on review activity over the cycle.\nWith that we we wrapped up the ptg with a reminder of our two cross project sessions with telemetry/horizon on day 3 and nova on day 4\nTelemetry/Horizon Context This ptg unlike many others i attended a combined telemetry/horizon cross project session lead by Victoria Martinez de la Cruz (vkmc). There we discussed a number of topics related to observability and how to provide tenant and admin static dashboards to visualise metrics.\nInitially we started by discussion why observability and metrics are distinct form the existing views provided by the admin hypervisors dashboard and how they are distinct form showback/chargeback provide by cloud-kitty\nshowback/change-back focus on billing and the ratings are base not on the utilisation of the provisioned resource but rather then allocation. i.e. you are billed equally for an idle vm that uses 4 cpus and 16G of ram vs a fully loaded vm. cloud kitty can help tell you how large your bill will be but it will not tell you if your application can run with half the resources. A observability view based on near real-time metrics aims to solve the latter problem.\nThe hypervisor dashboard is also related but distinct. the hypervisor view is intended to provide a semi real-time view of the capacity of a could for admin to be able to plan if they need more capacity. Again this view looks more at the available resources and the allocated resource but does not provide an overview of the utilisation.\nPOC An initial poc of what the dashboard could looks like is available here https://github.com/SeanMooney/grian-horizon-plugin proposal create a new horizon plugin as an official deliverable of the telemetry project in the governance repos this will be hosted in the https://opendev.org/openstack namespace and will be released as an official deliverable via the release repo we need to spike on d3.js vs rickshaw vs chart.js for charting and assess the version compatible and tech debt initially the new metrics dashboard will integrate with Prometheus as a metrics store. a fake backend may be created for testing, other backends are out of scope but could be added later. a combination of new panels and tabs in existing panels will be added. initially all dashboards will be static and direct quires to the backend will not be supported this will allow the plugin to provide multi tenancy as required in the absence of multi tenancy support in Prometheus. the plugin will be stateless with the time span that can be viewed determined by Prometheus. the design goal would be to support short term metrics of up to 7 days. the python-observability client will be used to query Prometheus. Nova The nova ptg was busy as always, i was also sick all week and had conflict with watcher/horizon so i was not able to attended all the sessions so ill focus on some of important topics form my perspective instead of trying to do a full summery. for those looking for a full summery here is a excellent write up Rene’s Official PTG Summary, i will also not cover the eventlet removal topic in detail as gibi has a write up of that here Eventlet removal - Flamingo PTG\nDAY 1 day 1 was a short day to allow for some cross project sessions to happen before nova officially started. as is normal we started the ptg with a retrospective on the past cycle, what worked well and how we wanted to proceed.\nproject planning In general not much has changed for this cycle but ill just highlight some important dates Timeline:\nSoft spec freeze (no new specs): June 1st Hard spec freeze (M2): July 3rd Feature Freeze (FF): August 28th Final release: late September / early October For those not familiar with the difference between the soft and hard spec freeze the soft freeze is the last date we ask contributors to propose large pieces of work. it serves as a signal that if your topic is complex and has not already been socialised and reviewed by that point it likely wont be mergeable by the hard freeze. This basically creates a buffer before people tend to disappear for PTO or others factors that might make getting a quorum difficult.\nThis worked well for us in Epoxy when we set the soft spec freeze in early December to acknowledge that we would loose quorum between late December and early January.\npython 3.13 and eventlet This are related but distinct topics. This cycle we would like to start early testing with python 3.13 as Ubuntu 25.04 has now moved to 3.13 and i believe Debian 13 may also. unfortunately monkey_patching the tread lib breaks on 3.13 so eventlet based service cant actually run on 3.13 under devstack unit test jobs seam to pass so that a start. we agreed to add a functional-py313 job if we can get it to pass however we do rely on eventlet there so that may or may not be possible.\nEventlet removal will be one of nova’s priories for the 2025.2 cycle as we acknowledged we are behind were we should be at this point in time. for those wanting to follow this progress we are going to track it as a recurring slot in our IRC meeting and gibi is planning to document the progress in there blog, the first post is already live!\nDay 2 day 2 kicked off with some security related topics and finished up with live migration discussions\nSEV TDX and arm CCA Confidential computing is not new by any means even in nova with support for SEV being available for many years. Last cycle tkajinam had started to make improvement in SEV-ES support in advance of the enablement of SEV-SNP, unfortunately we did not have review bandwidth to land there enhancements. We resolved to prioritise the review of there work and also discussed the new development that are happening on intel and arm platforms. TL;DR the kernel, qemu and libvirt work is not complete for both TDX and CCA at this time and in the arm case there still isn’t hardware in production. effectively its too early to actually add support for arm CCA support and its likely to be a 2027.1 topic based on Ubuntu 26.04+ intel TDX is closer but also likely to early to supprot in 2025.2\nPQOS (platform quality of service) This is and old one. so like 7 years ago when i was still at intel there was an effort to support intel cache and memory bandwidth both statically in nova (pushed by myself) and using an external resource management daemon call RDT. both effort die out but in the last month or two people have started asking about it again.\ni was not able to attend this session but it seam like the new contributors are going to pickup my old spec and re-propose it for 2025.2 with updates. Since i wrote that in 2019 the way intel CMT and MBT work has been simplified to some degree so they will update the spec to reflect that and we will review.\nWhile this may be useful for some real-time workloads that are latency sensitive, its non trivial to add, complex to configure and likely not a generically useful outside a select number of workloads. This will be on to monitor but likely not a feature that will be used in many clouds. for those that need it however it could be a big improvement.\nlive migration topics vtpm TL;DR vtpm live migration is our largest gap in this area we briefly discussed later in the ptg that the pattern for vTPM secret handling will also inform how we address the same challenges for cinder encrypted volume live migration and local nova storage encryption when we eventually get back to that. The big change in our plan for vTPM is that this work will be taken over by another contributor and artom will work with them to hand off this work early in the cycle.\npci/vGPU live migration follow ups In epoxy nova added support for vfio variant drivers and with that support for pci devices that\nEnable VFIO devices with kernel variant drivers Live migrate VFIO devices using kernel variant drivers During the ptg we discussed what could not be completed and the divergence of the code paths between flavor based pci pass-though live migration and neutron SR-IOV port live migration.\nThe big takeaway form this session was we will address https://bugs.launchpad.net/nova/+bug/2103631 in two steps. First we will move the check before the conductor calls the scheduler. this is a backporable fix and will provide most of the optimization benefit by entirely eliminating the placement and scheduler work. Second we will move the check to the api and return a 409. this will fully optimize the early exit but it will not be as backportable. While the api can already return a 409 https://docs.openstack.org/api-ref/compute/#live-migrate-server-os-migratelive-action chaning a 202 to a 409 is not always considered valid for backports.\nDAY 3 day 3 was perhaps the most eclitic of topics with everything form confirming we will complete previous work started in epoxy to discussion of eventlet removal and cross project session with glance/cinder .\nVDI enhancements, os-traits and one-time-use devices VDI and one time use devices were short as the first was a carry over form work that was approved form epoxy but didn’t merge in time and one time use devices merged the week before the ptg :) VDI in openstack is a somewhat complex topic. on one hand we know that there are a number of gaming companies that host there server side infrastructure on openstack but more recently with the rise fo cloud game streaming companies there are now a new type of nova user that use both GPU pass-though and spice fundamentally to delver high performance vdi solutions to there customers. Spice and to a similar degree RDP are both better positioned to meet the semi real-time demands of cloud gaming then VNC ever will be but its also an area of nova that until recently has not had much development in a very long time.\nboth however hit a common pain point, how we use and release os-traits so we spent a while discussing possible path forward to streamline the work flow. fundamentally we did not resolve to actually make those changes but absorbing os-traits and os-resource-class back into placement but we generally agreed that we could do that if we happen to find the time.\none time use devices also sparked a discussion on a related placement bug and its fix this could be seen as controversial but we all agreed that the existing behaviour is wrong and that we shoudl fix it. Dan presented there solution and we generally agreed that it seam like a reasonable approach and that we should proceed with the review. For operators resolving the current limitation that placement Allocations cannot be adjusted when provider is over capacity will transitively fix 3 separate nova bugs all rooted in this underlying limitation.\nThis session was also very useful as John Garbutt reminded us that we used the set reserved=total trick to fix a race condition with cleaning in ironic. i.e. for ironic nodes we set reserved=total when we provision a node and reserved is only reset after cleaning is complete. So while this is undocumented behaviour its not undefined and we now have 2 users of it so we probably shoudl test this in placement properly and update the api ref to document it.\ntech debt oenstack client support last cycle was a departure form previous cycles when we actually had multiple api changes. unfortunately all 4 api change merged late and none of them landed the sdk and osc change required to fully complete the features. manila share support spice direct Show Scheduler Hints in Server Details image properties in server show\nWhile this is a low-light all of the above have patches in flight and should land early this cycle. This topic was actually split between this session on Wednesday with the main topic discussion happening on Thursday with the client/sdk teams and QA teams for the tempest impact.\nglance/nova/cinder corss project unfortunately i could not attend this due to a conflict but the main topics covered were captured in the glance ptg summery the two most impoant topics in the nova context were\nnew location API (Cinder \u0026 Nova) Glance has introduced two new Location APIs in the Dalmatian cycle. We can use these APIs to address OSSN-0090 and OSSN-0065. Patches for Nova and Cinder must still be merged, hopefully during the Flamingo cycle.\nfreeze glance client development Cinder and Nova will need to use the OpenStack SDK instead of the glanceclient. There is no need to complete this work during this cycle, but we should at least have a good idea of what APIs are currently being used, so that we can have a plan for the next cycles.\nHow nova store image properties in our db you may wonder why this is not relevant to the glance discussion it is but this is really internal to nova usage of image properties specifically not storing them if we don’t need them. There is a lot of history to this topic but the clips note version is nova stop supporting custom image properties more than a decade ago but our persistence of them in our db was a little leaky see: https://bugs.launchpad.net/nova/+bug/2098384 for more details. We agreed that moving forward we should not store image metadata keys in our db that are not part of the standard set and we should plan to provide a way to clean up the stale entries that already exist.\nFinalizing the secure RBAC goal the TL;DR of this is the implementing of the manager role and service roles in our policies started a while ago but unfortunately gmann did not have time to work on it in Epoxy until late in the cycle and we didn’t have time to review it when they got unblocked. so in 2025.2 we would like to prioritise completing this work which is the final work remaining for nova to complete the SRBAC community goal.\nEventlet removal i wont go into this in detail because gibi already wrote up a better summery but This was the second session on eventlet in the context of nova (the first being the community one lead by oslo). Here we discussed the concrete next steps for nova both in terms fo what to do for 2025.2 and beyond. The important take always are we will track the progress of this goal weekly in our meetings, gibi and kamil will work to incrementally remove our direct eventlet imports throughout the cycle and try to re-implement scatter gather in the api so that it can be the first nova service to run without eventlet. This is probably the largest change in novas implementation since we added python3 support or the initial move form twisted to eventlet in the very early days. As a community we have 12-18 months to complete this transition and the nova core team committed to prioritise this work going forward.\nDay 4 as with day 3 there were many different topic raised mainly in the context of new features combined with cross project session with neutron, tempest and client/sdk teams.\nA QOS api for nova This was and is likely the most contoverial topic that was raised during the entire ptg bar providing hooks for the XML which was a straight -3 as that is never going to happen. during this session we discussed a proposal that is documented here\nthis topic not only involved nova but also cinder and neutron changes. in essence this requires fundamentally changes to how nova neutron and cinder work internally and together.\nNova does not supprot changing the QOS applied to neutron Port or cinder volume while the resource is attached to nova instance. on the neutron size if the QoS policy is a min bandwidth or PPS policy, changing the QOS on a port can invalidate the placement of the VMs breaking how those features work.\nFrom a nova point of view if you allow the qos policy on a neutron port to change in neutron while its attached to a nova instance it is therefor a bug.\nCinder and nova have a similar impedance mismatch in that cinder defines it qos policies on the volume type not on the volume. that means that changing the qos on a volume type would impact many volumes nova has not way to support that today. To make matters worse operators already abuse the fact that the fronted qos is updated on live migration to apply these changes live to existing instance even though nova has no idea this is happening and that might fail.\nThis was a somewhat challenging session as the topic was brought by an operator that already designed how they wanted it to work and were asking for use to just accept the changes even though there design is not really compatible with how self service multi tenant cloud should work.\nwe spend a lot of the session discussing a possible future nova api for QoS similar to neutrons i.e. where a operator can define a set of QoS policies (effectively a sub set fo flavor extra specs) which an end user could select form and that an operator could apply to a project by default.\nwhile that may be a valid evolution of novas api it touch on previous landmines like composable flavors. we will also need to carefully think about the RBAC implication of who should be able to set a QoS policy on a Project or instance level and if it shoudl be settable via a Flavor. For example the manager role likely shoudl be able to set it on a project level if and only if an admin has not set it. admin likely should be able to set a qos policy in a flavor but when not set a person with the member role should be able to select form any of the QoS policies defined by an admin as they can in neutron today.\nwhile we tired to convey some of this feedback i feel like they did not internalise that well. we also provide context to them on previous discussion form 2019 https://etherpad.opendev.org/p/r.23a243a5860c7406917f86f48cfb4491#L389 during the shanghai ptg.\nthe proposer resolved to follow up with each team separately and file a spec for the works but we will have to wait and see if took on board our feedback or not.\nOVN live migration who would have guessed that its still a pain… we discussed https://bugs.launchpad.net/nova/+bug/2073254 in Thursday and then again on Friday the TL;DR is neutron now send the event at the right time so we can remove the hacks we added 5 years ago so i have proposed https://review.opendev.org/c/openstack/nova/+/946950 to address that.\nWe also discussed that we may want to move the creation of the tap to os-vif instead of libvirt. https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1174 This likely should only be done if neutron asked us too but we can make progress even without this.\nWatcher topics This was the main topic i added this PTG to discuss how we could evolve nova to better support watcher use-cases. In this session we discussed how we can annotate instance with metadata to inform policy. i.e. can this instance be live migrated. the second topic was how can nova tell us if a vm can be migrated or if other lifecycle operations are valid like cold migration or shelve. the third topic was slightly different on the usefulness of weighing host by traits with two different use cases, preferred/avoided traits via flavor and automatically avoiding host with *_pressure traits.\nannotating instances with metadata The proposal to add a new api for instance annotation was deferred/rejected in favor of using the existing instance metadata for this use-cases. the proposed set of metadata pairs are\nlifecyle:evacuatable=true|false lifecyle:cold-migratable=true|false lifecyle:shelvable=true|false lifecycle:premetable=true|false ha:maintance-stragey:in_place|power_off|migrate ha:role=primary|secondary ha:priority:[string|number] these can be documented by the service that uses them but we may want to maintain a list of useful metadata keys like the glance useful image properties doc in nova if they are used. We can/should document that nova considers this a valid use-case for metadata provided the user applies the metadata themselves.\nnote: on Friday we had related discussion about admin metadata which ill cover later.\ninstance capabilities we might work on the name because i will never spell that properly :) this we said we should do and i should write a spec during the session artom mention that in addition to the lifecycle annotations above another capability could be resume on host reboot. after the discussion i realized there are some other lifecycle operation like suspend, pause and snapshot that might make sense to report.\nso the full set would be\nlifecyle:evacuatable=true|false lifecyle:cold-migratable=true|false lifecyle:shelvable=true|false lifecycle:premetable=true|false lifecyle:resume_on_host_reboot=Ture|False lifecyle:pause=true|false lifecyle:suspend=true|false lifecyle:rescue=true|false lifecyle:snapshot=true|false the idea of instance capabilitiesis that nova should know what operations are valid to perform on an instance based on the virt driver, flavor, image and other project resources like manilla shares, cinder volumes, neutron ports, and cyborg accelerators.\nThis will need a spec to proceed but the general direction is seen as positive.\n*_PRESSURE traits and trait based weighing Firstly we started this conversation by asserting that if the Trait is custom nova will not touch it. So any human or service could annotate nova resource providers with CUSTOM_ traits\nsecond i asserted i would like to have Standard traits for CPU RAM and DISK pressure\nPRESSURE_CPU PRESSURE_MEMORY PRESSURE_DISK network pressure could be added late btu its not one of the one reported by the kernel today https://docs.kernel.org/accounting/psi.html#pressure-interface\none of the open question i had for this session is would nova be ok with the libvirt driver reporting these traits or should watcher annotate host with them based on a periodic audit. the core team was generally ok with the libvirt driver doing this but we will confirm that as part of a spec.\nThe second half of this topic was focused on how these newly reported traits when placing vms\nThe proposal create 1-2 new weightier to way based on traits.\nfor the *pressure traits we likely want to aovid them by default however there may be other traits that an operator may want to avoid or prefer. so that implies that there should be a config option. we decide to punt to the spec/implementation if we wat to include the pressure traits by default or not.\nthe second weigher would be a flavor based weigher that would extend the existing required/forbidden syntax to supprot preferred and avoided\nboth weigher will rely on a common change to the host state object to pass the provider summaries this will allow filters and weighers consider traits and available resource classes.\nnote i did not discuss this in the session but this would also allow me to introduce what i have called in the past a boring host weigher. the concept of the boring host weigher is simple prefer host with fewer traits and resource class or generally a simpler provider tree. The logic, simpler the provider tree the less fancy the hardware available on the host and there for its less likely that placing a random vm on the host will prevent a future vm that need one of the special aspect of the host form fitting.\nWe agreed that while these are all nice to have and in scope of nova we will propose concrete next steps in a spec if/when we work on them.\nOpenApi Stephen made massive progress last cycle in nova but we ran out of review bandwidth they summerised there progress in general across openstack in a recent blog post An Update on OpenAPI in Openstack during the ptg session we resolved to try and continue making progress on this as nova is in a good position to complete this work. we also discussed that once this work is complete and nova has both request and response schema for all apis we could remove the tempest schema validation in a later release. This would be a nice reduction in technical debt for tempest but will need to wait until all stable/* release of nova support open API validation.\nclient changes we covered this earlier but Stephen also lead a discussion on how can we work better between the nova and client/sdk teams to make sure we complete the client work when we make api changes other then not merging them really really late which we should avoid anyway we mainly just reiterated that direct pings are ok and actually welcome when we thing a change is ready to merge. in other word reach out when we are close and use depend-on ectra to call out the deps on the api changes.\nTesting, testing and more testing i wont spend much time on this but interleaved with other topics we discussed how to improve testing in tempest, and can we finish the efforts to []emulate hardware in ci](https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L859) to test OTU devices/pci passthough, ndd generic mdevs as well as maybe test sr-iov via IGB down the road.\nthe answer is yes, we can and should update the spec template to note that at a minimum we shoudl update the tempest schema tests when we add a new api. dan has started a effort to test pci passhtough as part fo testing OTU device using the virtio-RNG. sylvain Will revive there mtty series to create a proxy for generic mdevs.\npower management next steps the final session of day 4 was a deep dive on where power management is, the original design goals and what shoudl we do to fix it. Actually that a lie we did this out of order and talked about this before the OpenAPI and client topics but I’m going to bottom on the etherpad so we will pretend this was the last sessions :)\nthe over all outcome fo this session was that we shoudl not rush to power off the cores on startup and should wait for the resource tracker to be populated with instance so we can ensure we never power off cores related to an instance that is running. second while we could modify the virt driver interface to allow the comptue manager to defer the power off we should explore if we can just internalise this entirely in the libvirt driver first. if we cant then virt driver change it is. in other words don’t rush to power off the core until we are sure we dont need to power them back up and that they are not in use.\nDay 5 live migration with cinder encrypted volumes This does not work today for exactly the same reason as vtpm migration. admins do not have access to secrets owned by users. The fix, learn lessons form how we fix this for vtpm and apply the same to cinder volumes. specifically the only way we can fix this is the host policy https://specs.openstack.org/openstack/nova-specs/specs/2025.1/approved/vtpm-live-migration.html#id2\ncinder volume secrets unlike vtpm are not delete form libvirt when we boot the vm. so we could copy them between hosts once we implement that for vtpm. we already depend on the fact the secret is available in libvirt for resuming guest with encrypted volumes on host reboot.\nManila cross project session now that you can attach a share to a nova instance we took time in this ptg to reflect on the next steps. There are many but in priority order they are the following:\nfinish the osc support and backport to epoxy testing (tempest, functional testing elsewhere) memfd memory type in nova (possible by default) online attach/detach attaching a share during server create live migration (still need to have this fully supproted in libvirt/qemu) that is a lot of future work but each is incrementally doable and we resolved to have specs as need for each item\nprovider.yaml enhancments The main topic of this session is simple. today you can add custom traits via provider.yaml if you later remove them from the file now will not remove it form the resource provider… it should do that.\nso we discuss a few ways to address that and i suggested that nova could copy the file to its state Dir .i.e. /var/lib/nova/last_applied_provider.yaml on the next restart nova could then compare the traits in the last_applied_provider.yaml to the current provider.yaml and compute if any trait shoudl be removed. nova would then up copy the new provider.yaml to last_appled_provider.yaml if it successfully updated placement.\nthe proposer will do a poc and write up a spec or specless blueprint for this. in the future we could support other changes like the ability to remove resource provider created via provider.yaml but that will initially be out of scope.\ninstance metadata protection This was another very productive session, the use case that was presented is as follows, I as a cloud operator have some knowledge of my infrasture that i want to apply to an instance, i would like to make this query able via the rest api, to that end i would like to annotate the instance with this metadata but i would like to be able to prevent a normal user form removing this metadata.\nthe discussion started with there proposal to have a config option to define metadata protection sort of like how you can define protect image properties that only an admin can set in glance.\nthe main problem with this approach is interoperability between clouds. we then discussed the idea of defining an admin prefix and perhaps a manager too allow user with those roles to set keys the member or reader users cannot modify. while this would work it woudl require any exiting metadata that an operator might be setting to be updated.\nfinally we discussed how the lock api and the fact that we record the role of who locked the instance. our general recommendation was to create a spec to allow recording the role of the user that set metadata and to prevent lower privileged roles from removing keys set by higher privileged roles. that would define the following prescience relationship admin \u003e manager \u003e member\nwe said we woudl be happy to review a spec to that effect if they write one.\nupdate/set delete on terminate for neuton port This was short, we should supprot it but we just have not had a contributor to work on it so patches are welcome. we suggested starting with a poc and filing a spec once they are happy they have something working\n[Graceful shutdown] (https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089) this was another rapid fire topic. we explained the previous attempt and the fact we currently don’t have capacity to work on this.\njohn raised an interesting alternative which would be to add a maintenance mode concept. this could be implemented as a block in the api of all write operations alternatively we could decorate all rpc functions that start an instance action or move op with a decorator that would reject the rpc with a 409 if the compute service was set to maintenance mode either as a new sate for disabled or as a new filed.\nwe all agree that a more graceful shutdown would be nice to have and a poc of different approaches might be a good next step currently the core team does not have time to work on it but we welcome other to propose solutions. The main call to action is to report any bugs for problem we know of when there is not a graceful shutdown while we may or may not be able to fix them that will provide a list of edge cases that need to be solved for.\niothread and block device queue Again this is a topic that was started on in the past and stalled out. there has been previous resarch done by redhat and other on how virt-blk scales with io vs iothread vs queues https://developers.redhat.com/articles/2024/09/05/scaling-virtio-blk-disk-io-iothread-virtqueue-mapping\nthis session basily was quick and after some discussion fo the history and previous recommendations of our virt team to always have at least 1 iothread we agreed that the propose should write a spec to introduce 2 new flavor extra specs/image properties to be able to request the number of iothread and block device queues. .e.g hw:io_threads=4 hw:blk_multiqueue:2\nthe detail will be covered in the the spec but when unset we should likely always allocate 1 iothread and preserve the libvirt default of creating one virtio-blk queue per vcpu.\nfiltering server list by neutron network the final large topic of the ptg was a proposal to filter server list by neutron network (possibly also subnet in the future)\nthe general problem to day is while you can make a single query to get all port device-id values (the nova instance the port is attached too) for all ports on a neutron network, you cant then call nova with 100s of uuids in one query to list the server details. the proposer uses this information to determine the owner of the vm so that when they need to do maintenance or a particular network runs out of ips they can reach out to there customer and ask them to move a port or release an ip ectra.\nmost of this session revolved around is this in scope of nova to do or not.\nin the end we resolved that if the data is in the nova db and easy to filter on or we can make a efficient query to another service that a user cant replicate because they cant pass the resultant uuid to nova for list, then its ok to extend server list to do this.\nwe will only add filter on a case by case basis. as a result we agreed that if the proposer writes a spec and works on a poc we will review them.\nPTG summary. a lot happened in nova and watcher this PTG and more happened in the TC and other project session that i have not covered at all. hopefully someone will find this useful. i have tried to summarize my understanding of all the sessions i attended and provide links to the sources where possible.\nwe got a lot done in what was a total of maybe 20 hour of real-time meetings but we equally now have alot of work to do to achieve all we discussed including many specs to read and write.\nI woudl personally like to thank rene for running the nova sessions and all the rest that contributed there time. it was a long, tiring and stressful week for many but it was also good to see us disucss these topics as a community in the open.\n",
  "wordCount" : "7411",
  "inLanguage": "en",
  "image": "https://www.seanmooney.info/images/papermod-cover.png","datePublished": "2025-04-15T09:26:03+01:00",
  "dateModified": "2025-04-15T09:26:03+01:00",
  "author":{
    "@type": "Person",
    "name": "Sean Mooney"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.seanmooney.info/blog/2025.2-ptg/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Today I learned",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.seanmooney.info/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.seanmooney.info/" accesskey="h" title="Today I learned (Alt + H)">Today I learned</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.seanmooney.info/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.seanmooney.info/">Home</a>&nbsp;»&nbsp;<a href="https://www.seanmooney.info/blog/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      2025.2 Ptg
    </h1>
    <div class="post-meta"><span title='2025-04-15 09:26:03 +0100 +0100'>April 15, 2025</span>&nbsp;·&nbsp;35 min&nbsp;·&nbsp;Sean Mooney&nbsp;|&nbsp;<a href="https://github.com/SeanMooney/seanmooney.info/tree/master/content/blog/2025.2-ptg.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#watcher" aria-label="Watcher">Watcher</a><ul>
                        
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#day-1" aria-label="Day 1">Day 1</a><ul>
                        
                <li>
                    <a href="#tech-debt" aria-label="Tech Debt">Tech Debt</a><ul>
                        
                <li>
                    <a href="#croniter" aria-label="Croniter">Croniter</a></li>
                <li>
                    <a href="#support-status-of-datasources" aria-label="Support status of datasources">Support status of datasources</a></li>
                <li>
                    <a href="#eventlet-removal" aria-label="Eventlet Removal">Eventlet Removal</a></li></ul>
                </li>
                <li>
                    <a href="#workflowapi-improvements" aria-label="Workflow/API Improvements">Workflow/API Improvements</a><ul>
                        
                <li>
                    <a href="#skipped-status-for-actions" aria-label="SKIPPED status for actions">SKIPPED status for actions</a></li></ul>
                </li>
                <li>
                    <a href="#watcher-dashboard" aria-label="watcher-dashboard">watcher-dashboard</a><ul>
                        
                <li>
                    <a href="#testing" aria-label="Testing">Testing</a></li>
                <li>
                    <a href="#exposing-parameter-in-audits" aria-label="Exposing parameter in audits">Exposing parameter in audits</a></li>
                <li>
                    <a href="#skipped-status" aria-label="SKIPPED status">SKIPPED status</a></li></ul>
                </li>
                <li>
                    <a href="#watcher-cluster-model-collector-improvement-ideas" aria-label="Watcher cluster model collector improvement ideas">Watcher cluster model collector improvement ideas</a></li>
                <li>
                    <a href="#day-1-summary" aria-label="Day 1 Summary">Day 1 Summary</a></li></ul>
                </li>
                <li>
                    <a href="#day-2" aria-label="Day 2">Day 2</a><ul>
                        
                <li>
                    <a href="#watcher-and-nova" aria-label="Watcher and Nova&rsquo;s visible constraints">Watcher and Nova&rsquo;s visible constraints</a></li>
                <li>
                    <a href="#noisy-neighbour-strategy" aria-label="Noisy neighbour strategy">Noisy neighbour strategy</a></li>
                <li>
                    <a href="#host-maintenance-strategy-new-use-case" aria-label="Host Maintenance strategy new use case">Host Maintenance strategy new use case</a></li>
                <li>
                    <a href="#ptg-wrappup" aria-label="PTG wrappup">PTG wrappup</a><ul>
                        
                <li>
                    <a href="#contributor-docs" aria-label="contributor docs">contributor docs</a></li>
                <li>
                    <a href="#retro-and-adhoc-planning" aria-label="retro and adhoc planning">retro and adhoc planning</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#telemetryhorizon" aria-label="Telemetry/Horizon">Telemetry/Horizon</a><ul>
                        
                <li>
                    <a href="#context" aria-label="Context">Context</a></li>
                <li>
                    <a href="#poc" aria-label="POC">POC</a></li>
                <li>
                    <a href="#proposal" aria-label="proposal">proposal</a></li></ul>
                </li>
                <li>
                    <a href="#nova" aria-label="Nova">Nova</a><ul>
                        
                <li>
                    <a href="#day-1-1" aria-label="DAY 1">DAY 1</a><ul>
                        
                <li>
                    <a href="#project-planning" aria-label="project planning">project planning</a></li>
                <li>
                    <a href="#python-313-and-eventlet" aria-label="python 3.13 and eventlet">python 3.13 and eventlet</a></li></ul>
                </li>
                <li>
                    <a href="#day-2-1" aria-label="Day 2">Day 2</a><ul>
                        
                <li>
                    <a href="#sev-tdx-and-arm-cca" aria-label="SEV TDX and arm CCA">SEV TDX and arm CCA</a></li>
                <li>
                    <a href="#pqos-platform-quality-of-service" aria-label="PQOS (platform quality of service)">PQOS (platform quality of service)</a></li>
                <li>
                    <a href="#live-migration-topics" aria-label="live migration topics">live migration topics</a><ul>
                        
                <li>
                    <a href="#vtpm" aria-label="vtpm">vtpm</a></li>
                <li>
                    <a href="#pcivgpu-live-migration-follow-ups" aria-label="pci/vGPU live migration follow ups">pci/vGPU live migration follow ups</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#day-3" aria-label="DAY 3">DAY 3</a><ul>
                        
                <li>
                    <a href="#vdi-enhancements-os-traits-and-one-time-use-devices" aria-label="VDI enhancements, os-traits and one-time-use devices">VDI enhancements, os-traits and one-time-use devices</a></li>
                <li>
                    <a href="#tech-debt-1" aria-label="tech debt">tech debt</a><ul>
                        
                <li>
                    <a href="#oenstack-client-support" aria-label="oenstack client support">oenstack client support</a></li>
                <li>
                    <a href="#glancenovacinder-corss-project" aria-label="glance/nova/cinder corss project">glance/nova/cinder corss project</a><ul>
                        
                <li>
                    <a href="#new-location-api-cinder--nova" aria-label="new location API (Cinder &amp; Nova)">new location API (Cinder &amp; Nova)</a><ul>
                        
                <li>
                    <a href="#freeze-glance-client-development" aria-label="freeze glance client development">freeze glance client development</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#how-nova-store-image-properties-in-our-db" aria-label="How nova store image properties in our db">How nova store image properties in our db</a></li>
                <li>
                    <a href="#finalizing-the-secure-rbac-goal" aria-label="Finalizing the secure RBAC goal">Finalizing the secure RBAC goal</a></li></ul>
                </li>
                <li>
                    <a href="#eventlet-removal-1" aria-label="Eventlet removal">Eventlet removal</a></li></ul>
                </li>
                <li>
                    <a href="#day-4" aria-label="Day 4">Day 4</a><ul>
                        
                <li>
                    <a href="#a-qos-api-for-nova" aria-label="A QOS api for nova">A QOS api for nova</a></li>
                <li>
                    <a href="#ovn-live-migration" aria-label="OVN live migration">OVN live migration</a></li>
                <li>
                    <a href="#watcher-topics" aria-label="Watcher topics">Watcher topics</a><ul>
                        
                <li>
                    <a href="#annotating-instances-with-metadata" aria-label="annotating instances with metadata">annotating instances with metadata</a></li>
                <li>
                    <a href="#instance-capabilities" aria-label="instance capabilities">instance capabilities</a></li>
                <li>
                    <a href="#heading" aria-label="*_PRESSURE traits and trait based weighing">*_PRESSURE traits and trait based weighing</a></li></ul>
                </li>
                <li>
                    <a href="#openapi" aria-label="OpenApi">OpenApi</a></li>
                <li>
                    <a href="#client-changes" aria-label="client changes">client changes</a></li>
                <li>
                    <a href="#testing-testing-and-more-testing" aria-label="Testing, testing and more testing">Testing, testing and more testing</a></li>
                <li>
                    <a href="#power-management-next-steps" aria-label="power management next steps">power management next steps</a></li></ul>
                </li>
                <li>
                    <a href="#day-5" aria-label="Day 5">Day 5</a><ul>
                        
                <li>
                    <a href="#live-migration-with-cinder-encrypted-volumes" aria-label="live migration with cinder encrypted volumes">live migration with cinder encrypted volumes</a></li>
                <li>
                    <a href="#manila-cross-project-session" aria-label="Manila cross project session">Manila cross project session</a></li>
                <li>
                    <a href="#provideryaml-enhancments" aria-label="provider.yaml enhancments">provider.yaml enhancments</a></li>
                <li>
                    <a href="#instance-metadata-protection" aria-label="instance metadata protection">instance metadata protection</a></li>
                <li>
                    <a href="#updateset-delete-on-terminate-for-neuton-port" aria-label="update/set delete on terminate for neuton port">update/set delete on terminate for neuton port</a></li>
                <li>
                    <a href="#graceful-shutdown-" aria-label="[Graceful shutdown] (https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089)">[Graceful shutdown] (https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089)</a></li>
                <li>
                    <a href="#iothread-and-block-device-queue" aria-label="iothread and block device queue">iothread and block device queue</a></li>
                <li>
                    <a href="#filtering-server-list-by-neutron-network" aria-label="filtering server list by neutron network">filtering server list by neutron network</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#ptg-summary" aria-label="PTG summary.">PTG summary.</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="watcher">Watcher<a hidden class="anchor" aria-hidden="true" href="#watcher">#</a></h1>
<p><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4">PTG Etherpad</a></p>
<h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>The 2025.2 PTG was the Second PTG i have attended for watcher.
The last PTG had one session to cover all the technical debt required
to revive the project. This time we extended the ptg to 2 days.</p>
<h2 id="day-1">Day 1<a hidden class="anchor" aria-hidden="true" href="#day-1">#</a></h2>
<h3 id="tech-debt">Tech Debt<a hidden class="anchor" aria-hidden="true" href="#tech-debt">#</a></h3>
<h4 id="croniter"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L39">Croniter</a><a hidden class="anchor" aria-hidden="true" href="#croniter">#</a></h4>
<p>The PTG started with a disscussion about technical debt that emerged during the
epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule
continuous audits based on cron strings. The Croniter libaray maintainer has decided
to stop maintaining this library and it is transitioning to a new maintainer.
While that gives watcher some breathing space, our usage is minimal, so we agreed
to drop the dependency by leveraging the existing functionality of apscheduler.</p>
<h4 id="support-status-of-datasources"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L51">Support status of datasources</a><a hidden class="anchor" aria-hidden="true" href="#support-status-of-datasources">#</a></h4>
<p>The second topic of the ptg focused on the support status of datasources and features of watcher in general.</p>
<p>As part of this discussion we touched on the idea of multiple levels of support:</p>
<ul>
<li>Core features (supported, tested in ci and ready for production use)</li>
<li>Experimental features (supported but not tested in CI or may not be suitable for production use)</li>
<li>Deprecated features (features that are no longer maintained and not suitable for production use)</li>
<li>Unsupported features (features that are not supported by the project, but may work)</li>
</ul>
<p>During this session we agreed to deprecate the support for the following features:</p>
<ul>
<li>Monasca</li>
<li>Grafana</li>
<li>Noisy Neighbour Strategy (l3 cache)</li>
</ul>
<p>Monasca support was dropped because it is no longer maintained and is officially being retired.</p>
<p>Grafana support was dropped because it is no longer maintained and has no existing ci coverage
under our new support levels it could be considered experimental as it may work under certain conditions
however since there is no ci and no active development it was agreed that we should drop support for this feature.</p>
<p>The Current Noisy Neighbour Strategy (l3 cache) was deprected because it depends on metrics that are nolnger
avialable for several years. a dedicated session on how to replace it was held later in the ptg to discuss
how to replace it going forward.</p>
<p>Finally we wrapped up this topic by agreeing that we should create a support and testing matrix for watcher takeing inspiration from
<a href="https://docs.openstack.org/cinder/latest/reference/support-matrix.html">https://docs.openstack.org/cinder/latest/reference/support-matrix.html</a> or <a href="https://docs.openstack.org/nova/latest/user/support-matrix.html">https://docs.openstack.org/nova/latest/user/support-matrix.html</a>
The intent is to provide a way for users to see which features are supported, tested and suitable for production use cases.</p>
<h4 id="eventlet-removal"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L73">Eventlet Removal</a><a hidden class="anchor" aria-hidden="true" href="#eventlet-removal">#</a></h4>
<p>The final technical debt topic was Eventlet removal.
Watcher like many other OpenStack projects have been using eventlet since its inception, and it has become clear that eventlet
is no longer sustainable. While no concrete proposal was made for this topic, to start evaluating the
removal this cycle with some initial POCs and aim to remove eventlet in 2026.1</p>
<h3 id="workflowapi-improvements"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L88">Workflow/API Improvements</a><a hidden class="anchor" aria-hidden="true" href="#workflowapi-improvements">#</a></h3>
<h4 id="skipped-status-for-actions">SKIPPED status for actions<a hidden class="anchor" aria-hidden="true" href="#skipped-status-for-actions">#</a></h4>
<p>This discusstion focused on the introduction of a new Skipped status for actions.
Today actions can be in one of the following states:</p>
<ul>
<li>pending</li>
<li>ongoing</li>
<li>succeeded</li>
<li>failed</li>
</ul>
<p>When constructing and executing an action plan it would be useful to extend the action states
to introduced a <code>skipped</code> state, which would allow users to mark an action as skipped, preventing it from being executed by the engine. The idea is that this can be used to skip actions that are known to fail or be undesirable to execute. Additionally if an actions preconditions cannot be met, the action can be marked as skipped instead of failed.</p>
<p>This lead to a larger discussion about the meaning of SUCCEEDED and FAILED for action plans in general.
<a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108">https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108</a>
We discussed that the current behavior where an action plan is considered succeeded when all actions are attempted
regardless of the success of the action is unintuitive and not in line with the state machine docs.</p>
<p>it was agree that if any action fails the overall action plan should be reported as failed.
Additionally in this context we agreed that if the SKIPPED state is added it should be considered as Succeeded, not Failed.</p>
<h3 id="watcher-dashboard"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L135">watcher-dashboard</a><a hidden class="anchor" aria-hidden="true" href="#watcher-dashboard">#</a></h3>
<p>This sessions was relatively short and focused on 2 areas testing and missing functionality</p>
<h4 id="testing">Testing<a hidden class="anchor" aria-hidden="true" href="#testing">#</a></h4>
<p>We opened the session discussing the fact that the horizon plugin as almost no testing.
the net effect of this is every change need to be manually verified by the reviewer.
We have two options with regard to testing we can build on django&rsquo;s unittests integration
to validate the core logic, or we could build out selenium testing.
no general conclusion was arrived at but we resolved to follow up with the horizon
core team in the future for guidance.</p>
<h4 id="exposing-parameter-in-audits">Exposing parameter in audits<a hidden class="anchor" aria-hidden="true" href="#exposing-parameter-in-audits">#</a></h4>
<p>This was simple and non controversial, today when creating a audit its not possible to
set the parameters filed via the horizon ui. we agreed this should be addresses as a speechless blueprint.</p>
<h4 id="skipped-status">SKIPPED status<a hidden class="anchor" aria-hidden="true" href="#skipped-status">#</a></h4>
<p>finally we discussed that if we extend the action status to model the new SKIPPED state we will need
to enhance the action plan dashboard to allow skipping an action and to display the skipped state when
a precondition fails.</p>
<h3 id="watcher-cluster-model-collector-improvement-ideas"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L154">Watcher cluster model collector improvement ideas</a><a hidden class="anchor" aria-hidden="true" href="#watcher-cluster-model-collector-improvement-ideas">#</a></h3>
<p>This was arguably another tech debt discussion in the guise of an operator pain point.
For performance reasons watcher is build with a cached data model that periodically refreshes, combined
with integration with notification for near real-time updates.</p>
<p>During the session we reviewed <a href="https://bugs.launchpad.net/watcher/&#43;bug/2104220">https://bugs.launchpad.net/watcher/+bug/2104220</a> and resolved
to investigate if we are properly consuming the notifications form nova.
While we are consuming the notifications bug #2104220 asserts that we are perhaps using the
wrong fields to update the instance host on live migration resulting in the source host being updated
instead of the destination host.</p>
<p>other mitigation were discussed such as adding the ability to force refresh the model when executing an
audit, creating a new audit type to just run the refresh, or adjusting the collector interval.</p>
<h3 id="day-1-summary">Day 1 Summary<a hidden class="anchor" aria-hidden="true" href="#day-1-summary">#</a></h3>
<p>Day one was pretty packed but we resolved to finish early when we reached the end of the agenda.</p>
<p>We resolved to proceed with 2 new specs for the SKIPPED sate and model collector plugins
and to treat the parameter enhancement to watcher-dashboard as a specless blueprint.
croniter will be replaced as a bug fix and we need to review the impact of bug: #2104220</p>
<h2 id="day-2">Day 2<a hidden class="anchor" aria-hidden="true" href="#day-2">#</a></h2>
<h3 id="watcher-and-nova"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L187">Watcher and Nova&rsquo;s visible constraints</a><a hidden class="anchor" aria-hidden="true" href="#watcher-and-nova">#</a></h3>
<p>Day 2 kicked off where day one ended on the topic of how watcher models nova instances
and the visibility of scheduler constraints. In the epoxy cycle we added a number of attributes
to the server show response that may be of interest to watcher, namely the scheduler hint and image properties.
additionally i noted that in a prior release we added the pinned AZ as a separate filed.
while extending the data model to provide this info would not be an api change we discussed that a spec
could be nice to have to document the changes but a specless blueprint could also be valid.</p>
<h3 id="noisy-neighbour-strategy"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L206">Noisy neighbour strategy</a><a hidden class="anchor" aria-hidden="true" href="#noisy-neighbour-strategy">#</a></h3>
<p>This discussion happend as a sperate session on the second day but we touched on it on day 1
as part of the tech debt session. The TL;dr is since the exisitng noisy neighbour policy is non functional
and since we want to aovid upgrade impacts a new SLA goal with 1 or multiple strateies shoudl be created.
A spec to define the semantics should be created but at a high level the open questions are</p>
<ul>
<li>should we have one stagey per metric or a single parmaterised stagey</li>
<li>which metrics shoudl we use as the key performance indicators to comptue if the sla is treshold is exceeded
<ul>
<li>cpu_steal</li>
<li>io_wait</li>
<li>cache_misses</li>
<li>others?</li>
</ul>
</li>
</ul>
<p>finally we agreed to deprecate the existing strategy this cycle for removal in 2026.2.</p>
<h3 id="host-maintenance-strategy-new-use-case"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L223">Host Maintenance strategy new use case</a><a hidden class="anchor" aria-hidden="true" href="#host-maintenance-strategy-new-use-case">#</a></h3>
<p>The last feature session discussed the limitation with the current host maintenance strategy
the main limitations resolved around how to express if a instance should be stopped, cold migrated or live migrated.
we agreed to continue with the spec <a href="https://review.opendev.org/c/openstack/watcher-specs/&#43;/943873">https://review.opendev.org/c/openstack/watcher-specs/+/943873</a>
and that it was ok to add new parameters to allow specifying which actions are allowed for the audit.</p>
<p>This topic was reprised in the nova room on day 4
TL;DR in the nova session we agreed to try to use server metadata to encode policies like live migratable.
this can be incorporated into the spec design as a follow up if there is time.
i.e. instance can be annotated by adding lifecycle:livemigratable=True|False and similar lifeccyle: metadata
keys to express which actions may be taken by an external orchestrator like watcher.</p>
<h3 id="ptg-wrappup">PTG wrappup<a hidden class="anchor" aria-hidden="true" href="#ptg-wrappup">#</a></h3>
<h4 id="contributor-docs"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L242">contributor docs</a><a hidden class="anchor" aria-hidden="true" href="#contributor-docs">#</a></h4>
<p>The second to last session was a quick discussion of some of the document that watcher is missing
namely the <code>scope of the project</code> doc and the <code>chronological release guide</code>.
these are both important document for use to create going forward, as is ensuring our existing
contributor docs are up to date regarding when to file a bug vs blueprint vs spec.</p>
<h4 id="retro-and-adhoc-planning"><a href="https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L259">retro and adhoc planning</a><a hidden class="anchor" aria-hidden="true" href="#retro-and-adhoc-planning">#</a></h4>
<p>To wrap up the ptg we had a short retro on how the last cycle went and any last minute topics.
traditionally one start the PTG with the retro rather then finishes but its better late then never.
we realised that one topic had not been raised which is the future of the core team
we briefly discussed that going forward we will review the core team member ship at the start of each slurp
release (i.e 2026.1) and remove inactive cores that have not participated in review, contribution or ptgs
since the prior slurp. As such no removal form the core team will be made for 2025.2 although new
addition may be made based on review activity over the cycle.</p>
<p>With that we we wrapped up the ptg with a reminder of our two cross project sessions with telemetry/horizon on day 3
and nova on day 4</p>
<h1 id="telemetryhorizon"><a href="https://etherpad.opendev.org/p/r.23126f9476afd06ca072ad95ec096a0d#L89">Telemetry/Horizon</a><a hidden class="anchor" aria-hidden="true" href="#telemetryhorizon">#</a></h1>
<h2 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h2>
<p>This ptg unlike many others i attended a combined telemetry/horizon cross project session lead
by Victoria Martinez de la Cruz (vkmc). There we discussed a number of topics related to observability
and how to provide tenant and admin static dashboards to visualise metrics.</p>
<p>Initially we started by discussion why observability and metrics are distinct form the existing views
provided by the admin hypervisors dashboard and how they are distinct form showback/chargeback provide by cloud-kitty</p>
<p>showback/change-back focus on billing and the ratings are base not on the utilisation of the provisioned resource
but rather then allocation. i.e. you are billed equally for an idle vm that uses 4 cpus and 16G of ram vs
a fully loaded vm. cloud kitty can help tell you how large your bill will be but it will not tell you if your
application can run with half the resources. A observability view based on near real-time metrics aims to solve
the latter problem.</p>
<p>The hypervisor dashboard is also related but distinct.
the hypervisor view is intended to provide a semi real-time view of the capacity of a could for admin
to be able to plan if they need more capacity. Again this view looks more at the available resources and the allocated
resource but does not provide an overview of the utilisation.</p>
<h2 id="poc">POC<a hidden class="anchor" aria-hidden="true" href="#poc">#</a></h2>
<p>An initial poc of what the dashboard could looks like is available here <a href="https://github.com/SeanMooney/grian-horizon-plugin">https://github.com/SeanMooney/grian-horizon-plugin</a>
<img alt="Admin Metrics Panel!" loading="lazy" src="https://i.imgur.com/HnJ3zNJ.png"></p>
<h2 id="proposal">proposal<a hidden class="anchor" aria-hidden="true" href="#proposal">#</a></h2>
<ul>
<li>create a new horizon plugin as an official deliverable of the telemetry project in the governance repos</li>
<li>this will be hosted in the <a href="https://opendev.org/openstack">https://opendev.org/openstack</a> namespace and will be released as an official deliverable
via the release repo</li>
<li>we need to spike on d3.js vs rickshaw vs chart.js for charting and assess the version compatible and tech debt</li>
<li>initially the new metrics dashboard will integrate with Prometheus as a metrics store.</li>
<li>a fake backend may be created for testing, other backends are out of scope but could be added later.</li>
<li>a combination of new panels and tabs in existing panels will be added.</li>
<li>initially all dashboards will be static and direct quires to the backend will not be supported</li>
<li>this will allow the plugin to provide multi tenancy as required in the absence of multi tenancy support in Prometheus.</li>
<li>the plugin will be stateless with the time span that can be viewed determined by Prometheus.</li>
<li>the design goal would be to support short term metrics of up to 7 days.</li>
<li>the python-observability client will be used to query Prometheus.</li>
</ul>
<h1 id="nova">Nova<a hidden class="anchor" aria-hidden="true" href="#nova">#</a></h1>
<p>The nova ptg was busy as always, i was also sick all week and had conflict with watcher/horizon
so i was not able to attended all the sessions so ill focus on some of important topics form my
perspective instead of trying to do a full summery. for those looking for a full summery
here is a excellent write up <a href="https://lists.openstack.org/archives/list/openstack-discuss@lists.openstack.org/thread/T6CLTKTGQM7KPV3GXATC4DJ4SUTOBM6N/">Rene&rsquo;s Official PTG Summary</a>, i will also not cover the eventlet removal topic in detail as gibi has a write up of that here
<a href="https://gibizer.github.io/posts/Eventlet-Removal-Flamingo-PTG/">Eventlet removal - Flamingo PTG</a></p>
<h2 id="day-1-1">DAY 1<a hidden class="anchor" aria-hidden="true" href="#day-1-1">#</a></h2>
<p>day 1 was a short day to allow for some cross project sessions to happen before nova officially started.
as is normal we started the ptg with a retrospective on the past cycle, what worked well and how
we wanted to proceed.</p>
<h3 id="project-planning">project planning<a hidden class="anchor" aria-hidden="true" href="#project-planning">#</a></h3>
<p>In general not much has changed for this cycle but ill just highlight some important dates
Timeline:</p>
<ul>
<li>Soft spec freeze (no new specs): June 1st</li>
<li>Hard spec freeze (M2): July 3rd</li>
<li>Feature Freeze (FF): August 28th</li>
<li>Final release: late September / early October</li>
</ul>
<p>For those not familiar with the difference between the soft and hard spec freeze
the soft freeze is the last date we ask contributors to propose large pieces of work.
it serves as a signal that if your topic is complex and has not already been socialised
and reviewed by that point it likely wont be mergeable by the hard freeze.
This basically creates a buffer before people tend to disappear for PTO or
others factors that might make getting a quorum difficult.</p>
<p>This worked well for us in Epoxy when we set the soft spec freeze in early December
to acknowledge that we would loose quorum between late December and early January.</p>
<h3 id="python-313-and-eventlet">python 3.13 and eventlet<a hidden class="anchor" aria-hidden="true" href="#python-313-and-eventlet">#</a></h3>
<p>This are related but distinct topics. This cycle we would like
to start early testing with python 3.13 as Ubuntu 25.04 has now moved to 3.13
and i believe Debian 13 may also. unfortunately monkey_patching the tread lib breaks
on 3.13 so eventlet based service cant actually run on 3.13 under devstack
unit test jobs seam to pass so that a start. we agreed to add a functional-py313
job if we can get it to pass however we do rely on eventlet there so that may or
may not be possible.</p>
<p>Eventlet removal will be one of nova&rsquo;s priories for the 2025.2 cycle as we
acknowledged we are behind were we should be at this point in time.
for those wanting to follow this progress we are going to track it as a recurring
slot in our IRC meeting and gibi is planning to document the progress in there
blog, the <a href="https://gibizer.github.io/posts/Eventlet-Removal-F-24/">first post</a> is already live!</p>
<h2 id="day-2-1">Day 2<a hidden class="anchor" aria-hidden="true" href="#day-2-1">#</a></h2>
<p>day 2 kicked off with some security related topics and finished up with live migration discussions</p>
<h3 id="sev-tdx-and-arm-cca">SEV TDX and arm CCA<a hidden class="anchor" aria-hidden="true" href="#sev-tdx-and-arm-cca">#</a></h3>
<p>Confidential computing is not new by any means even in nova with support for SEV being available for many years.
Last cycle tkajinam had started to make improvement in SEV-ES support in advance of the enablement of SEV-SNP,
unfortunately we did not have review bandwidth to land there enhancements. We resolved to prioritise the review of there
work and also discussed the new development that are happening on intel and arm platforms. TL;DR the kernel, qemu and libvirt
work is not complete for both TDX and CCA at this time and in the arm case there still isn&rsquo;t hardware in production.
effectively its too early to actually add support for arm CCA support and its likely to be a 2027.1 topic based on Ubuntu 26.04+
intel TDX is closer but also likely to early to supprot in 2025.2</p>
<h3 id="pqos-platform-quality-of-service"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L323">PQOS (platform quality of service)</a><a hidden class="anchor" aria-hidden="true" href="#pqos-platform-quality-of-service">#</a></h3>
<p>This is and old one. so like 7 years ago when i was still at intel there was an
effort to support intel cache and memory bandwidth both statically in nova (pushed by myself)
and using an external resource management daemon call RDT. both effort die out but in the last
month or two people have started asking about it again.</p>
<p>i was not able to attend this session but it seam like the new contributors are going to pickup my
<a href="https://review.opendev.org/c/openstack/nova-specs/&#43;/662264">old spec</a> and re-propose it for 2025.2
with updates. Since i wrote that in 2019 the way intel CMT and MBT work has been simplified to some
degree so they will update the spec to reflect that and we will review.</p>
<p>While this may be useful for some real-time workloads that are latency sensitive, its non trivial
to add, complex to configure and likely not a generically useful outside a select number of workloads.
This will be on to monitor but likely not a feature that will be used in many clouds. for those
that need it however it could be a big improvement.</p>
<h3 id="live-migration-topics">live migration topics<a hidden class="anchor" aria-hidden="true" href="#live-migration-topics">#</a></h3>
<h4 id="vtpm"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L369">vtpm</a><a hidden class="anchor" aria-hidden="true" href="#vtpm">#</a></h4>
<p>TL;DR vtpm live migration is our largest gap in this area
we briefly discussed later in the ptg that the pattern for vTPM secret handling will
also inform  how we address the same challenges for cinder encrypted volume  live migration
and local nova storage encryption when we eventually get back to that.
The big change in our plan for vTPM is that this work will be taken over by another contributor
and artom will work with them to hand off this work early in the cycle.</p>
<h4 id="pcivgpu-live-migration-follow-ups"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L406">pci/vGPU live migration follow ups</a><a hidden class="anchor" aria-hidden="true" href="#pcivgpu-live-migration-follow-ups">#</a></h4>
<p>In epoxy nova added support for vfio variant drivers and with that support for pci devices that</p>
<ul>
<li><a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/enable-vfio-devices-with-kernel-variant-drivers.html">Enable VFIO devices with kernel variant drivers</a></li>
<li><a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/migrate-vfio-devices-using-kernel-variant-drivers.html">Live migrate VFIO devices using kernel variant drivers</a></li>
</ul>
<p>During the ptg we discussed what could not be completed and the divergence of the code paths
between flavor based pci pass-though live migration and neutron SR-IOV port live migration.</p>
<p>The big takeaway form this session was we will address <a href="https://bugs.launchpad.net/nova/&#43;bug/2103631">https://bugs.launchpad.net/nova/+bug/2103631</a>
in two steps. First we will move the check before the conductor calls the scheduler.
this is a backporable fix and will provide most of the optimization benefit by entirely eliminating
the placement and scheduler work. Second we will move the check to the api and return a 409.
this will fully optimize the early exit but it will not be as backportable.
While the api can already return a 409
<a href="https://docs.openstack.org/api-ref/compute/#live-migrate-server-os-migratelive-action">https://docs.openstack.org/api-ref/compute/#live-migrate-server-os-migratelive-action</a>
chaning a 202 to a 409 is not always considered valid for backports.</p>
<h2 id="day-3">DAY 3<a hidden class="anchor" aria-hidden="true" href="#day-3">#</a></h2>
<p>day 3 was perhaps the most eclitic of topics with everything form confirming we will
complete previous work started in epoxy to discussion of eventlet removal and cross project
session with glance/cinder .</p>
<h3 id="vdi-enhancements-os-traits-and-one-time-use-devices">VDI enhancements, os-traits and one-time-use devices<a hidden class="anchor" aria-hidden="true" href="#vdi-enhancements-os-traits-and-one-time-use-devices">#</a></h3>
<p>VDI and one time use devices  were short as the first was a carry over form work that was approved
form epoxy but didn&rsquo;t merge in time and one time use devices merged the week before the ptg :)
VDI in openstack is a somewhat complex topic. on one hand we know that there are a number of gaming companies
that host there server side infrastructure on openstack but more recently with the rise fo cloud game
streaming companies there are now a new type of nova user that use both GPU pass-though and spice fundamentally
to delver high performance vdi solutions to there customers. Spice and to a similar degree RDP are both better
positioned to meet the semi real-time demands of cloud gaming then VNC ever will be but its also an area of nova
that until <a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/libvirt-spice-direct-consoles.html">recently</a> has not had much development in a very long time.</p>
<p>both however hit a common pain point, how we use and release os-traits so we spent a while discussing possible path
forward to streamline the work flow. fundamentally we did not resolve to actually make those changes but absorbing
os-traits and os-resource-class back into placement but we generally agreed that we could do that if we happen to find
the time.</p>
<p>one time use devices also sparked a discussion on a related placement <a href="https://bugs.launchpad.net/placement/&#43;bug/2104040">bug</a>
and its <a href="https://review.opendev.org/c/openstack/placement/&#43;/945465">fix</a> this could be seen as controversial but we all
agreed that the existing behaviour is wrong and that we shoudl fix it. Dan presented there solution and we generally
agreed that it seam like a reasonable approach and that we should proceed with the review.
For operators resolving the current limitation that placement Allocations cannot be adjusted when provider is over capacity
will transitively fix 3 separate nova bugs all rooted in this underlying limitation.</p>
<p>This session was also very useful as John Garbutt reminded us that we used the set reserved=total trick
to fix a race condition with cleaning in ironic. i.e. for ironic nodes we set reserved=total when
we provision a node and reserved is only reset after cleaning is complete. So while this is undocumented
behaviour its not undefined and we now have 2 users of it so we probably shoudl test this in placement properly
and update the api ref to document it.</p>
<h3 id="tech-debt-1">tech debt<a hidden class="anchor" aria-hidden="true" href="#tech-debt-1">#</a></h3>
<h4 id="oenstack-client-support"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L488">oenstack client support</a><a hidden class="anchor" aria-hidden="true" href="#oenstack-client-support">#</a></h4>
<p>last cycle was a departure form previous cycles when we actually had multiple api changes.
unfortunately all 4 api change merged late and none of them landed the sdk and osc change required to fully complete the features.
<a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/libvirt-virtiofs-attach-manila-shares.html">manila share support</a>
<a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/libvirt-virtiofs-attach-manila-shares.html">spice direct</a>
<a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/show-scheduler-hints-in-server-details.html">Show Scheduler Hints in Server Details</a>
<a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/implemented/image-properties-in-server-show.html">image properties in server show</a></p>
<p>While this is a low-light all of the above have patches in flight and should land early this cycle.
This topic was actually split between this session on Wednesday with the main topic discussion happening
on Thursday with the <a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L835">client/sdk</a> teams and QA teams for the <a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L872">tempest impact</a>.</p>
<h4 id="glancenovacinder-corss-project">glance/nova/cinder corss project<a hidden class="anchor" aria-hidden="true" href="#glancenovacinder-corss-project">#</a></h4>
<p>unfortunately i could not attend this due to a conflict but the main topics covered were
captured in the <a href="https://lists.openstack.org/archives/list/openstack-discuss@lists.openstack.org/thread/XBEYBGUDCPEQVDVLWWWGVRKQO6JZXL56/">glance ptg summery</a>
the two most impoant topics in the nova context were</p>
<h5 id="new-location-api-cinder--nova">new location API (Cinder &amp; Nova)<a hidden class="anchor" aria-hidden="true" href="#new-location-api-cinder--nova">#</a></h5>
<p>Glance has introduced two new Location APIs in the Dalmatian cycle. We
can use these APIs to address OSSN-0090 and OSSN-0065. Patches for Nova
and Cinder must still be merged, hopefully during the Flamingo cycle.</p>
<h6 id="freeze-glance-client-development">freeze glance client development<a hidden class="anchor" aria-hidden="true" href="#freeze-glance-client-development">#</a></h6>
<p>Cinder and Nova will need to use the OpenStack SDK instead of the
glanceclient. There is no need to complete this work during this cycle,
but we should at least have a good idea of what APIs are currently being
used, so that we can have a plan for the next cycles.</p>
<h4 id="how-nova-store-image-properties-in-our-db"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L548">How nova store image properties in our db</a><a hidden class="anchor" aria-hidden="true" href="#how-nova-store-image-properties-in-our-db">#</a></h4>
<p>you may wonder why this is not relevant to the glance discussion
it is but this is really internal to nova usage of image properties specifically not storing them if we don&rsquo;t need them.
There is a lot of history to this topic but the clips note version is nova stop supporting custom image properties
more than a decade ago but our persistence of them in our db was a little leaky see: <a href="https://bugs.launchpad.net/nova/&#43;bug/2098384">https://bugs.launchpad.net/nova/+bug/2098384</a>
for more details. We agreed that moving forward we should not store image metadata keys in our db that are not part of
the standard set and we should plan to provide a way to clean up the stale entries that already exist.</p>
<h4 id="finalizing-the-secure-rbac-goal"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L536">Finalizing the secure RBAC goal</a><a hidden class="anchor" aria-hidden="true" href="#finalizing-the-secure-rbac-goal">#</a></h4>
<p>the TL;DR of this is the implementing of the manager role and service roles in our policies started a while
ago but unfortunately gmann did not have time to work on it in Epoxy until late in the cycle and we didn&rsquo;t
have time to review it when they got unblocked. so in 2025.2 we would like to prioritise completing this
work which is the final work remaining for nova to complete the SRBAC community goal.</p>
<h3 id="eventlet-removal-1"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L582">Eventlet removal</a><a hidden class="anchor" aria-hidden="true" href="#eventlet-removal-1">#</a></h3>
<p>i wont go into this in detail because gibi already wrote up a better <a href="https://gibizer.github.io/posts/Eventlet-Removal-Flamingo-PTG/">summery</a> but
This was the second session on eventlet in the context of nova (the first being the community one lead by oslo).
Here we discussed the concrete next steps for nova both in terms fo what to do for 2025.2 and beyond.
The important take always are we will track the progress of this goal weekly in our meetings, gibi and kamil will work to
incrementally remove our direct eventlet imports throughout the cycle and try to re-implement scatter gather in the api
so that it can be the first nova service to run without eventlet.
This is probably the largest change in novas implementation since we added python3 support or the initial move form twisted
to eventlet in the very early days. As a community we have 12-18 months to complete this transition and the nova
core team committed to prioritise this work going forward.</p>
<h2 id="day-4">Day 4<a hidden class="anchor" aria-hidden="true" href="#day-4">#</a></h2>
<p>as with day 3 there were many different topic raised mainly in the context of new features
combined with cross project session with neutron, tempest and client/sdk teams.</p>
<h3 id="a-qos-api-for-nova"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L582">A QOS api for nova</a><a hidden class="anchor" aria-hidden="true" href="#a-qos-api-for-nova">#</a></h3>
<p>This was and is likely the most contoverial topic that was raised during the entire ptg
bar providing hooks for the XML which was a straight -3 as that is never going to happen.
during this session we discussed a proposal that is documented <a href="https://etherpad.opendev.org/p/r.58d2817e375719958d64f4ef2098412e">here</a></p>
<p>this topic not only involved nova but also cinder and neutron changes.
in essence this requires fundamentally changes to how nova neutron and cinder work internally and together.</p>
<p>Nova does not supprot changing the QOS applied to neutron Port or cinder volume while the resource is attached
to nova instance. on the neutron size if the QoS policy is a min bandwidth or PPS policy, changing the QOS
on a port can invalidate the placement of the VMs breaking how those features work.</p>
<p>From a nova point of view if you allow the qos policy on a neutron port to change in neutron while its
attached to a nova instance it is therefor a bug.</p>
<p>Cinder and nova have a similar impedance mismatch in that cinder defines it qos policies on the volume type
not on the volume. that means that changing the qos on a volume type would impact many volumes
nova has not way to support that today. To make matters worse operators already abuse the fact
that the fronted qos is updated on live migration to apply these changes live to existing
instance even though nova has no idea this is happening and that might fail.</p>
<p>This was a somewhat challenging session as the topic was brought by an operator that already designed how they
wanted it to work and were asking for use to just accept the changes even though there design is not really compatible
with how self service multi tenant cloud should work.</p>
<p>we spend a lot of the session discussing a possible future nova api for QoS similar to neutrons
i.e. where a operator can define a set of QoS policies (effectively a sub set fo flavor extra specs)
which an end user could select form and that an operator could apply to a project by default.</p>
<p>while that may be a valid evolution of novas api it touch on previous landmines like composable flavors.
we will also need to carefully think about the RBAC implication of who should be able to set a QoS
policy on a Project or instance level and if it shoudl be settable via a Flavor.
For example the manager role likely shoudl be able to set it on a project level if and only if an admin
has not set it. admin likely should be able to set a qos policy in a flavor but when not set a person
with the member role should be able to select form any of the QoS policies defined by an admin as they
can in neutron today.</p>
<p>while we tired to convey some of this feedback i feel like they did not internalise that well.
we also provide context to them on previous discussion form 2019 <a href="https://etherpad.opendev.org/p/r.23a243a5860c7406917f86f48cfb4491#L389">https://etherpad.opendev.org/p/r.23a243a5860c7406917f86f48cfb4491#L389</a>
during the shanghai ptg.</p>
<p>the proposer resolved to follow up with each team separately and file a spec for the works
but we will have to wait and see if took on board our feedback or not.</p>
<h3 id="ovn-live-migration"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L714">OVN live migration</a><a hidden class="anchor" aria-hidden="true" href="#ovn-live-migration">#</a></h3>
<p>who would have guessed that its still a pain&hellip;
we discussed <a href="https://bugs.launchpad.net/nova/&#43;bug/2073254">https://bugs.launchpad.net/nova/+bug/2073254</a> in Thursday and then again on Friday
the TL;DR is neutron now send the event at the right time so we can remove the hacks we added 5
years ago so i have proposed <a href="https://review.opendev.org/c/openstack/nova/&#43;/946950">https://review.opendev.org/c/openstack/nova/+/946950</a>
to address that.</p>
<p>We also discussed that we may want to move the creation of the tap to os-vif instead of
libvirt. <a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1174">https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1174</a>
This likely should only be done if neutron asked us too but we can make progress even
without this.</p>
<h3 id="watcher-topics"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L729">Watcher topics</a><a hidden class="anchor" aria-hidden="true" href="#watcher-topics">#</a></h3>
<p>This was the main topic i added this PTG to discuss how we could evolve nova to better support
watcher use-cases. In this session we discussed how we can annotate instance with metadata to inform
policy. i.e. can this instance be live migrated. the second topic was how can nova tell us
if a vm can be migrated or if other lifecycle operations are valid like cold migration or shelve.
the third topic was slightly different on the usefulness of weighing host by traits with two different
use cases, preferred/avoided traits via flavor and automatically avoiding host with *_pressure traits.</p>
<h4 id="annotating-instances-with-metadata"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L731">annotating instances with metadata</a><a hidden class="anchor" aria-hidden="true" href="#annotating-instances-with-metadata">#</a></h4>
<p>The proposal to add a new api for instance annotation was deferred/rejected in favor of using the
existing instance metadata for this use-cases.
the proposed set of metadata pairs are</p>
<ul>
<li>lifecyle:evacuatable=true|false</li>
<li>lifecyle:cold-migratable=true|false</li>
<li>lifecyle:shelvable=true|false</li>
<li>lifecycle:premetable=true|false</li>
<li>ha:maintance-stragey:in_place|power_off|migrate</li>
<li>ha:role=primary|secondary</li>
<li>ha:priority:[string|number]</li>
</ul>
<p>these can be documented by the service that uses them but we may want to maintain a list
of useful metadata keys like the glance useful image properties doc in nova if they are used.
We can/should document that nova considers this a valid use-case for metadata provided the
user applies the metadata themselves.</p>
<p>note: on Friday we had related discussion about admin metadata which ill cover later.</p>
<h4 id="instance-capabilities"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L766">instance capabilities</a><a hidden class="anchor" aria-hidden="true" href="#instance-capabilities">#</a></h4>
<p>we might work on the name because i will never spell that properly :)
this we said we should do and i should write a spec
during the session artom mention that in addition to the lifecycle annotations above
another capability could be resume on host reboot.
after the discussion i realized there are some other lifecycle operation like suspend, pause and snapshot
that might make sense to report.</p>
<p>so the full set would be</p>
<ul>
<li>lifecyle:evacuatable=true|false</li>
<li>lifecyle:cold-migratable=true|false</li>
<li>lifecyle:shelvable=true|false</li>
<li>lifecycle:premetable=true|false</li>
<li>lifecyle:resume_on_host_reboot=Ture|False</li>
<li>lifecyle:pause=true|false</li>
<li>lifecyle:suspend=true|false</li>
<li>lifecyle:rescue=true|false</li>
<li>lifecyle:snapshot=true|false</li>
</ul>
<p>the idea of <code>instance capabilities</code>is that nova should know what operations
are valid to perform on an instance based on the virt driver, flavor, image and
other project resources like manilla shares, cinder volumes, neutron ports, and cyborg
accelerators.</p>
<p>This will need a spec to proceed but the general direction is seen as positive.</p>
<h4 id="heading"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L779">*_PRESSURE traits and trait based weighing</a><a hidden class="anchor" aria-hidden="true" href="#heading">#</a></h4>
<p>Firstly we started this conversation by asserting that if the Trait is custom nova will not touch it.
So any human or service could annotate nova resource providers with CUSTOM_ traits</p>
<p>second i asserted i would like to have Standard traits for CPU RAM and DISK pressure</p>
<ul>
<li>PRESSURE_CPU</li>
<li>PRESSURE_MEMORY</li>
<li>PRESSURE_DISK</li>
</ul>
<p>network pressure could be added late btu its not one of the one reported by the kernel today
<a href="https://docs.kernel.org/accounting/psi.html#pressure-interface">https://docs.kernel.org/accounting/psi.html#pressure-interface</a></p>
<p>one of the open question i had for this session is would nova be ok with the libvirt driver
reporting these traits or should watcher annotate host with them based on a periodic audit.
the core team was generally ok with the libvirt driver doing this but we will confirm
that as part of a spec.</p>
<p>The second half of this topic was focused on how these newly reported traits when placing vms</p>
<p>The proposal create 1-2 new weightier to way based on traits.</p>
<p>for the *pressure traits we likely want to aovid them by default however there may be other
traits that an operator may want to avoid or prefer. so that implies that there should be a config option.
we decide to punt to the spec/implementation if we wat to include the pressure traits by default or not.</p>
<p>the second weigher would be a flavor based weigher that would extend the existing required/forbidden syntax
to supprot <code>preferred</code> and <code>avoided</code></p>
<p>both weigher will rely on a common change to the host state object to pass the provider summaries
this will allow filters and weighers consider traits and available resource classes.</p>
<p>note i did not discuss this in the session but this would also allow me to introduce what i have called
in the past a <code>boring host weigher</code>. the concept of the <code>boring host weigher</code> is simple
prefer host with fewer traits and resource class or generally a simpler provider tree.
The logic, simpler the provider tree the less fancy the hardware available on the host and there
for its less likely that placing a random vm on the host will prevent a future vm that need one
of the special aspect of the host form fitting.</p>
<p>We agreed that while these are all nice to have and in scope of nova we will
propose concrete next steps in a spec if/when we work on them.</p>
<h3 id="openapi"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L823">OpenApi</a><a hidden class="anchor" aria-hidden="true" href="#openapi">#</a></h3>
<p>Stephen made massive progress last cycle in nova but we ran out of review bandwidth
they summerised there progress in general across openstack in a recent blog post
<a href="https://that.guru/blog/an-update-on-openapi-in-openstack/">An Update on OpenAPI in Openstack</a>
during the ptg session we resolved to try and continue making progress on this as nova
is in a good position to complete this work.
we also discussed that once this work is complete and nova has both request and response
schema for all apis we could remove the tempest schema validation in a later release.
This would be a nice reduction in technical debt for tempest but will need to wait
until all stable/* release of nova support open API validation.</p>
<h3 id="client-changes"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L835">client changes</a><a hidden class="anchor" aria-hidden="true" href="#client-changes">#</a></h3>
<p>we covered this earlier but Stephen also lead a discussion on how can we work better between the nova and
client/sdk teams to make sure we complete the client work when we make api changes
other then not merging them really really late which we should avoid anyway we mainly
just reiterated that direct pings are ok and actually welcome when we thing a change is ready to merge.
in other word reach out when we are close and use depend-on ectra to call out the deps
on the api changes.</p>
<h3 id="testing-testing-and-more-testing">Testing, testing and more testing<a hidden class="anchor" aria-hidden="true" href="#testing-testing-and-more-testing">#</a></h3>
<p>i wont spend much time on this but interleaved with other topics we discussed how to improve <a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L872">testing in tempest</a>,
and can we finish the efforts to []emulate hardware in ci](<a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L859">https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L859</a>)
to test OTU devices/pci passthough, ndd generic mdevs as well as maybe test sr-iov via IGB down the road.</p>
<p>the answer is yes, we can and should update the spec template to note that at a minimum we shoudl update the
tempest schema tests when we add a new api.
dan has started a effort to test pci passhtough as part fo testing OTU device using the virtio-RNG.
sylvain Will revive there mtty series to create a proxy for generic mdevs.</p>
<h3 id="power-management-next-steps"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L893">power management next steps</a><a hidden class="anchor" aria-hidden="true" href="#power-management-next-steps">#</a></h3>
<p>the final session of day 4 was a deep dive on where power management is, the original
design goals and what shoudl we do to fix it. Actually that a lie we did this out of order
and talked about this before the OpenAPI and client topics but I&rsquo;m going to bottom on the etherpad
so we will pretend this was the last sessions :)</p>
<p>the over all outcome fo this session was that we shoudl not rush to power off the cores on startup
and should wait for the resource tracker to be populated with instance so we can ensure
we never power off cores related to an instance that is running.
second while we could modify the virt driver interface to allow the comptue manager to defer
the power off we should explore if we can just internalise this entirely in the libvirt driver first.
if we cant then virt driver change it is. in other words don&rsquo;t rush to power off the core until we are
sure we dont need to power them back up and that they are not in use.</p>
<h2 id="day-5">Day 5<a hidden class="anchor" aria-hidden="true" href="#day-5">#</a></h2>
<h3 id="live-migration-with-cinder-encrypted-volumes"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L939">live migration with cinder encrypted volumes</a><a hidden class="anchor" aria-hidden="true" href="#live-migration-with-cinder-encrypted-volumes">#</a></h3>
<p>This does not work today for exactly the same reason as vtpm migration.
admins do not have access to secrets owned by users.
The fix, learn lessons form how we fix this for vtpm and apply the same to cinder volumes.
specifically the only way we can fix this is the <code>host</code> policy
<a href="https://specs.openstack.org/openstack/nova-specs/specs/2025.1/approved/vtpm-live-migration.html#id2">https://specs.openstack.org/openstack/nova-specs/specs/2025.1/approved/vtpm-live-migration.html#id2</a></p>
<p>cinder volume secrets unlike vtpm are not delete form libvirt when we boot the vm.
so we could copy them between hosts once we implement that for vtpm.
we already depend on the fact the secret is available in libvirt for resuming guest with encrypted volumes on host reboot.</p>
<h3 id="manila-cross-project-session"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L954">Manila cross project session</a><a hidden class="anchor" aria-hidden="true" href="#manila-cross-project-session">#</a></h3>
<p>now that you can attach a share to a nova instance we took time in this ptg to reflect on the next steps.
There are many but in priority order they are the following:</p>
<ul>
<li>finish the osc support and backport to epoxy</li>
<li>testing (tempest, functional testing elsewhere)</li>
<li>memfd memory type in nova (possible by default)</li>
<li>online attach/detach</li>
<li>attaching a share during server create</li>
<li>live migration (still need to have this fully supproted in libvirt/qemu)</li>
</ul>
<p>that is a lot of future work but each is incrementally doable and we resolved to have specs as need for each item</p>
<h3 id="provideryaml-enhancments"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1023">provider.yaml enhancments</a><a hidden class="anchor" aria-hidden="true" href="#provideryaml-enhancments">#</a></h3>
<p>The main topic of this session is simple.
today you can add custom traits via provider.yaml
if you later remove them from the file now will not remove it form the resource provider&hellip;
it should do that.</p>
<p>so we discuss a few ways to address that and i suggested that nova could copy the file to its state
Dir .i.e. /var/lib/nova/last_applied_provider.yaml
on the next restart nova could then compare the traits in the last_applied_provider.yaml to the current provider.yaml
and compute if any trait shoudl be removed.
nova would then up copy the new provider.yaml to last_appled_provider.yaml if it successfully updated placement.</p>
<p>the proposer will do a poc and write up a spec or specless blueprint for this.
in the future we could support other changes like the ability to remove resource provider created via provider.yaml
but that will initially be out of scope.</p>
<h3 id="instance-metadata-protection"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1047">instance metadata protection</a><a hidden class="anchor" aria-hidden="true" href="#instance-metadata-protection">#</a></h3>
<p>This was another very productive session, the use case that was presented is as follows,
I as a cloud operator have some knowledge of my infrasture that i want to apply to an instance,
i would like to make this query able via the rest api, to that end i would like to annotate the instance
with this metadata but i would like to be able to prevent a normal user form removing this metadata.</p>
<p>the discussion started with there proposal to have a config option to define metadata protection sort
of like how you can define protect image properties that only an admin can set in glance.</p>
<p>the main problem with this approach is interoperability between clouds.
we then discussed the idea of defining an <code>admin</code> prefix and perhaps a <code>manager</code>
too allow user with those roles to set keys the member or reader users cannot modify.
while this would work it woudl require any exiting metadata that an operator might be
setting to be updated.</p>
<p>finally we discussed how the lock api and the fact that we record the role of who locked the instance.
our general recommendation was to create a spec to allow recording the role of the user that set
metadata and to prevent lower privileged roles from removing keys set by higher
privileged roles.  that would define the following prescience relationship admin &gt; manager &gt; member</p>
<p>we said we woudl be happy to review a spec to that effect if they write one.</p>
<h3 id="updateset-delete-on-terminate-for-neuton-port"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1076">update/set delete on terminate for neuton port</a><a hidden class="anchor" aria-hidden="true" href="#updateset-delete-on-terminate-for-neuton-port">#</a></h3>
<p>This was short, we should supprot it but we just have not had a contributor to work on it so patches are welcome.
we suggested starting with a poc and filing a spec once they are happy they have something working</p>
<h3 id="graceful-shutdown-">[Graceful shutdown] (<a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089">https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1089</a>)<a hidden class="anchor" aria-hidden="true" href="#graceful-shutdown-">#</a></h3>
<p>this was another rapid fire topic.
we explained the previous attempt and the fact we currently don&rsquo;t have capacity to work on this.</p>
<p>john raised an interesting alternative which would be to add a maintenance mode concept.
this could be implemented as a block in the api of all write operations
alternatively we could decorate all rpc functions that start an instance action or move op
with a decorator that would reject the rpc with a 409 if the compute service was set to maintenance mode
either as a new sate for disabled or as a new filed.</p>
<p>we all agree that a more graceful shutdown would be nice to have
and a poc of different approaches might be a good next step
currently the core team does not have time to work on it but we welcome other to propose solutions.
The main call to action is to report any bugs for problem we know of when there is not a graceful shutdown
while we may or may not be able to fix them that will provide a list of edge cases that need to be solved
for.</p>
<h3 id="iothread-and-block-device-queue"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1111">iothread and block device queue</a><a hidden class="anchor" aria-hidden="true" href="#iothread-and-block-device-queue">#</a></h3>
<p>Again this is a topic that was started on in the past and stalled out.
there has been previous resarch done by redhat and other on how virt-blk scales with io vs iothread vs queues
<a href="https://developers.redhat.com/articles/2024/09/05/scaling-virtio-blk-disk-io-iothread-virtqueue-mapping">https://developers.redhat.com/articles/2024/09/05/scaling-virtio-blk-disk-io-iothread-virtqueue-mapping</a></p>
<p>this session basily was quick and after some discussion fo the history and previous recommendations of
our virt team to always have at least 1 iothread we agreed that the propose should write a spec
to introduce 2 new flavor extra specs/image properties to be able to request the number of iothread
and block device queues. .e.g <code>hw:io_threads=4</code> <code>hw:blk_multiqueue:2</code></p>
<p>the detail will be covered in the the spec but when unset we should likely always allocate 1 iothread
and preserve the libvirt default of creating one virtio-blk queue per vcpu.</p>
<h3 id="filtering-server-list-by-neutron-network"><a href="https://etherpad.opendev.org/p/r.bf5f1185e201e31ed8c3adeb45e3cf6d#L1144">filtering server list by neutron network</a><a hidden class="anchor" aria-hidden="true" href="#filtering-server-list-by-neutron-network">#</a></h3>
<p>the final large topic of the ptg was a proposal to filter server list by neutron network (possibly also subnet in the future)</p>
<p>the general problem to day is while you can make a single query to get all port device-id values (the nova instance the port is attached too)
for all ports on a neutron network, you cant then call nova with 100s of uuids in one query to list the server details.
the proposer uses this information to determine the owner of the vm so that when they need to do maintenance or a particular network
runs out of ips they can reach out to there customer and ask them to move a port or release an ip ectra.</p>
<p>most of this session revolved around is this in scope of nova to do or not.</p>
<p>in the end we resolved that if the data is in the nova db and easy to filter on or we can make a efficient
query to another service that a user cant replicate because they cant pass the resultant uuid to nova for list,
then its ok to extend server list to do this.</p>
<p>we will only add filter on a case by case basis.
as a result we agreed that if the proposer writes a spec and works on a poc
we will review them.</p>
<h1 id="ptg-summary">PTG summary.<a hidden class="anchor" aria-hidden="true" href="#ptg-summary">#</a></h1>
<p>a lot happened in nova and watcher this PTG and more happened in the TC and other project session that i have
not covered at all. hopefully someone will find this useful. i have tried to summarize my understanding of all the
sessions i attended and provide links to the sources where possible.</p>
<p>we got a lot done in what was a total of maybe 20 hour of real-time meetings but we equally now have
alot of work to do to achieve all we discussed including many specs to read and write.</p>
<p>I woudl personally like to thank rene for running the nova sessions and all the rest that contributed there
time. it was a long, tiring and stressful week for many but it was also good to see us disucss these
topics as a community in the open.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.seanmooney.info/tags/openstack/">Openstack</a></li>
      <li><a href="https://www.seanmooney.info/tags/ptg/">Ptg</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.seanmooney.info/blog/ai-to-spec/">
    <span class="title">« Prev</span>
    <br>
    <span>Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance</span>
  </a>
  <a class="next" href="https://www.seanmooney.info/blog/what-is-openstack/">
    <span class="title">Next »</span>
    <br>
    <span>what is openstack</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on x"
            href="https://x.com/intent/tweet/?text=2025.2%20Ptg&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f&amp;hashtags=openstack%2cptg">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f&amp;title=2025.2%20Ptg&amp;summary=2025.2%20Ptg&amp;source=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f&title=2025.2%20Ptg">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on whatsapp"
            href="https://api.whatsapp.com/send?text=2025.2%20Ptg%20-%20https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on telegram"
            href="https://telegram.me/share/url?text=2025.2%20Ptg&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 2025.2 Ptg on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=2025.2%20Ptg&u=https%3a%2f%2fwww.seanmooney.info%2fblog%2f2025.2-ptg%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© Sean Mooney</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
