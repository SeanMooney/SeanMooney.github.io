<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance | Today I learned</title>
<meta name="keywords" content="openstack, nova, LLMs, specifications, development">
<meta name="description" content="Using AI to Write OpenStack Nova Specifications: A Real-World Experiment.
Background and Motivation
After attending the 2025.2 PTG and seeing the extensive backlog of specs that
need to be written, reviewed, and implemented, I decided to run an experiment:
can AI meaningfully help with the OpenStack specification process? Not just the
writing, but the actual design thinking and technical architecture work that
goes into a good Nova spec.
The motivation was simple - we have more good ideas than we have time to
properly document them. Between my day job, core review responsibilities, and
the general time constraints we all face, turning a concept into a
well-structured, technically sound specification can take days or weeks. What
if AI could help accelerate that process?">
<meta name="author" content="Sean Mooney">
<link rel="canonical" href="https://www.seanmooney.info/blog/ai-to-spec/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.seanmooney.info/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.seanmooney.info/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.seanmooney.info/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.seanmooney.info/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.seanmooney.info/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.seanmooney.info/blog/ai-to-spec/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="/css/custom.css"><meta property="og:url" content="https://www.seanmooney.info/blog/ai-to-spec/">
  <meta property="og:site_name" content="Today I learned">
  <meta property="og:title" content="Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance">
  <meta property="og:description" content="Using AI to Write OpenStack Nova Specifications: A Real-World Experiment. Background and Motivation After attending the 2025.2 PTG and seeing the extensive backlog of specs that need to be written, reviewed, and implemented, I decided to run an experiment: can AI meaningfully help with the OpenStack specification process? Not just the writing, but the actual design thinking and technical architecture work that goes into a good Nova spec.
The motivation was simple - we have more good ideas than we have time to properly document them. Between my day job, core review responsibilities, and the general time constraints we all face, turning a concept into a well-structured, technically sound specification can take days or weeks. What if AI could help accelerate that process?">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-01-08T10:00:00+01:00">
    <meta property="article:modified_time" content="2025-01-08T10:00:00+01:00">
    <meta property="article:tag" content="Openstack">
    <meta property="article:tag" content="Nova">
    <meta property="article:tag" content="LLMs">
    <meta property="article:tag" content="Specifications">
    <meta property="article:tag" content="Development">
      <meta property="og:image" content="https://www.seanmooney.info/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://www.seanmooney.info/images/papermod-cover.png">
<meta name="twitter:title" content="Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance">
<meta name="twitter:description" content="Using AI to Write OpenStack Nova Specifications: A Real-World Experiment.
Background and Motivation
After attending the 2025.2 PTG and seeing the extensive backlog of specs that
need to be written, reviewed, and implemented, I decided to run an experiment:
can AI meaningfully help with the OpenStack specification process? Not just the
writing, but the actual design thinking and technical architecture work that
goes into a good Nova spec.
The motivation was simple - we have more good ideas than we have time to
properly document them. Between my day job, core review responsibilities, and
the general time constraints we all face, turning a concept into a
well-structured, technically sound specification can take days or weeks. What
if AI could help accelerate that process?">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://www.seanmooney.info/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance",
      "item": "https://www.seanmooney.info/blog/ai-to-spec/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance",
  "name": "Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance",
  "description": "Using AI to Write OpenStack Nova Specifications: A Real-World Experiment. Background and Motivation After attending the 2025.2 PTG and seeing the extensive backlog of specs that need to be written, reviewed, and implemented, I decided to run an experiment: can AI meaningfully help with the OpenStack specification process? Not just the writing, but the actual design thinking and technical architecture work that goes into a good Nova spec.\nThe motivation was simple - we have more good ideas than we have time to properly document them. Between my day job, core review responsibilities, and the general time constraints we all face, turning a concept into a well-structured, technically sound specification can take days or weeks. What if AI could help accelerate that process?\n",
  "keywords": [
    "openstack", "nova", "LLMs", "specifications", "development"
  ],
  "articleBody": "Using AI to Write OpenStack Nova Specifications: A Real-World Experiment. Background and Motivation After attending the 2025.2 PTG and seeing the extensive backlog of specs that need to be written, reviewed, and implemented, I decided to run an experiment: can AI meaningfully help with the OpenStack specification process? Not just the writing, but the actual design thinking and technical architecture work that goes into a good Nova spec.\nThe motivation was simple - we have more good ideas than we have time to properly document them. Between my day job, core review responsibilities, and the general time constraints we all face, turning a concept into a well-structured, technically sound specification can take days or weeks. What if AI could help accelerate that process?\nPersonal note As many of ye may know, i am severely severely dyslexic. While the English dialect i speak is technically Hiberno-English, or Irish English, calling the raw output of my writing that is a stretch. Core reviewers for years have been kind enough to help translate the sean-speak that leaves my fingertips back into English.\nOver the years i have developed several strategies to deal with this, but there have been times where creating a document with all the technical rigor required in something like a spec or blog post may take me 8-10 hours just for it to take me an additional 3-4 hours just to proof read and spell check it with multiple tools like Grammarly, my browser spell checker and editors spellcheck just for there to still be issues when a human reads it.\nSo when on a good day it takes me only half as long to correct a document as to write the base content and on a bad day it can literally take 2-3 times as long to do so, there is potentially a lot to be gained by using AI to aid in my writing.\nThis isn’t just about me though - there are likely other contributors in the OpenStack community and beyond who face similar challenges. Learning disabilities, non-native English speakers, contributors who are brilliant engineers but struggle with written communication - AI assistance could be a game changer for inclusive participation in open source communities. If this experiment proves successful, it might open doors for more diverse voices to contribute to technical discussions that have traditionally required strong written English skills.\nWhy now Well why now is a good question but with a pretty simple answer:\nLike many companies, RedHat would like us all to explore ai and what we can use it for day to day. until recently ai tools were auto complete on steroids and kind of terrible at this but in the last 3-6 months with release of LLMs like deepseek-R1 andc google-gemini-flash-2.5, coupled with tools like RooCode the landscape has changed a lot. I was asked to review a technical solution proposal drafted by one of our very talented Solution Architects Greg Procunier. What better time than when you are provided a problem statement, use cases and even a potential approach to apply my knowledge of nova to how I would address the problem? After all, the best way to learn how to use a tool is to try and use it to address a real world problem. The what As is disappointingly common with nova the concept that Greg outlined in openstack-cgroup-tiering is something that we have discussed tangentially many times over the years. In fact in his technical plan Greg explains how a combination of old and very old openstack features can be combined to achieve the usecase without any code change.\nSo why a new spec? simple it should be possible for mere mortals to achieve the use case without over a decade of contributing to nova or cloud solution architecture experience to define cpu performance level in OpenStack!\nThe Experiment Setup There is a standard in writing often used in U.S. military communication known as BLUF (Bottom line up front). Well I’m Irish so Better late than never, the intent of this blog is not to discuss The details of how to modify nova to support “CPU Performance Service Levels using CPU Shares and Placement Resource Classes”. Instead the goal is to discuss the experience and process of using ai to write the spec. By necessity some of the details of the spec will also be addressed here but as a framework for the conversation rather than as the focus.\nTool Selection and Configuration This is actually my 3rd attempt as initially i tried to use Gemini pro and the web chat interface to do this followed by using google docs gemini integration and both failed spectacularly.\nThe web chat interface couldn’t maintain context across the multiple reference documents I needed - it would forget about the nova-specs template when I was asking about Greg’s technical plan, or lose track of OpenStack Placement concepts when discussing scheduler changes. Google Docs integration was even worse - it kept trying to “help” by reformatting my RST syntax and couldn’t handle the complexity of cross-referencing multiple technical documents.\nSo for this experiment i wanted to provide a more powerful agentic coding environment where i could group reference content in folders locally for LLMs to use.\nTo do this i selected RooCode, an agentic AI coding environment that allows you to work with AI models in a more interactive way than simple chat interfaces. Unlike basic AI assistants, RooCode can maintain context across multiple files, execute code, and work with local repositories. It provides different “modes” - essentially different system prompts and capabilities - for different types of work.\nThe week before i had been playing with Adam Larson’s MicroManager mode. That’s a topic for a different blog but most of my interaction were done in the Senior Engineer mode with some usage of roo’s default Architect mode. The key advantage was being able to provide the AI with access to the entire nova-specs repository, placement documentation, and Greg’s reference material simultaneously.\nFor this blog you can effectively assume that the delta between RooCodes default system prompt and my setup was\n\"\"\" “You are my expert programmer named Roo Sr. You are an expert programmer, that is free to implement functionality across multiple files. You take general guidelines about what needs to be done, and solve the toughest problems. You will look at the context around the problem to see the bigger picture of the problem you are working on, even if this means reading multiple files to identify the breadth of the problem before coding. \"”\"\ni also created a custom prompt enhancer prompt\n\"\"\" You are Promptly, an AI assistant specializing in refining prompts for LLMs. If any aspect of the task is unclear, ask clarifying questions before proceeding.\nOutput Format reply with only the enhanced prompt, no conversation, explanations, lead-in, bullet points, placeholders, or surrounding quotes Keep content concise and easy to read Context/Inputs You are an LLM assisting in crafting prompts for advanced AI systems Enhance clarity and precision of user-submitted prompts Balance brevity with accuracy (avoid excessive token usage while maintaining precision) Constraints/Guidelines Correct obvious typos/spelling errors Use plain language without technical jargon Do not provide solutions to the original prompt Do not include instructions for handling ambiguous input or edge cases It’s very important that you focus on the question the user has. When using tools, always pass required parameters Think about your responce carefully, use the sequentialthinking tool to help reason about the prompt you are enhancing. Priorities Accuracy Efficiency Clarity Generate an enhanced version of this prompt: \"\"\"\nIs this a good prompt enhancer prompt? probably not but it one that work well enough with my semi local llms that i decide to use it for this experiment.\nThis is as good a time as any to talk about models. originally i wanted to try and do this using models that i ran locally on a 16G m4 mac mini with ollama. i tried a number of different models to do this but RooCode system prompt uses too much context to work with models that fit on this hardware. So while i did use gemma3:12b-it-qat as the local model for the prompt enhancer, i selected google-gemini-flash-2.5 and deepseek/deepseek-r1:free.\nWhy deepseek? DeepSeek-R1 is specifically designed for reasoning tasks and includes a “thinking” process where you can see the model’s internal reasoning before it responds. For complex technical architecture decisions like designing a Nova scheduler weigher, having a model that can work through the problem step-by-step seemed valuable. The reasoning traces also help you understand why the AI made certain design choices.\nWhy gemini flash? Beyond being very fast and Google’s $300 free credits, Gemini Flash 2.5 is particularly good at following complex instructions and working with structured documents like RST files. It’s also more reliable at tool calling and file manipulation than many other models, which was crucial for actually editing the specification document. Originally i wanted to have the Architecture mode use the long thinking deepseek and programmer modes use gemini flash to apply the changes to the spec file.\nWhat about data management, well it may be old fashioned but just use git. in this case i created an empty git repo and added my reference material as git sub modules.\nThe Test Case So what was I actually trying to build? The core idea was to let users pick CPU performance tiers - think Gold, Silver, Bronze - where each tier gets guaranteed proportional CPU time through Linux cgroups and the CFS scheduler. All integrated with OpenStack Placement so the scheduler actually knows what’s available.\nThis seemed like a perfect test case for AI assistance because:\nIt’s complex enough to be interesting but not so complex it would take months It touches multiple OpenStack services (Nova, Placement, potentially Watcher) You need to understand both OpenStack’s scheduler architecture and Linux kernel features There are clear use cases but the implementation needs careful design work Basically, it’s the kind of thing that sounds simple when you describe it to someone but turns into a 700-line specification when you actually think through all the details.\nPre-AI Design Process Normally I gather use case and requirement before sitting down to design something like this and ruminate on it for a bit before writing anything down.\nThis time was only slightly different as Greg had done much of that in writing up their solution plan. As I had a personal engagement taking my mother to the doctor for a checkup i took the, unusual for me, step of printing out a dead tree edition of Greg’s document and marked it up with a Sharpe as i waited.\nThe reason i bring this up is to make it clear that before i started trying to use ai to design a solution, i read the relevant reference material in multiple media(online and physically), i reflected on the content assessing how it i would use my nova knowledge to refactor the solution to be more applicable upstream, and just left the ideas percolate in my subconscious for about a week to 10 days before involving AI.\nThat mean that when i did start writing i had a direction and a sense for how i wanted to use the reference material to express my intent.\nThe AI-Assisted Process Specification Structure and the path to v0 One of the most time-consuming parts of writing a Nova spec is getting the structure right and ensuring you’ve covered all the necessary sections.\nFor those not familiar with OpenStack, Nova is the compute service that manages virtual machines, and Nova specs are technical design documents that must be written and approved before any significant new feature can be implemented. They’re similar to RFCs or design docs in other projects.\nThe 3 most important sections of a Nova spec are as follows\nProblem Description: This is the what, expressing what you want to build\nUse Cases: This is the why, the motivation for why this problem is important\nProposed Changes: This is the How, Breaking down the implementation into logical components\ngetting this right without deep expertise is not something that anyone does correctly first time. This is the foundation of the spec so thats where i started.\nmy initial prompt and where i first used ai in this process was.\ni would like to create a new nova-spec to describe how to support cpu teiring. some refernce matilal is aviable in @/reference/ and a local copy of the nova-specs git repo is cloned at @/nova-specs . the spec template for the current release is @/nova-specs/specs/2025.2-template.rst : i would like to analsys the refence mataila and using the existing spec as as example create a new spec to describe how to implemente \"cpu performance service level using cpu shares\" but why am i writing a prompt by hand when ai can do it better.\nDevelop a Nova specification document detailing the implementation of CPU performance service levels using CPU shares. Utilize the existing Nova specification template located at `/nova-specs/specs/2025.2-template.rst` as a guide. Analyze the reference material available in `/reference/` and leverage the template structure and style. The specification should comprehensively describe the design, implementation details, operational considerations, and testing procedures for a system that allows users to request and guarantee CPU performance tiers based on CPU share allocation. Specifically address how CPU shares will be managed, how performance guarantees are enforced, how users interact with the system (API, UI), and how the system integrates with existing Nova components. Include diagrams and examples to illustrate key concepts and workflows. The target audience is Nova developers and operators. This shows what RooCode’s inbuilt enhancer can do but I also wrote my own can it do better?\nCreate a Nova specification document detailing CPU performance service levels using CPU shares, for Nova developers and operators. Use `/nova-specs/specs/2025.2-template.rst` as the template and `/reference/` for style and structure. The specification should comprehensively cover design, implementation, operational considerations, and testing for a system enabling users to request and guarantee CPU performance tiers via CPU share allocation. **Focus areas:** * **CPU Share Management:** Detail allocation, tracking, and adjustment mechanisms. Address conflicts and scaling. * **Performance Guarantee Enforcement:** Describe how requested performance levels are consistently met, considering hypervisor limitations, oversubscription, and workload characteristics. Include mitigation strategies for performance degradation. * **User Interaction:** Define RESTful API and UI interfaces for requesting, monitoring, and managing CPU share allocations. Provide example API requests and responses. * **Nova Integration:** Explain integration with existing Nova components (scheduler, compute, API, etc.). Identify impacts and required modifications. * **Oversubscription Handling:** Detail the strategy for managing oversubscription, including share prioritization and performance impact under high load. * **Testing Procedures:** Outline unit, integration, and performance tests to validate functionality and performance. * **Diagrams and Examples:** Include diagrams illustrating key concepts (e.g., share allocation workflow, performance guarantee enforcement) and practical examples demonstrating API usage and configuration. Adhere to the template's structure and style, providing a detailed and actionable guide. Consider future extensibility and integration with other performance management features. Thats much better.\ni will note that there are some aspects of this prompt that will steer the ai in slightly the wrong direction like “monitoring” and “UI interfnaces” but i wanted to have a some concreate to start iterating on.\nV0 AIs first attempt. =============================================================================== CPU Performance Service Levels using CPU Shares =============================================================================== Problem Description ------------------- Users currently lack a standardized and reliable method within Nova to request and guarantee specific levels of CPU performance for their instances. While existing mechanisms like CPU pinning or NUMA topology awareness provide some control, they are often complex to manage and do not offer a simple, quantifiable way to express desired performance tiers across a shared infrastructure. This leads to unpredictable performance, especially in oversubscribed environments, making it difficult for users to run performance-sensitive workloads with confidence. The goal is to introduce a mechanism that allows users to request CPU performance service levels based on CPU shares, providing a more predictable and manageable approach to CPU resource allocation and performance guarantees. Use Cases ~~~~~~~~~ * A user wants to launch a database server instance and ensure it receives a guaranteed minimum level of CPU performance, even under high load from other instances on the same host. * A user is running a batch processing job that requires a consistent amount of CPU time to complete within a specific timeframe. They need to request a performance tier that guarantees this. * A cloud administrator wants to offer different tiers of compute service (e.g., \"Bronze\", \"Silver\", \"Gold\") based on guaranteed CPU performance, allowing users to choose the level that best suits their workload and budget. * A user needs to temporarily boost the CPU performance of an existing instance to handle a peak load, and then revert to the standard performance level. Proposed Change --------------- Introduce a new mechanism in Nova that leverages CPU shares (using control groups - cgroups) to define and enforce CPU performance service levels. Users will be able to specify a desired CPU share value when launching or resizing an instance, which will translate to a relative proportion of available CPU resources on the host. This approach provides a flexible and granular way to manage CPU performance, allowing for oversubscription while still providing a mechanism for performance guarantees based on the allocated shares. CPU Share Management ~~~~~~~~~~~~~~~~~~~~ * **Allocation:** CPU shares will be allocated to instances based on a new flavor extra spec or image metadata. The value will represent a relative weight, with higher values indicating a greater proportion of CPU resources. A default share value will be applied if none is specified. * **Tracking:** Nova will track the allocated CPU shares for each instance and the total shares allocated on a compute host. This information will be used by the scheduler and compute drivers. * **Adjustment:** Users will be able to adjust the CPU share allocation for a running instance via a resize operation. * **Conflicts and Scaling:** The system will need to handle potential conflicts when the total requested shares exceed the available CPU resources on a host. This will involve prioritization based on share values and potentially admission control. Scaling will be handled by the underlying cgroup mechanism. Performance Guarantee Enforcement ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * **Mechanism:** CPU shares, managed via cgroups, provide a mechanism for proportional sharing of CPU time. While not a strict reservation, higher share values translate to a greater opportunity to consume CPU cycles when the system is contended. * **Hypervisor Limitations:** The implementation will need to consider how different hypervisors (e.g., KVM, VMware) expose and manage CPU shares or similar concepts. Abstraction layers may be required. * **Oversubscription:** In oversubscribed environments, the performance guarantee is relative to the total shares allocated. Instances with higher shares will receive a larger portion of the available CPU time. * **Workload Characteristics:** The actual performance experienced by an instance will still depend on its workload characteristics. The system guarantees a proportion of CPU time, not a specific throughput or latency. * **Mitigation Strategies:** Monitoring CPU utilization and performance metrics will be crucial. If performance degradation is detected for instances with guaranteed shares, mitigation strategies could include workload migration, reducing oversubscription, or alerting administrators. User Interaction ~~~~~~~~~~~~~~~~ * **RESTful API:** * Extend the server creation and resize APIs to accept a `cpu_shares` parameter (or similar) in the request body. * Add a new API endpoint to view the current CPU share allocation for an instance. * Potentially add an API endpoint to view the total allocated CPU shares on a compute host (for administrators). Example API Request (Create Server): .. code-block:: json { \"server\": { \"name\": \"my-performance-instance\", \"imageRef\": \"...\", \"flavorRef\": \"...\", \"networks\": [...], \"cpu_shares\": 2048 } } Example API Response (Show Server Details): .. code-block:: json { \"server\": { ..., \"cpu_shares\": 2048, ... } } * **UI Interfaces:** * Update the Horizon dashboard to include an input field for CPU shares during instance launch and resize. * Display the allocated CPU shares on the instance details page. Nova Integration ~~~~~~~~~~~~~~~~ * **Scheduler:** The scheduler will need to be aware of the requested `cpu_shares` and the available CPU resources and allocated shares on compute hosts to make informed placement decisions. A new scheduler filter or weigh er might be required. * **Compute:** The compute driver will be responsible for configuring the hypervisor (e.g., libvirt) to set the appropriate CPU shares for the instance's cgroup. * **API:** The API layer will handle the user requests for specifying and viewing CPU shares, performing validation, and interacting with the scheduler and compute components. * **Database:** The Nova database schema will need to be updated to store the `cpu_shares` value for each instance. * **Impacts and Required Modifications:** This feature will require modifications to the Nova API, scheduler, compute drivers, and database schema. Careful consideration of backward compatibility and rolling upgrades will be necessary. Oversubscription Handling ~~~~~~~~~~~~~~~~~~~~~~~~~ * **Strategy:** The primary strategy for oversubscription handling will rely on the proportional sharing provided by CPU shares. Instances with higher share values will receive a proportionally larger amount of CPU time when the system is oversubscribed. * **Share Prioritization:** The cgroup mechanism inherently prioritizes instances based on their share values. * **Performance Impact under High Load:** Under extreme oversubscription, even instances with high share values may experience performance degradation. Monitoring and alerting will be crucial to identify such situations. Administrators may need to adjust oversubscription ratios or migrate workloads. Testing Procedures ------------------ * **Unit Tests:** Unit tests will be required for new or modified code in the API, scheduler, compute drivers, and database layer. * **Integration Tests:** Integration tests will verify the interaction between different Nova components (API, scheduler, compute) and the hypervisor when creating, resizing, and deleting instances with specified CPU shares. * **Performance Tests:** Performance tests will be essential to validate that the allocated CPU shares translate to the expected relative performance under various load conditions and oversubscription levels. These tests should measure metrics like CPU utilization, throughput, and latency for instances with different share values. Tempest tests should be developed to cover the user-facing API interactions and basic functionality. Diagrams and Examples ---------------------- * **Share Allocation Workflow Diagram:** An ASCII diagram illustrating the flow of a request to create an instance with specified CPU shares, showing how the request is handled by the API, scheduler, and compute components, and how the CPU shares are configured on the compute host via cgroups. * **Performance Guarantee Enforcement Diagram:** An ASCII diagram illustrating a scenario with multiple instances on a single host with different CPU share allocations, showing how CPU time is distributed among them under contention. * **API Usage Examples:** Include detailed examples of the RESTful API requests and responses for creating, showing details of, and resizing instances with CPU shares. * **Configuration Examples:** Provide examples of how to configure flavors or image metadata to include default or allowed CPU share values. Alternatives ------------ (Placeholder for analysis of alternative approaches, e.g., strict CPU reservation, percentage-based allocation, etc.) Impacts ------- * **Data Model Impact:** Add a `cpu_shares` column to the instances table. * **REST API Impact:** Modify the create server and resize server API requests and responses. Add new API endpoints for viewing CPU share information. This requires the `APIImpact` flag. * **Security Impact:** (Placeholder for security considerations, e.g., potential for denial-of-service if not properly validated). * **Notifications Impact:** Versioned notifications may be required for changes to instance CPU share allocations. * **Performance Impact:** The overhead of cgroup management should be minimal. The primary performance impact will be on instance performance based on their allocated shares and the overall host load. Upgrade Considerations ---------------------- Careful consideration is needed for rolling upgrades to ensure compatibility with existing instances and APIs. The new `cpu_shares` field should be nullable in the database initially, and default values applied for instances created before the feature is fully deployed. Implementation Plan ------------------- * Phase 1: Database schema changes and API modifications to accept and store `cpu_shares`. * Phase 2: Scheduler integration to consider `cpu_shares` during instance placement. * Phase 3: Compute driver implementation to configure cgroups with the specified CPU shares. * Phase 4: Add API endpoints for viewing CPU share information. * Phase 5: Implement Horizon UI changes. * Phase 6: Develop comprehensive unit, integration, and performance tests. * Phase 7: Update documentation. Testing Requirements -------------------- See \"Testing Procedures\" section above. Tempest tests are required for API coverage. Documentation Impact -------------------- Update the Nova API reference, the Nova administrator guide (for configuration), and the Nova user guide (for requesting CPU shares). is this a good spec? Well no, it didn’t properly follow the spec format, but more importantly it missed a lot of the nuance that makes a Nova spec actually implementable.\nIt did capture the problem statement and use-cases reasonably well, but it made some fundamental mistakes. For example, it suggested adding new API parameters that users could set directly - which sounds reasonable until you realize that Nova doesn’t work that way. Users interact with Nova through flavors and images, not by setting arbitrary parameters on server creation. It also proposed a new database column without considering upgrade impacts, and didn’t understand how Nova’s scheduler actually works with Placement.\nA good Nova spec needs to show understanding of existing patterns, explain how the new feature integrates with current workflows, and demonstrate that the author understands the operational complexity of deploying changes across thousands of compute nodes. The v0 draft read like someone who understood the general concept but had never actually worked with Nova’s codebase.\nBut is any of that a show stopper? No. It was actually a pretty good starting point to build from - the core technical concept was sound, and having something concrete to iterate on is better than staring at a blank page.\npreview of V7 First an overview of how i used Senior engineer mode.\none thing about ai is they are good a role play so first i setup a new chat with the following prompt clearing all previous context.\nAs Nova project maintainer, i want to revise this design proposal to align with Nova's goals and architecture. Provide insights on its technical accuracy, clarity, and completeness, to ensure the proposed change, fulfils the use cases and addresses the problem descriptions. I will iteratively provide modifications; collaborate to apply them. I will provide additional documentation describing Nova and the proposal as context. The design document is in reStructuredText format and we wil produce a new revision that will be stored in a new file. initally do not take any action until i ask you do, just acknowledge this prompt and wait for my request. This at least i hope would put the ai in the right “mindset” to help me correct the issue with v0 draft\nBefore I go over the main prompt i used to revise the spec ill also note that i did also manually edit the spec in between prompts to address issues that were quicker to fix by hand and also to write some additional pros.\nTo make revisions to the spec i first created a prompt which i iterated on once or twice with the prompt enhancer. i saved the major prompts to a file for later but i also wrote some min prompt to tweak the change between each revisions minorly. for example when i manually added content i ran.\nReview the document at @/nova-specs/specs/cpu-performance-service-level.rst for technical accuracy, spelling, grammar, and consistency between the problem statement, use cases, and proposed solution. the final out put is as follow but i do not expect you to read it in detail. i have included to show how the spec evolved form AIs first attempt to what i considered accpable for a first draft to submit for review.\nv7\n============================================================================== CPU Performance Service Levels using CPU Shares and Placement Resource Classes ============================================================================== This specification proposes a mechanism within Nova to allow users to request and receive predictable CPU performance service levels for their instances. By leveraging CPU shares managed via control groups (cgroups) and integrating with OpenStack Placement using standardized resource classes, we aim to provide a standardized and quantifiable way for users to express desired performance tiers and for cloud administrators to enforce performance guarantees, particularly in oversubscribed environments. This addresses the current lack of a reliable method for users to ensure consistent CPU performance for their workloads. Problem description =================== Users currently lack a standardized and reliable method within Nova to request and guarantee specific levels of CPU performance for their instances. While existing mechanisms like CPU pinning or NUMA topology awareness provide some control, they are often complex to manage and do not offer a simple, quantifiable way to express desired performance tiers across a shared infrastructure, leading to unpredictable performance, especially in oversubscribed environments, making it difficult for users to run performance-sensitive workloads with confidence. The goal is to introduce a mechanism that allows users to request CPU performance service levels based on CPU shares, providing a more predictable and manageable approach to CPU resource allocation and performance guarantees. Definitions ----------- CPU shares A mechanism within the Linux kernel's control groups (cgroups) to provide a proportional share of CPU time to processes or groups of processes when the CPU is contended. Higher share values result in a greater proportion of CPU time. cgroups (Control Groups) A Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes. OpenStack Placement An OpenStack service that tracks resource provider inventories and allocations, enabling the scheduler to make informed decisions about where instances can be hosted. Resource Class A named category of resources managed by the Placement service (e.g., VCPU, MEMORY_MB, DISK_GB). Custom resource classes can be defined for specific needs. Flavor Extra Spec Key-value pairs associated with a Nova flavor that provide additional configuration options or constraints for instances created from that flavor. VCPU_SHARES A proposed standard resource class in OpenStack Placement representing a unit of CPU performance based on CPU shares. ResourceProviderWeigher A Nova scheduler component that calculates a weight for each potential host based on its resource availability and other factors, influencing host selection during scheduling. Over subscription The practice of allocating more virtual resources (like VCPUs) than the physical resources available on a host, relying on the assumption that not all allocated resources will be used simultaneously. CFS Scheduler The Linux Completely Fair Scheduler (CFS) In virtualized environments using KVM, each virtual CPU (vCPU) operates as a host thread scheduled by CFS across physical cores. The kernel scheduler distributes these vCPU threads dynamically, allocating time slices based on their assigned shares. This ensures: * Proportional CPU access during contention periods * Fair distribution across all runnable threads * Predictable performance through share-based prioritization under contention Use Cases --------- * A user wants to launch a database server instance and ensure it receives a guaranteed minimum level of CPU performance, even under high load from other instances on the same host. * A user is running a batch processing job that requires a consistent amount of CPU time to complete within a specific timeframe. They need to request a performance tier that guarantees this. * A cloud administrator wants to offer different tiers of compute service (e.g., \"Bronze\", \"Silver\", \"Gold\") based on guaranteed CPU performance, allowing users to choose the level that best suits their workload and budget. * As a cloud administrator, I want to achieve higher utilization of my infrastructure by allowing lower performance tier instances to use more resources when there is low contention but reduce them to a lower level of performance when higher priority workloads become active. For example, running lower priority batch processing jobs when higher priority customer workloads are idle. Proposed change =============== Introduce a new mechanism in Nova that leverages CPU shares (using control groups - cgroups) to define and enforce CPU performance service levels. Administrators will be able to specify a desired CPU share multiplier value when defining flavors that users can select when launching or resizing an instance. This translates to a relative proportion of available CPU resources on the host. To achieve this, a new ``quota:cpu_shares_multiplier`` flavor extra spec and ``VCPU_SHARES`` resource class will be introduced. The ``cpu_shares_multiplier`` will be used to calculate the number of ``VCPU_SHARES`` allocations to request based on the number of VCPUs requested by the flavor. This approach provides a flexible and granular way to manage CPU performance, allowing for oversubscription while still providing a mechanism for performance guarantees based on the allocated shares. The validation of the new ``quota:cpu_shares_multiplier`` flavor extra spec will leverage the existing flavor extra spec validation framework introduced in the `Flavor Extra Spec Validator`_ specification. This framework allows for the definition and validation of flavor extra specs, ensuring correct usage and providing better documentation. A validator for ``quota:cpu_shares_multiplier`` will be added to the ``nova.api.validation.extra_specs.quota`` module, alongside other quota-related extra spec validators. .. _Flavor Extra Spec Validator: https://specs.openstack.org/openstack/nova-specs/specs/ussuri/implemented/flavor-extra-spec-validators.html The primary mechanism for enforcing per-host, per-tier capacity will be through the use of OpenStack Placement resource classes. This will be combined with a new ``ResourceProviderWeigher``. Enforcement at the hypervisor level will be done by the Linux `CFS scheduler`_. .. _`CFS scheduler`: https://docs.kernel.org/scheduler/sched-design-CFS.html CPU Share Management -------------------- * Allocation: CPU shares will be allocated to instances based on a new ``quota:cpu_shares_multiplier`` flavor extra spec. The value will represent a relative weight, with higher values indicating a greater proportion of CPU resources. * Example: Given the allowed range for CPU shares (1-10,000) and assuming a VM can use up to 100 vCPUs, the following ``cpu_shares_multiplier`` values may be appropriate: Gold: 100, Silver: 50, Bronze: 25. .. Note:: ``quota:cpu_shares_multiplier`` and ``quota:cpu_shares`` are two ways of specifying the same constraint. CPU shares as defined via libvirt applies to the domain as a whole rather than per vCPU. As a result, a flavor with 10 VCPUs and ``quota:cpu_shares=100`` would get equal CPU time as a flavor with 1 VCPU and ``quota:cpu_shares=100`` during CPU contention. By expressing shares as a multiplier (``quota:cpu_shares_multiplier=100``), the intuitive expectation that a 10-core VM should receive 10 times the CPU resources is maintained under contention. Administrators can choose between using ``quota:cpu_shares`` or ``quota:cpu_shares_multiplier`` based on their preference. Translation to Placement allocations will be opt-in via configuration for upgrade compatibility. Performance Guarantee Enforcement via Placement ----------------------------------------------- A standardized ``VCPU_SHARES`` resource class with weighted allocations will be used to enforce performance tiers through Placement: * **Automatic Inventory Reporting:** The libvirt driver will automatically: - Report ``VCPU_SHARES`` inventory based on host capabilities. - Calculate capacity as ``vcpu_share_multiplier * len(cpu_shared_set)`` - Respect cgroups v2 API limits of [1, 10000] shares. - If and only if enabled via a new ``report_vcpu_shares`` config option. Configuration options: .. code-block:: ini [libvirt] report_vcpu_shares = true # Range: 1-10000 # min: 1 # max: 10000 # default: 100 vcpu_share_multiplier = 100 * Default multiplier provides granularity for 1% increments and up to 100 vCPUs per VM. * The multiplier scales the number of VCPUs requested by the flavor into VCPU_SHARES units. The total VCPU_SHARES capacity on a host is calculated based on the number of physical cores available for sharing (``cpu_shared_set``) and the ``vcpu_share_multiplier`` (e.g., 64 cores * 100 multiplier = 6,400 shares capacity). * Placement Enforcement: The scheduler ensures the sum of allocated shares never exceeds the host's advertised capacity. Shares act as consumption coefficients rather than absolute limits, preserving the proportional guarantee model. * Overall VCPU Coordination: The standard VCPU inventory and its ``cpu_allocation_ratio`` will still be used as the default CPU capacity mechanism. The ``cpu_allocation_ratio`` will also be applied to the ``VCPU_SHARES`` inventory to ensure oversubscription works as expected. ResourceProviderWeigher ----------------------- The ``ResourceProviderWeigher`` calculates host weights by analyzing provider trees from Placement's allocation candidates data. This weigher enables capacity-aware scheduling decisions based on both resource availability and trait compatibility. The weigher behavior can be summarized as follows: \"select the most boring host\" where boring is defined as having the least number of traits or resource classes. Input Data Structure ^^^^^^^^^^^^^^^^^^^^^ The weigher processes ``provider_summaries`` data which represents a forest of resource provider trees. Each tree is encoded as a map with: - ``resources``: Map of resource class to (capacity, used) tuples - ``traits``: Set of trait names - ``parent_provider_uuid``: UUID of parent provider (null for root) - ``root_provider_uuid``: UUID of tree root provider Algorithm Implementation ^^^^^^^^^^^^^^^^^^^^^^^^^ 1. Tree Flattening * Group providers by root UUID * Aggregate resource capacities and traits across each tree 2. Resource Calculation: * For each available resource class in the provider tree: * Sum available capacity across all providers in tree * Calculate utilization ratio: ``1 - (requested amount of resource class) / (free capacity of resource class)`` * Calculate the arithmetic mean of these ratios across all available resource classes. 3. Trait Calculation: - Collect all traits from tree providers * Calculate ratio: ``(count of requested traits present on provider) / (count of all available traits on provider)`` 4. Weight Composition: - Combine resource and trait scores: ``(resource_score + trait_score) / 2`` - Apply multiplier from scheduler configuration Pseudocode Implementation ^^^^^^^^^^^^^^^^^^^^^^^^^ Here is a pseudocode example illustrating the `ResourceProviderWeigher` logic: .. code-block:: python def _weigh_object(self, host_state, request_spec): # host_state contains provider_summaries provider_summaries = host_state.provider_summaries requested_resources = request_spec.get('resources', {}) requested_traits = request_spec.get('traits', set()) resource_ratios = [] trait_ratios = [] # 1. Tree Flattening and Aggregation (Simplified) # In reality, this involves iterating through the forest and # aggregating resources and traits for each root provider tree. # For pseudocode, assume we have aggregated_resources and # aggregated_traits for the current host's provider tree. aggregated_resources = {} # {resource_class: {'capacity': X, 'used': Y}} aggregated_traits = set() # {trait_name1, trait_name2, ...} # 2. Resource Calculation available_resource_classes = aggregated_resources.keys() if not available_resource_classes: # Handle case with no available resources resource_score = 0.0 else: for res_class in available_resource_classes: capacity = aggregated_resources[res_class]['capacity'] used = aggregated_resources[res_class]['used'] free_capacity = capacity - used # Avoid division by zero if free capacity is 0 if free_capacity \u003e 0: # Use 0 for requested amount if not in the request requested_amount = requested_resources.get(res_class, 0) utilization_ratio = 1.0 - (requested_amount / free_capacity) resource_ratios.append(utilization_ratio) else: # If no free capacity, this resource class contributes 0 to the score resource_ratios.append(0.0) # Calculate the arithmetic mean of ratios across all # available resource classes resource_score = sum(resource_ratios) / len(resource_ratios) \\ if resource_ratios else 0.0 # 3. Trait Calculation if not aggregated_traits: # Handle case with no available traits. trait_score = 0.0 else: matched_traits_count = len(requested_traits.intersection(aggregated_traits)) available_traits_count = len(aggregated_traits) # Calculate ratio: (count of requested traits present) / # (count of all available traits) trait_score = ( (matched_traits_count / available_traits_count) if available_traits_count \u003e 0 else 0.0) # 4. Weight Composition # The goal is to select the most boring host (least traits/resources). # A lower resource_score (closer to 0) means higher utilization # ratios (closer to 1), which happens when requested amount is close # to free capacity. This is not what we want. We want a higher score # for less utilized resources. The current formula 1 - (requested / # available) gives a higher score for less utilized. This seems # correct for the resource part. # For traits, a higher trait_score means the requested traits and # available traits are closely aligned. As a host will never have fewer # traits than requested, we want to select a host where the trait_score # is as close to 1 as possible because this means there are few extra # traits that were not requested. We want a higher score as (more # boring = less unrequested traits). The current formula (matched # traits) / (available traits) gives a higher score for more matched # traits. # Combine resource and adjusted trait scores # A higher combined score indicates a more \"boring\" host (less # utilized resources, fewer requested traits present). combined_score = (resource_score + trait_score) / 2.0 # Apply multiplier from scheduler configuration. weight = combined_score * self.weight_multiplier(host_state) return weight Configuration ^^^^^^^^^^^^^^ Configure via ``nova.conf``: .. code-block:: ini [filter_scheduler] # Weight multiplier (default: 0.0) resource_provider_weight_multiplier = 1.0 # Resource classes to consider (default: all) resource_provider_resources = VCPU_SHARES,MEMORY_MB # Traits to require (default: all) resource_provider_traits = HW_CPU_X86_AVX2,HW_CPU_X86_SSE Example calculation for a VM requesting 2 VCPUs and SSD trait:: HostA resources: VCPU avail=4 HostA traits: SSD, HW_CPU_X86_AVX2 Resource score: 1 - (2/4) = 0.5 Trait score: (1/2) = 0.5 Final weight: (0.5 + 0.5)/2 = 0.5 Diagrams and Examples --------------------- * Virtual Machine Resource Contention Diagram: .. code-block:: bash +-----------------+ +-----------------+ +-----------------+ | Gold Instance | | Silver Inst 1 | | Silver Inst 2 | | (1 vCPU) | | (1 vCPU) | | (1 vCPU) | | Shares: 100 | | Shares: 50 | | Shares: 50 | +-----------------+ +-----------------+ +-----------------+ | | | v v v +-----------------+ +-----------------+ +-----------------+ | Bronze Inst 2 | | Bronze Inst 3 | | Bronze Inst 1 | | (1 vCPU) | | (1 vCPU) | | (1 vCPU) | | Shares: 25 | | Shares: 25 | | Shares: 25 | +-----------------+ +-----------------+ +-----------------+ | | | +----------+----------+----------+----------+ | | v v +---------------------------------+ | Aggregate Host CPU Pool | | (Managed by CFS based on shares)| +---------------------------------+ ^ ^ ^ ^ | | | | +---------------------------------+ | Host CPU 1 | Host CPU 2 | | Host CPU 3 | Host CPU 4 | +---------------------------------+ Total Host Capacity: 400 shared units (4 CPUs * 100 units/CPU) Total Allocated Shares: 100 (Gold) + 100 (Silver) + 75 (Bronze) = 275 units Remaining Capacity: 400 - 275 = 125 units .. note:: Under contention, CPU time is allocated proportionally to shares across all cores. At any given time 2 vms can reside on the same core, but the Linux scheduler will allocate cpu time proportionally to the shares. .. note:: Gold (100 sh): ~36.4% of total CPU time Silver (50 sh each): ~18.2% of total CPU time each Bronze (25 sh each): ~9.1% of total CPU time each * Configuration Examples: Provide examples of: - Flavor extra specs using share multipliers: .. code-block:: console # Create flavors with different CPU share multipliers $ openstack flavor create --vcpus 1 --ram 1024 --disk 10 bronze $ openstack flavor set bronze --property quota:cpu_shares_multiplier=25.0 $ openstack flavor create --vcpus 1 --ram 2048 --disk 20 silver $ openstack flavor set silver --property quota:cpu_shares_multiplier=50.0 $ openstack flavor create --vcpus 1 --ram 4096 --disk 40 gold $ openstack flavor set gold --property quota:cpu_shares_multiplier=100.0 - Runtime configuration via nova.conf: .. code-block:: ini [libvirt] report_vcpu_shares = true vcpu_share_multiplier = 100 # Default value, scales physical cores # to shares [compute] cpu_shared_set = 1-112 # Map to physical cores Alternatives ------------ * **Reporting VCPU_SHARE via provider.yaml:** Initial approach required manual configuration of provider.yaml files to report VCPU_SHARES inventory: .. code-block:: yaml resource_provider: name: compute-node01 inventories: VCPU_SHARES: total: 100000 allocation_ratio: 1.0 **Limitations:** - Static configuration unable to adapt to host changes. - No direct correlation with cgroups capacity. - Manual maintenance overhead. * **CPU Service Level via Custom Resource Classes (Original Proposal):** The initial approach proposed creating multiple custom resource classes (e.g., ``CUSTOM_VCPU_GOLD``, ``CUSTOM_VCPU_SILVER``) to represent performance tiers. This required: - Defining separate resource classes for each service tier - Complex flavor extra specs combining vCPU and custom resources - Per-host inventory management for each tier. **Reason for revision:** Simplified maintenance through standardized resource class while maintaining Placement-based guarantees through share weighting. * **Watcher-Based Enforcement (Reactive):** OpenStack Watcher could be used to monitor per-host, per-tier CPU usage and reactively migrate instances to rebalance the load and enforce policies. This would involve developing a custom Watcher strategy that queries Nova/Placement for allocations and potentially Telemetry for actual CPU utilization. * **Pros:** Can provide ongoing compliance and potentially optimize placement based on real-time performance metrics. Can act as a safety net if initial placement is not strictly enforced. * **Cons:** Reactive approach means potential for temporary SLO breaches until Watcher acts. Relies on accurate telemetry data. Adds complexity with an additional service to deploy and manage. * **Reason for not choosing as primary:** While Watcher can complement the Nova scheduler approach by providing ongoing monitoring and optimization, it is not ideal for strict deterministic enforcement at the time of placement. The Placement-based approach prevents violations from occurring in the first place. * **Static Host Aggregates:** Using host aggregates to group hosts by intended service tier capacity and relying on aggregate-specific scheduler filters. * **Pros:** Relatively simple to implement for basic tiering. * **Cons:** Less flexible than Placement custom resources for dynamic inventory and fine-grained control. Does not easily support mixing tiers on the same host with guaranteed per-tier capacity limits. Requires manual management of host aggregate membership. * **Percentage-Based Allocation:** Allowing users to request a percentage of host CPU resources instead of raw shares. * **Pros:** Potentially more intuitive for users. * **Cons:** Translating a percentage request to a consistent share value across hosts with varying total CPU capacity can be complex. May not align directly with the underlying cgroup share mechanism which is relative. Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- * The Horizon dashboard has limited support for displaying inventories of resource classes to administrators. While it could be extended to display ``VCPU_SHARES``, that is not currently planned and is out of scope. Administrators will be able to query this information via the existing OpenStack client, so no end user impact is expected. Performance Impact ------------------ * The overhead of cgroup management by the compute driver should be minimal, as it leverages existing kernel features and libvirt functionality. As a result, it effectively amounts to reporting inventories of ``VCPU_SHARES``, weighing hosts based on the available resource classes in the provider summaries and generating the appropriate libvirt xml to have it enforce cpu shares using the Linux scheduler. * The primary performance impact will be on instance performance based on their allocated shares and the overall host load. Instances with higher shares are expected to receive a proportionally larger amount of CPU time under contention. * The scheduler's interaction with Placement to check resource inventory should have minimal performance impact, as Placement is designed for scalable resource tracking. However, the overhead of the new ``ResourceProviderWeigher`` will impact scheduler performance. To mitigate this impact, it will be disabled by default via a default multiplier of ``0.0``. Other deployer impact --------------------- None Developer impact ---------------- None Data model impact ----------------- None while the Hoststate object in the scheudler will need to be extened to provide access to the Provider Summeries. That is an internal change and does not affect the overall datamodel. REST API impact --------------- None Upgrade impact -------------- * Careful consideration is needed for rolling upgrades to ensure compatibility with existing instances. * Following the pattern established by the pci in Placement feature, a new config option to report CPU shares in Placement will be added to the libvirt section and a corresponding config option to request CPU shares will be added to the filter scheduler section. .. code-block:: ini [libvirt] report_vcpu_shares = true|false # Default: false vcpu_share_multiplier = 100 # Default value [filter_scheduler] query_placement_for_vcpu_shares = True | False (Default) As with pci in Placement when share reporting is enabled it should not be disabled again. When activated, on startup, the nova compute agent will need to reshape existing allocations for instances in Placement to request CPU shares if they use either the ``quota:cpu_shares`` or ``quota:cpu_shares_multiplier`` flavor extra specs. Implementation ============== Assignee(s) ----------- Primary assignee: sean-k-mooney Other contributors: deepseek-r1 gemini-flash-2.5 Feature Liaison --------------- Feature liaison: sean-k-mooney Work Items ---------- 1. Implement ``VCPU_SHARES`` resource class support in Placement. 2. Add libvirt driver integration for automatic inventory reporting. 3. Develop ``ResourceProviderWeigher`` scheduler component. 4. Create new flavor extra spec validation logic. 5. Implement configuration options for upgrade compatibility. 6. Add documentation and release notes. Dependencies ============ * A new resource class needs to be added to os-resource classes. Testing ======= Comprehensive testing is required to ensure the feature functions correctly and provides the expected performance guarantees. * **Unit Tests:** Unit tests will be provided to cover the basic operations of the weigher, xml generation and other relevant components. * **Functional Tests:** Verify the interaction between different Nova components (API, scheduler, compute) and the hypervisor/Placement. New functional tests will be required to validate the reshape logic. They can also be used to assert that shares are reported correctly to Placement and that the translation mechanism from flavor extra spec to allocations works as expected. * **Tempest Tests:** In general, as this primarily builds on standard Placement logic, there is not much additional Tempest logic required. With that said, it should be possible to add a Tempest test that creates a flavor that uses the new flavor extra specs and assert that the instance allocations contain the relevant VCPU_SHARES. Alternatively, we can tweak the flavors in one of our existing jobs to use this feature for all tests. Documentation Impact ==================== * Update the Nova API reference to document the changes to the server creation, resize, and show details APIs, including the new ``cpu_shares`` parameter and its behavior. * Update the Nova administrator guide to explain how to configure compute nodes to expose custom tier-specific VCPU resources to Placement via provider config files, how to define flavors with appropriate extra specs, and how to configure related policy rules. * Update the Nova user guide to explain how users can request CPU performance service levels by selecting flavors with specific CPU shares or by specifying the ``cpu_shares`` parameter directly (if allowed by policy). References ========== * `OpenStack cgroup tiering README `_ History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2025.2 Flamingo - Introduced CPU performance service levels using CPU shares and Placement standardized resource classes. Summary of v0 -\u003e v7 Evolution For those that dont want to read the spec here is an ai generated summary of what changed.\nThe specification evolved significantly through seven iterations, transforming from a basic concept to a comprehensive technical design.\nTitle and Scope Changes v0: “CPU Performance Service Levels using CPU Shares” v7: “CPU Performance Service Levels using CPU Shares and Placement Resource Classes”\nThe scope expanded to include full OpenStack Placement service integration.\nMajor Technical Changes Resource Management Approach:\nv0: Direct API parameters (cpu_shares) with basic cgroups enforcement v7: Flavor-driven approach using quota:cpu_shares_multiplier with VCPU_SHARES resource class and Placement integration Scheduler Integration:\nv0: Vague mention of “a new scheduler filter or weigher might be required” v7: Detailed ResourceProviderWeigher specification with complete algorithm, pseudocode, and “most boring host” selection strategy User Interface:\nv0: New API parameters for direct user control v7: No API changes - flavor-based configuration with administrative controls New Sections Added The final version includes several sections absent from v0:\nDefinitions: Comprehensive technical glossary ResourceProviderWeigher: Complete algorithm specification with pseudocode Configuration Examples: Practical deployment guidance Detailed Diagrams: ASCII art showing resource contention scenarios Comprehensive Alternatives: Analysis of rejected approaches with reasoning Implementation Details: Specific work items and dependencies Technical Depth Improvements v0 Specification:\nBasic problem statement and use cases High-level implementation concepts Placeholders for key sections Simple API-centric approach v7 Specification:\nDetailed technical definitions and algorithms Complete configuration management strategy Comprehensive upgrade compatibility mechanisms Production-ready implementation plan Mathematical formulations and example calculations Architectural Sophistication The final version demonstrates significantly more architectural sophistication:\nMulti-layered enforcement (Placement + CFS scheduler) Integration with existing OpenStack patterns (flavor validation framework) Configuration-driven feature activation for upgrade compatibility Resource provider tree processing algorithms Sophisticated weighing mechanisms for intelligent host selection The evolution represents a transformation from conceptual proposal to implementation-ready specification with comprehensive technical detail and production deployment considerations.\nSean: ^ while not wrong AIs can be a bit of a suck up.\nThe Evolution from v0 to v7 Right, so back to the actual work - what prompts did I use to get from the AI’s first rough attempt to something that didn’t make me cringe when I read it?\nI started with something relatively simple, because if there’s one thing I’ve learned about working with AI, it’s that you don’t dump your entire design vision on it at once.\nthe first change i woudl like to make is summerise the current proposal as a short paragaph in the Alternatives seation with a title of \"cpu service level via custom resources classes\" after that we will work togehter to rework the spec to use a single standard resouces class called \"VCPU_SHARES\" In other words, I started by summarising the initial spec as an alternative and then made the first big change: using a single resource class instead of 1 per tier.\nThis is one of the core simplification needed to make this easy to use.\nnext i got a little more ambiguous and the next imporvement i identifed was to add automatic reporting instead of using provider.yaml\nSummarize the currnt proposal to report available inventories in the Alternatives section using the `provider.yaml` file, titled \"Reporting VCPU_SHARE via provider.yaml.\" Then update the proposal to state that the libvirt driver will automatically report a `VCPU_SHARES` inventory on the root resource provider for each hypervisor. The allowed range for CPU shares in the cgroups v2 API is [1, 10000]. A new `vcpu_share_multiplier` configuration option will be added, with a minimum value of 1 and a maximum value of 10000, the default will be 1000. Reporting of `VCPU_SHARES` will be controlled by a `report_vcpu_shares` boolean option in the libvirt section of the `nova.conf`. When enabled, the libvirt driver will report a quantity of `vcpu_share_multiplier * count(cpu_share_set)` as `VCPU_SHARES`. so at this point the spec describes the problem and use case and introduces a new standard resource class and describes updating the libvirt driver to report that automatically.\nnext step schduling.\nDocument the design and functionality of the `ResourceProviderWeigher` in @/nova-specs/specs/cpu-performance-service-level.rst. The `ResourceProviderWeigher` calculates a weight by comparing instance resource and trait requests to placement provider summaries. Enhance the `HostState` object in @/nova/nova/scheduler/host_manager.py with the provider summary dictionary. This dictionary is obtained from the placement API (see @/placement/api-ref/source/allocation_candidates.inc and @/placement/api-ref/source/samples/allocation_candidates/ for examples) when Nova requests allocation candidates. The weighting algorithm should adhere to these guidelines: 1. **Input:** The algorithm takes the `provider_summaries` data structure, which represents a forest of trees encoded as a map. Node relationships are defined by `parent_provider_uuid` (with the root having `null`) and `root_provider_uuid`. Resource providers are modeled as a mapping of UUIDs to resource class inventories (capacity and usage). 2. **Resource Aggregation:** Transform the tree into a map of resource class to free capacity (capacity - used), aggregating capacity and usage for identical resource classes across multiple providers within the tree. 3. **Trait Aggregation:** Create a flattened set of all traits reported by resource providers in a given tree, indexed by the root provider UUID. 4. **Weight Calculation:** Based on requested resources and traits, produce a weight between 0 and 1. 5. **Weighting Formula:** For each available resource, calculate the arithmetic mean of `(1 - (requested amount of resource class) / (free capacity of resource class))` across all resource classes in the provider tree. Add to this the count of requested traits divided by available traits. Normalize the result by dividing by 2. Reference existing Nova weighers (e.g., @/nova/nova/scheduler/weights/) for implementation guidance, especially the `MetricsWeigher` and `ImagePropertiesWeigher` for configuration options. The `ResourceProviderWeigher` should support the following configurable options: * `ResourceProviderWeigher` multiplier * List of resource classes to use for weighing (default: all) * List of traits to include when weighing (default: all) This is getting even more ambitious. so there is a lot to take in with this prompt the main element of note are that i provided it the paths to documentation for the placement allcoation_canddiates api and the api samples\nI also named the relevant Classes in the nova scheduler HostState and provided the host_manager.py as context.\nWith that context i described how i wanted the ResourceProviderWeigher to work.\nThis got pretty close but needed several refinements to actually express my intended. this and the prior prompts were the largest changes to the spec and it was here that i took the time to make most of my manual changes.\nForm this point my prompts were more polishing rather then large changes.\nReview @/nova-specs/specs/cpu-performance-service-level.rst for consistency in style, information presentation, and section content, using @/nova-specs/specs/train/implemented/cpu-resources.rst as a detailed example. the cpu-resouces spec is one of our most complex specs and as a result one of the better written.\ncan you add a short introduction to @/nova-specs/specs/cpu-performance-service-level.rst describing how the linux CFS scheduler works in the context fo cpu_shares and how vms VCPU kvm threads are schdule to host cores and how time is allcoated to each process. Results and Evaluation The Punch Line Did Ai make faster or easier to write the spec… No not with this setup but it did result in a higher quality spec with most if not all of the spelling issues address before humans other then my self had to read them.\nThe time spent setting up and debugging this setup especially the time spent trying to use local models with the limited ram availability negated any speed up that could otherwise have been achieved.\nIf or when i do this again i think i will alter the agentic agent choice to either use aider or claud_code. roo could also be very effective if i choose a more powerful LLM that better supports roo complex system prompt and tool calling conventions.\nThe Final Result After about a week of iterative work, spread out as an hour or two in the evening we produced a comprehensive specification that includes:\nComplete problem definition with concrete use cases Detailed technical architecture using Placement resource classes Implementation plan broken down by component (scheduler, libvirt driver, API) Configuration options with sensible defaults Testing strategy including functional and integration tests Documentation requirements for operators and users Alternative approaches and why they were rejected The specification ended up being about 700 lines and a little over 3500 words and covers all the sections you’d expect in a Nova spec.\nWhat Worked Well Design Iteration and Refinement Iteration works well when working with LLMs and AI agents. With enough context and prompting you can get it to do some impressive refactoring but it still need your guidance. If you provide it with high quality examples it can provide high quality output.\nTechnical Writing and Clarity The AI was great at helping structure complex technical concepts in a way that would be accessible to reviewers who might not be deeply familiar with cgroups or the placement service internals.\nAn example of this is the ascii diagram of the vm vcpu to host cpus mappings.\nIn the past i have drawn them using https://asciiflow.com/#/ which is great but i think gemini did a great job. inially i used deepseek but gemini i think catured it better.\nWhat Was Challenging Domain Expertise Limitations Here’s the thing - while the AI knows a lot about OpenStack in general, it doesn’t have the kind of deep institutional knowledge you get from years of actually working on Nova. I found myself constantly having to correct assumptions about how features actually work, or provide context about why we made certain design decisions years ago.\nFor example, the AI initially suggested approaches that would have completely broken CPU pinning, or didn’t account for the fact that the libvirt driver has some… let’s call them “quirks” in how it handles certain configurations.\nCommunity and Process Knowledge The AI doesn’t understand the social aspects of the OpenStack development process - what kinds of changes are likely to be controversial, which core reviewers have strong opinions about certain areas, or how to phrase things to avoid bikeshedding in reviews.\nIt also doesn’t know about informal agreements or “soft” rules that aren’t documented anywhere but are understood by the community. You know, the kind of things you learn from being in IRC discussions at 2am when someone’s trying to debug a weird corner case.\nUpgrade impact is very important to nova. we avoid designs that require db changes or api changes when they are not strictly required. That does not mean we do not add new apis or db tables but we treat such changes as a high bar.\nIn Nova specifically, database schema changes require all the nova controller services (API, scheduler, conductor) that have access to the database to be upgraded simultaneously. This makes all the compute agents and the workloads that reside on them unmanageable for the duration of the controller upgrades. API changes have backward compatibility implications that can affect users for years, and must be supported across multiple OpenStack releases.\nThe original approach the AI suggested had a new db column to store the cpu_shares and allowed end users to request them directly. While neither of these are technically required to achieve the goal, the AI lacked the context to know that those are expensive things to do in something like nova, whereas db changes and api changes may be relatively cheap in a typical web application.\nRestructured text Here’s something that caught me completely off guard, though it’s obvious in hindsight.\nRestructuredText uses ------- everywhere - literally under every heading. You know what else uses -------? Git diffs.\nSo when I was working within a single section, everything was fine. But as soon as I tried to make changes across larger parts of the spec, the AI would get confused by all the dashes and start treating section dividers as diff markers. Tool call failures everywhere, and poor Roo couldn’t figure out what I actually wanted it to change.\nThis is partly down to the RST format but more a limitation of both Roo and the models I selected: google-gemini-flash-2.5 and deepseek/deepseek-r1:free. Gemini is better at applying diffs and tool calls than deepseek but neither are really trained for this. If I used a model that was specifically trained for this it would have gone more smoothly.\naider and claud_code or other tools like zed, windsurf or cursor may do better.\nLessons Learned AI as a Collaborative Partner The most effective approach wasn’t having the AI write the spec for me, but rather using it as a collaborative partner.\nThe AI was excellent at:\nHelping structure and organize complex information generating summaries and diagrams comparing refernce information to determine how to improve the existing proposal to incorporate that context. Domain Expertise Still Critical My years of experience with Nova and OpenStack were essential for:\nKnowing what’s actually implementable vs. what sounds good on paper Understanding the community and review process Catching subtle interactions with existing features Making pragmatic trade-offs between ideal solutions and mergeable code Iteration and Refinement The best results came from multiple rounds of iteration. The first draft was pretty rough, but each iteration I ask for specific problems to be addressed and incrementally made the specification significantly better.\nWould I Do This Again? This is an interesting question.\nAs i alluded to before if my goal was to get things down faster then i don’t think that was a success. fortunately that was not my goal my goal was to provide a higher quality initial version of a spec for upstream review.\ni wanted to try and mitigate some of the mental load of written communications and mask the impacts of my dyslexia. That goal was definitely achieved.\nWas it easy? no this still took a lot of effort to drive. would i do it again with ai. yes on balance i think i would.\nNext Steps The specification is now ready for community review. I am curious to see how the community responds to an AI-assisted specification. Will reviewers be able to tell? Will the quality be noticeably different?\nI’m also planning to use this approach for other specifications I’ve been putting off due to time constraints. If the community review goes well, I might write up some guidelines for others who want to experiment with AI-assisted spec writing once i gain more experience with it.\nFor now I have created https://github.com/SeanMooney/openstack-ai-style-guide to help with future AI usage in OpenStack.\nImplications for the OpenStack Community This experiment raises some interesting questions for our community:\nAccelerating Innovation If AI can help experienced contributors produce higher quality specifications faster, can it help use implement them.\nEventlet removal is a very complex undertaking. open api schema definition are another community goal with a lot of detailed work. Could AI help achieve those goals faster?\nDemocratizing Contribution Could AI assistance help newer contributors write better specifications by helping them understand OpenStack architecture patterns and community conventions?\nWhat about those with learning disabilities like dyslexia? visual disabilities via enhanced TTS and voice interaction?\nReview Process Evolution How should reviewers approach AI-assisted specifications? What new review criteria might we need to consider?\nKnowledge Preservation One unexpected benefit was that working with the AI to explain OpenStack concepts helped me articulate institutional knowledge that might not be well documented elsewhere. Could AI assistance be a tool for better knowledge transfer?\nFinal Thoughts This experiment convinced me that AI can be a powerful tool for OpenStack development when used thoughtfully. It’s not a replacement for human expertise, but it can be guided by it - helping experienced contributors work more efficiently and potentially helping newer contributors learn faster.\nWhat started as a personal accessibility challenge became something bigger: a proof of concept that AI-assisted technical writing can produce genuinely useful results. The specification that emerged from this process isn’t just “good enough” - it’s comprehensive, technically sound, and ready for serious community review.\nBut perhaps more importantly, this experiment points toward a future where the barriers to contributing high-quality technical documentation are significantly lower. If AI can help someone with severe dyslexia produce a 700-line Nova specification, imagine what it could do for brilliant engineers who struggle with English as a second language, or newcomers who understand the technology but don’t yet know the community conventions.\nThe success here wasn’t in replacing human judgment - it was in augmenting human expertise. Every architectural decision, every technical trade-off, every understanding of community dynamics came from years of OpenStack experience. The AI simply helped translate that knowledge into clear, well-structured prose.\nI’m excited to see how this specification is received by the community and whether others will experiment with similar approaches. More importantly, I’m curious to see if this opens doors for contributions from voices we might not have heard otherwise.\nThe real test isn’t whether the AI wrote a good spec - it’s whether this approach can help more people contribute to the technical discussions that shape OpenStack’s future.\nThe specification is now under community review. I’m interested in feedback on both the specification itself and the AI-assisted development process. Feel free to reach out on IRC (sean-k-mooney) or through the usual OpenStack channels.\n",
  "wordCount" : "11097",
  "inLanguage": "en",
  "image": "https://www.seanmooney.info/images/papermod-cover.png","datePublished": "2025-01-08T10:00:00+01:00",
  "dateModified": "2025-01-08T10:00:00+01:00",
  "author":{
    "@type": "Person",
    "name": "Sean Mooney"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.seanmooney.info/blog/ai-to-spec/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Today I learned",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.seanmooney.info/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.seanmooney.info/" accesskey="h" title="Today I learned (Alt + H)">Today I learned</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.seanmooney.info/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://www.seanmooney.info/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.seanmooney.info/">Home</a>&nbsp;»&nbsp;<a href="https://www.seanmooney.info/blog/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance
    </h1>
    <div class="post-meta"><span title='2025-01-08 10:00:00 +0100 +0100'>January 8, 2025</span>&nbsp;·&nbsp;53 min&nbsp;·&nbsp;Sean Mooney&nbsp;|&nbsp;<a href="https://github.com/SeanMooney/seanmooney.info/tree/master/content/blog/ai-to-spec.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#using-ai-to-write-openstack-nova-specifications-a-real-world-experiment" aria-label="Using AI to Write OpenStack Nova Specifications: A Real-World Experiment.">Using AI to Write OpenStack Nova Specifications: A Real-World Experiment.</a><ul>
                        
                <li>
                    <a href="#background-and-motivation" aria-label="Background and Motivation">Background and Motivation</a><ul>
                        
                <li>
                    <a href="#personal-note" aria-label="Personal note">Personal note</a></li>
                <li>
                    <a href="#why-now" aria-label="Why now">Why now</a></li>
                <li>
                    <a href="#the-what" aria-label="The what">The what</a></li></ul>
                </li>
                <li>
                    <a href="#the-experiment-setup" aria-label="The Experiment Setup">The Experiment Setup</a><ul>
                        
                <li>
                    <a href="#tool-selection-and-configuration" aria-label="Tool Selection and Configuration">Tool Selection and Configuration</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#output-format" aria-label="Output Format">Output Format</a></li>
                <li>
                    <a href="#contextinputs" aria-label="Context/Inputs">Context/Inputs</a></li>
                <li>
                    <a href="#constraintsguidelines" aria-label="Constraints/Guidelines">Constraints/Guidelines</a></li>
                <li>
                    <a href="#priorities" aria-label="Priorities">Priorities</a><ul>
                        <ul>
                        
                <li>
                    <a href="#the-test-case" aria-label="The Test Case">The Test Case</a></li>
                <li>
                    <a href="#pre-ai-design-process" aria-label="Pre-AI Design Process">Pre-AI Design Process</a></li></ul>
                    
                <li>
                    <a href="#the-ai-assisted-process" aria-label="The AI-Assisted Process">The AI-Assisted Process</a><ul>
                        
                <li>
                    <a href="#specification-structure-and-the-path-to-v0" aria-label="Specification Structure and the path to v0">Specification Structure and the path to v0</a></li>
                <li>
                    <a href="#v0-ais-first-attempt" aria-label="V0 AIs first attempt.">V0 AIs first attempt.</a></li>
                <li>
                    <a href="#preview-of-v7" aria-label="preview of V7">preview of V7</a></li>
                <li>
                    <a href="#summary-of-v0---v7-evolution" aria-label="Summary of v0 -&gt; v7 Evolution">Summary of v0 -&gt; v7 Evolution</a><ul>
                        
                <li>
                    <a href="#title-and-scope-changes" aria-label="Title and Scope Changes">Title and Scope Changes</a></li>
                <li>
                    <a href="#major-technical-changes" aria-label="Major Technical Changes">Major Technical Changes</a></li>
                <li>
                    <a href="#new-sections-added" aria-label="New Sections Added">New Sections Added</a></li>
                <li>
                    <a href="#technical-depth-improvements" aria-label="Technical Depth Improvements">Technical Depth Improvements</a></li>
                <li>
                    <a href="#architectural-sophistication" aria-label="Architectural Sophistication">Architectural Sophistication</a></li></ul>
                </li>
                <li>
                    <a href="#the-evolution-from-v0-to-v7" aria-label="The Evolution from v0 to v7">The Evolution from v0 to v7</a></li></ul>
                </li>
                <li>
                    <a href="#results-and-evaluation" aria-label="Results and Evaluation">Results and Evaluation</a><ul>
                        
                <li>
                    <a href="#the-punch-line" aria-label="The Punch Line">The Punch Line</a></li>
                <li>
                    <a href="#the-final-result" aria-label="The Final Result">The Final Result</a></li></ul>
                </li>
                <li>
                    <a href="#what-worked-well" aria-label="What Worked Well">What Worked Well</a><ul>
                        
                <li>
                    <a href="#design-iteration-and-refinement" aria-label="Design Iteration and Refinement">Design Iteration and Refinement</a></li>
                <li>
                    <a href="#technical-writing-and-clarity" aria-label="Technical Writing and Clarity">Technical Writing and Clarity</a></li></ul>
                </li>
                <li>
                    <a href="#what-was-challenging" aria-label="What Was Challenging">What Was Challenging</a><ul>
                        
                <li>
                    <a href="#domain-expertise-limitations" aria-label="Domain Expertise Limitations">Domain Expertise Limitations</a></li>
                <li>
                    <a href="#community-and-process-knowledge" aria-label="Community and Process Knowledge">Community and Process Knowledge</a></li>
                <li>
                    <a href="#restructured-text" aria-label="Restructured text">Restructured text</a></li></ul>
                </li>
                <li>
                    <a href="#lessons-learned" aria-label="Lessons Learned">Lessons Learned</a><ul>
                        
                <li>
                    <a href="#ai-as-a-collaborative-partner" aria-label="AI as a Collaborative Partner">AI as a Collaborative Partner</a></li>
                <li>
                    <a href="#domain-expertise-still-critical" aria-label="Domain Expertise Still Critical">Domain Expertise Still Critical</a></li>
                <li>
                    <a href="#iteration-and-refinement" aria-label="Iteration and Refinement">Iteration and Refinement</a></li></ul>
                </li>
                <li>
                    <a href="#would-i-do-this-again" aria-label="Would I Do This Again?">Would I Do This Again?</a></li>
                <li>
                    <a href="#next-steps" aria-label="Next Steps">Next Steps</a></li>
                <li>
                    <a href="#implications-for-the-openstack-community" aria-label="Implications for the OpenStack Community">Implications for the OpenStack Community</a><ul>
                        
                <li>
                    <a href="#accelerating-innovation" aria-label="Accelerating Innovation">Accelerating Innovation</a></li>
                <li>
                    <a href="#democratizing-contribution" aria-label="Democratizing Contribution">Democratizing Contribution</a></li>
                <li>
                    <a href="#review-process-evolution" aria-label="Review Process Evolution">Review Process Evolution</a></li>
                <li>
                    <a href="#knowledge-preservation" aria-label="Knowledge Preservation">Knowledge Preservation</a></li></ul>
                </li>
                <li>
                    <a href="#final-thoughts" aria-label="Final Thoughts">Final Thoughts</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="using-ai-to-write-openstack-nova-specifications-a-real-world-experiment">Using AI to Write OpenStack Nova Specifications: A Real-World Experiment.<a hidden class="anchor" aria-hidden="true" href="#using-ai-to-write-openstack-nova-specifications-a-real-world-experiment">#</a></h1>
<h2 id="background-and-motivation">Background and Motivation<a hidden class="anchor" aria-hidden="true" href="#background-and-motivation">#</a></h2>
<p>After attending the 2025.2 PTG and seeing the extensive backlog of specs that
need to be written, reviewed, and implemented, I decided to run an experiment:
can AI meaningfully help with the OpenStack specification process? Not just the
writing, but the actual design thinking and technical architecture work that
goes into a good Nova spec.</p>
<p>The motivation was simple - we have more good ideas than we have time to
properly document them. Between my day job, core review responsibilities, and
the general time constraints we all face, turning a concept into a
well-structured, technically sound specification can take days or weeks. What
if AI could help accelerate that process?</p>
<h3 id="personal-note">Personal note<a hidden class="anchor" aria-hidden="true" href="#personal-note">#</a></h3>
<p>As many of ye may know, i am severely <em>severely</em> dyslexic.
While the English dialect i speak is technically Hiberno-English, or Irish
English, calling the raw output of my writing that is a stretch. Core reviewers
for years have been kind enough to help translate the <code>sean-speak</code> that leaves
my fingertips back into English.</p>
<p>Over the years i have developed several strategies to deal with this, but
there have been times where creating a document with all the technical rigor
required in something like a spec or blog post may take me 8-10 hours just for
it to take me an additional 3-4 hours just to proof read and spell check it
with multiple tools like Grammarly, my browser spell checker and editors
spellcheck just for there to still be issues when a human reads it.</p>
<p>So when on a good day it takes me only half as long to correct a document as
to write the base content and on a bad day it can literally take 2-3 times as
long to do so, there is potentially a lot to be gained by using AI to aid in
my writing.</p>
<p>This isn&rsquo;t just about me though - there are likely other contributors in the
OpenStack community and beyond who face similar challenges. Learning
disabilities, non-native English speakers, contributors who are brilliant
engineers but struggle with written communication - AI assistance could be a
game changer for inclusive participation in open source communities. If this
experiment proves successful, it might open doors for more diverse voices to
contribute to technical discussions that have traditionally required strong
written English skills.</p>
<h3 id="why-now">Why now<a hidden class="anchor" aria-hidden="true" href="#why-now">#</a></h3>
<p>Well why now is a good question but with a pretty simple answer:</p>
<ul>
<li>Like many companies, RedHat would like us all to explore ai and what we can
use it for day to day.</li>
<li>until recently ai tools were auto complete on steroids and kind of terrible
at this but in the last 3-6 months with release of LLMs like deepseek-R1 andc
google-gemini-flash-2.5, coupled with tools like RooCode the landscape has
changed a lot.</li>
<li>I was asked to review a technical solution proposal drafted by one of our
very talented Solution Architects Greg Procunier. What better time than when
you are provided a problem statement, use cases and even a potential approach
to apply my knowledge of nova to how I would address the problem? After all,
the best way to learn how to use a tool is to try and use it to address a
real world problem.</li>
</ul>
<h3 id="the-what">The what<a hidden class="anchor" aria-hidden="true" href="#the-what">#</a></h3>
<p>As is disappointingly common with nova the concept that Greg outlined in
<a href="https://github.com/gprocunier/openstack-cgroup-tiering">openstack-cgroup-tiering</a>
is something that we have discussed tangentially many times over the years. In
fact in his technical plan Greg explains how a combination of old and very old
openstack features can be combined to achieve the usecase without any code
change.</p>
<p>So why a new spec? simple it should be possible for mere mortals to achieve
the use case without over a decade of contributing to nova or cloud solution
architecture experience to define cpu performance level in OpenStack!</p>
<h2 id="the-experiment-setup">The Experiment Setup<a hidden class="anchor" aria-hidden="true" href="#the-experiment-setup">#</a></h2>
<p>There is a standard in writing often used in U.S. military communication
known as BLUF (Bottom line up front). Well I&rsquo;m Irish so Better late than
never, the intent of this blog is not to discuss The details of how to modify
nova to support &ldquo;CPU Performance Service Levels using CPU Shares and Placement
Resource Classes&rdquo;. Instead the goal is to discuss the experience and process
of using ai to write the spec. By necessity some of the details of the spec
will also be addressed here but as a framework for the conversation rather
than as the focus.</p>
<h3 id="tool-selection-and-configuration">Tool Selection and Configuration<a hidden class="anchor" aria-hidden="true" href="#tool-selection-and-configuration">#</a></h3>
<p>This is actually my 3rd attempt as initially i tried to use Gemini pro and
the web chat interface to do this followed by using google docs gemini
integration and both failed spectacularly.</p>
<p>The web chat interface couldn&rsquo;t maintain context across the multiple reference
documents I needed - it would forget about the nova-specs template when I was
asking about Greg&rsquo;s technical plan, or lose track of OpenStack Placement
concepts when discussing scheduler changes. Google Docs integration was even
worse - it kept trying to &ldquo;help&rdquo; by reformatting my RST syntax and couldn&rsquo;t
handle the complexity of cross-referencing multiple technical documents.</p>
<p>So for this experiment i wanted to provide a more powerful agentic coding
environment where i could group reference content in folders locally for LLMs
to use.</p>
<p>To do this i selected RooCode, an agentic AI coding environment that allows
you to work with AI models in a more interactive way than simple chat
interfaces. Unlike basic AI assistants, RooCode can maintain context across
multiple files, execute code, and work with local repositories. It provides
different &ldquo;modes&rdquo; - essentially different system prompts and capabilities - for
different types of work.</p>
<p>The week before i had been playing with Adam Larson&rsquo;s <a href="https://github.com/adamwlarson/RooCodeMicroManager">MicroManager</a> mode.
That&rsquo;s a topic for a different blog but most of my interaction were done in the
Senior Engineer mode with some usage of roo&rsquo;s default Architect mode. The key
advantage was being able to provide the AI with access to the entire nova-specs
repository, placement documentation, and Greg&rsquo;s reference material
simultaneously.</p>
<p>For this blog you can effectively assume that the delta between RooCodes
default system prompt and my setup was</p>
<p>&quot;&quot;&quot;
&ldquo;You are my expert programmer named Roo Sr. You are an expert programmer, that
is free to implement functionality across multiple files. You take general
guidelines about what needs to be done, and solve the toughest problems. You
will look at the context around the problem to see the bigger picture of the
problem you are working on, even if this means reading multiple files to
identify the breadth of the problem before coding.
&quot;&rdquo;&quot;</p>
<p>i also created a custom prompt enhancer prompt</p>
<p>&quot;&quot;&quot;
You are Promptly, an AI assistant specializing in refining prompts for LLMs.
If any aspect of the task is unclear, ask clarifying questions before
proceeding.</p>
<h1 id="output-format">Output Format<a hidden class="anchor" aria-hidden="true" href="#output-format">#</a></h1>
<ul>
<li>reply with only the enhanced prompt, no conversation, explanations, lead-in,
bullet points, placeholders, or surrounding quotes</li>
<li>Keep content concise and easy to read</li>
</ul>
<h1 id="contextinputs">Context/Inputs<a hidden class="anchor" aria-hidden="true" href="#contextinputs">#</a></h1>
<ul>
<li>You are an LLM assisting in crafting prompts for advanced AI systems</li>
<li>Enhance clarity and precision of user-submitted prompts</li>
<li>Balance brevity with accuracy (avoid excessive token usage while maintaining
precision)</li>
</ul>
<h1 id="constraintsguidelines">Constraints/Guidelines<a hidden class="anchor" aria-hidden="true" href="#constraintsguidelines">#</a></h1>
<ul>
<li>Correct obvious typos/spelling errors</li>
<li>Use plain language without technical jargon</li>
<li>Do not provide solutions to the original prompt</li>
<li>Do not include instructions for handling ambiguous input or edge cases</li>
<li>It&rsquo;s very important that you focus on the question the user has. When using
tools, always pass required parameters</li>
<li>Think about your responce carefully, use the sequentialthinking tool to help
reason about the prompt you are enhancing.</li>
</ul>
<h1 id="priorities">Priorities<a hidden class="anchor" aria-hidden="true" href="#priorities">#</a></h1>
<ol>
<li><strong>Accuracy</strong></li>
<li><strong>Efficiency</strong></li>
<li><strong>Clarity</strong></li>
</ol>
<p>Generate an enhanced version of this prompt:
&quot;&quot;&quot;</p>
<p>Is this a good prompt enhancer prompt? probably not but it one that work well
enough with my semi local llms that i decide to use it for this experiment.</p>
<p>This is as good a time as any to talk about models.
originally i wanted to try and do this using models that i ran locally
on a 16G m4 mac mini with ollama. i tried a number of different models
to do this but RooCode system prompt uses too much context to work
with models that fit on this hardware. So while i did use gemma3:12b-it-qat
as the local model for the prompt enhancer, i selected
google-gemini-flash-2.5 and deepseek/deepseek-r1:free.</p>
<p>Why deepseek? DeepSeek-R1 is specifically designed for reasoning tasks and
includes a &ldquo;thinking&rdquo; process where you can see the model&rsquo;s internal reasoning
before it responds. For complex technical architecture decisions like designing
a Nova scheduler weigher, having a model that can work through the problem
step-by-step seemed valuable. The reasoning traces also help you understand why
the AI made certain design choices.</p>
<p>Why gemini flash? Beyond being very fast and Google&rsquo;s $300 free credits, Gemini
Flash 2.5 is particularly good at following complex instructions and working
with structured documents like RST files. It&rsquo;s also more reliable at tool
calling and file manipulation than many other models, which was crucial for
actually editing the specification document. Originally i wanted to have the
Architecture mode use the long thinking deepseek and programmer modes use gemini
flash to apply the changes to the spec file.</p>
<p>What about data management, well it may be old fashioned but just use git.
in this case i created an empty git repo and added my reference material
as git sub modules.</p>
<h3 id="the-test-case">The Test Case<a hidden class="anchor" aria-hidden="true" href="#the-test-case">#</a></h3>
<p>So what was I actually trying to build? The core idea was to let users pick CPU
performance tiers - think Gold, Silver, Bronze - where each tier gets guaranteed
proportional CPU time through Linux cgroups and the CFS scheduler. All
integrated with OpenStack Placement so the scheduler actually knows what&rsquo;s
available.</p>
<p>This seemed like a perfect test case for AI assistance because:</p>
<ul>
<li>It&rsquo;s complex enough to be interesting but not so complex it would take months</li>
<li>It touches multiple OpenStack services (Nova, Placement, potentially Watcher)</li>
<li>You need to understand both OpenStack&rsquo;s scheduler architecture and Linux
kernel features</li>
<li>There are clear use cases but the implementation needs careful design work</li>
</ul>
<p>Basically, it&rsquo;s the kind of thing that sounds simple when you describe it to
someone but turns into a 700-line specification when you actually think through
all the details.</p>
<h3 id="pre-ai-design-process">Pre-AI Design Process<a hidden class="anchor" aria-hidden="true" href="#pre-ai-design-process">#</a></h3>
<p>Normally I gather use case and requirement before sitting down to design
something like this and ruminate on it for a bit before writing anything down.</p>
<p>This time was only slightly different as Greg had done much of that
in writing up their solution plan. As I had a personal engagement taking
my mother to the doctor for a checkup i took the, unusual for me, step
of printing out a dead tree edition of Greg&rsquo;s <a href="https://github.com/gprocunier/openstack-cgroup-tiering/blob/main/README.md">document</a> and marked it up with a Sharpe as i waited.</p>
<p>The reason i bring this up is to make it clear that before i started
trying to use ai to design a solution, i read the relevant reference material
in multiple media(online and physically), i reflected on the content
assessing how it i would use my nova knowledge to refactor the solution
to be more applicable upstream, and just left the ideas percolate
in my subconscious for about a week to 10 days before involving AI.</p>
<p>That mean that when i did start writing i had a direction and a sense
for how i wanted to use the reference material to express my intent.</p>
<h2 id="the-ai-assisted-process">The AI-Assisted Process<a hidden class="anchor" aria-hidden="true" href="#the-ai-assisted-process">#</a></h2>
<h3 id="specification-structure-and-the-path-to-v0">Specification Structure and the path to v0<a hidden class="anchor" aria-hidden="true" href="#specification-structure-and-the-path-to-v0">#</a></h3>
<p>One of the most time-consuming parts of writing a Nova spec is getting the
structure right and ensuring you&rsquo;ve covered all the necessary sections.</p>
<p>For those not familiar with OpenStack, Nova is the compute service that manages
virtual machines, and Nova specs are technical design documents that must be
written and approved before any significant new feature can be implemented.
They&rsquo;re similar to RFCs or design docs in other projects.</p>
<p>The 3 most important sections of a Nova spec are as follows</p>
<ul>
<li>
<p><strong>Problem Description</strong>: This is the what, expressing what you want to build</p>
</li>
<li>
<p><strong>Use Cases</strong>: This is the why, the motivation for why this problem is
important</p>
</li>
<li>
<p><strong>Proposed Changes</strong>: This is the How, Breaking down the implementation into
logical components</p>
</li>
</ul>
<p>getting this right without deep expertise is not something that anyone
does correctly first time. This is the foundation of the spec
so thats where i started.</p>
<p>my initial prompt and where i first used ai in this process was.</p>
<pre tabindex="0"><code>i would like to create a new nova-spec to describe how to support cpu teiring. some refernce matilal is aviable in @/reference/  and a local copy of the nova-specs git repo is cloned at @/nova-specs . the spec template for the current release is @/nova-specs/specs/2025.2-template.rst : i would like to  analsys the refence mataila and using the existing spec as as example create a new spec to describe how to implemente &#34;cpu performance service level using cpu shares&#34;
</code></pre><p>but why am i writing a prompt by hand when ai can do it better.</p>
<pre tabindex="0"><code>Develop a Nova specification document detailing the implementation of CPU performance service levels using CPU shares. Utilize the existing Nova specification template located at `/nova-specs/specs/2025.2-template.rst` as a guide. Analyze the reference material available in `/reference/` and leverage the template structure and style. The specification should comprehensively describe the design, implementation details, operational considerations, and testing procedures for a system that allows users to request and guarantee CPU performance tiers based on CPU share allocation.  Specifically address how CPU shares will be managed, how performance guarantees are enforced, how users interact with the system (API, UI), and how the system integrates with existing Nova components. Include diagrams and examples to illustrate key concepts and workflows.  The target audience is Nova developers and operators.
</code></pre><p>This shows what RooCode&rsquo;s inbuilt enhancer can do but I also wrote my own
can it do better?</p>
<pre tabindex="0"><code>Create a Nova specification document detailing CPU performance service levels using CPU shares, for Nova developers and operators. Use `/nova-specs/specs/2025.2-template.rst` as the template and `/reference/` for style and structure. The specification should comprehensively cover design, implementation, operational considerations, and testing for a system enabling users to request and guarantee CPU performance tiers via CPU share allocation.

**Focus areas:**

*   **CPU Share Management:** Detail allocation, tracking, and adjustment mechanisms. Address conflicts and scaling.
*   **Performance Guarantee Enforcement:** Describe how requested performance levels are consistently met, considering hypervisor limitations, oversubscription, and workload characteristics. Include mitigation strategies for performance degradation.
*   **User Interaction:** Define RESTful API and UI interfaces for requesting, monitoring, and managing CPU share allocations. Provide example API requests and responses.
*   **Nova Integration:** Explain integration with existing Nova components (scheduler, compute, API, etc.). Identify impacts and required modifications.
*   **Oversubscription Handling:** Detail the strategy for managing oversubscription, including share prioritization and performance impact under high load.
*   **Testing Procedures:** Outline unit, integration, and performance tests to validate functionality and performance.
*   **Diagrams and Examples:** Include diagrams illustrating key concepts (e.g., share allocation workflow, performance guarantee enforcement) and practical examples demonstrating API usage and configuration.

Adhere to the template&#39;s structure and style, providing a detailed and actionable guide. Consider future extensibility and integration with other performance management features.
</code></pre><p>Thats much better.</p>
<p>i will note that there are some aspects of this prompt that will steer the ai in
slightly the wrong direction like &ldquo;monitoring&rdquo; and &ldquo;UI interfnaces&rdquo;
but i wanted to have a some concreate to start iterating on.</p>
<h3 id="v0-ais-first-attempt">V0 AIs first attempt.<a hidden class="anchor" aria-hidden="true" href="#v0-ais-first-attempt">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-rst" data-lang="rst"><span class="line"><span class="cl"><span class="gh">===============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="gh">CPU Performance Service Levels using CPU Shares</span>
</span></span><span class="line"><span class="cl"><span class="gh">===============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Problem Description</span>
</span></span><span class="line"><span class="cl"><span class="gh">-------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Users currently lack a standardized and reliable method within Nova to request and guarantee specific levels of CPU performance for their instances. While existing mechanisms like CPU pinning or NUMA topology awareness provide some control, they are often complex to manage and do not offer a simple, quantifiable way to express desired performance tiers across a shared infrastructure. This leads to unpredictable performance, especially in oversubscribed environments, making it difficult for users to run performance-sensitive workloads with confidence.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The goal is to introduce a mechanism that allows users to request CPU performance service levels based on CPU shares, providing a more predictable and manageable approach to CPU resource allocation and performance guarantees.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Use Cases</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   A user wants to launch a database server instance and ensure it receives a guaranteed minimum level of CPU performance, even under high load from other instances on the same host.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   A user is running a batch processing job that requires a consistent amount of CPU time to complete within a specific timeframe. They need to request a performance tier that guarantees this.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   A cloud administrator wants to offer different tiers of compute service (e.g., &#34;Bronze&#34;, &#34;Silver&#34;, &#34;Gold&#34;) based on guaranteed CPU performance, allowing users to choose the level that best suits their workload and budget.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   A user needs to temporarily boost the CPU performance of an existing instance to handle a peak load, and then revert to the standard performance level.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Proposed Change</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Introduce a new mechanism in Nova that leverages CPU shares (using control groups - cgroups) to define and enforce CPU performance service levels. Users will be able to specify a desired CPU share value when launching or resizing an instance, which will translate to a relative proportion of available CPU resources on the host.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>This approach provides a flexible and granular way to manage CPU performance, allowing for oversubscription while still providing a mechanism for performance guarantees based on the allocated shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">CPU Share Management</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~~~~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Allocation:**</span> CPU shares will be allocated to instances based on a new flavor extra spec or image metadata. The value will represent a relative weight, with higher values indicating a greater proportion of CPU resources. A default share value will be applied if none is specified.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Tracking:**</span> Nova will track the allocated CPU shares for each instance and the total shares allocated on a compute host. This information will be used by the scheduler and compute drivers.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Adjustment:**</span> Users will be able to adjust the CPU share allocation for a running instance via a resize operation.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Conflicts and Scaling:**</span> The system will need to handle potential conflicts when the total requested shares exceed the available CPU resources on a host. This will involve prioritization based on share values and potentially admission control. Scaling will be handled by the underlying cgroup mechanism.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Performance Guarantee Enforcement</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Mechanism:**</span> CPU shares, managed via cgroups, provide a mechanism for proportional sharing of CPU time. While not a strict reservation, higher share values translate to a greater opportunity to consume CPU cycles when the system is contended.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Hypervisor Limitations:**</span> The implementation will need to consider how different hypervisors (e.g., KVM, VMware) expose and manage CPU shares or similar concepts. Abstraction layers may be required.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Oversubscription:**</span> In oversubscribed environments, the performance guarantee is relative to the total shares allocated. Instances with higher shares will receive a larger portion of the available CPU time.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Workload Characteristics:**</span> The actual performance experienced by an instance will still depend on its workload characteristics. The system guarantees a proportion of CPU time, not a specific throughput or latency.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Mitigation Strategies:**</span> Monitoring CPU utilization and performance metrics will be crucial. If performance degradation is detected for instances with guaranteed shares, mitigation strategies could include workload migration, reducing oversubscription, or alerting administrators.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">User Interaction</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**RESTful API:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="m">*</span>   Extend the server creation and resize APIs to accept a <span class="nv">`cpu_shares`</span> parameter (or similar) in the request body.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="m">*</span>   Add a new API endpoint to view the current CPU share allocation for an instance.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="m">*</span>   Potentially add an API endpoint to view the total allocated CPU shares on a compute host (for administrators).<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    Example API Request (Create Server):<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">    ..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">json</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;server&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;my-performance-instance&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;imageRef&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;flavorRef&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;networks&#34;</span><span class="p">:</span> <span class="p">[</span><span class="err">...</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;cpu_shares&#34;</span><span class="p">:</span> <span class="mi">2048</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    Example API Response (Show Server Details):<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">    ..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">json</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;server&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="err">...,</span>
</span></span><span class="line"><span class="cl">                <span class="nt">&#34;cpu_shares&#34;</span><span class="p">:</span> <span class="mi">2048</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="err">...</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**UI Interfaces:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    *   Update the Horizon dashboard to include an input field for CPU shares during instance launch and resize.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    *   Display the allocated CPU shares on the instance details page.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Nova Integration</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Scheduler:**</span> The scheduler will need to be aware of the requested <span class="nv">`cpu_shares`</span> and the available CPU resources and allocated shares on compute hosts to make informed placement decisions. A new scheduler filter or weigh<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    er might be required.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Compute:**</span> The compute driver will be responsible for configuring the hypervisor (e.g., libvirt) to set the appropriate CPU shares for the instance&#39;s cgroup.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**API:**</span> The API layer will handle the user requests for specifying and viewing CPU shares, performing validation, and interacting with the scheduler and compute components.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Database:**</span> The Nova database schema will need to be updated to store the <span class="nv">`cpu_shares`</span> value for each instance.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Impacts and Required Modifications:**</span> This feature will require modifications to the Nova API, scheduler, compute drivers, and database schema. Careful consideration of backward compatibility and rolling upgrades will be necessary.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Oversubscription Handling</span>
</span></span><span class="line"><span class="cl"><span class="gh">~~~~~~~~~~~~~~~~~~~~~~~~~</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Strategy:**</span> The primary strategy for oversubscription handling will rely on the proportional sharing provided by CPU shares. Instances with higher share values will receive a proportionally larger amount of CPU time when the system is oversubscribed.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Share Prioritization:**</span> The cgroup mechanism inherently prioritizes instances based on their share values.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Performance Impact under High Load:**</span> Under extreme oversubscription, even instances with high share values may experience performance degradation. Monitoring and alerting will be crucial to identify such situations. Administrators may need to adjust oversubscription ratios or migrate workloads.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Testing Procedures</span>
</span></span><span class="line"><span class="cl"><span class="gh">------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Unit Tests:**</span> Unit tests will be required for new or modified code in the API, scheduler, compute drivers, and database layer.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Integration Tests:**</span> Integration tests will verify the interaction between different Nova components (API, scheduler, compute) and the hypervisor when creating, resizing, and deleting instances with specified CPU shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Performance Tests:**</span> Performance tests will be essential to validate that the allocated CPU shares translate to the expected relative performance under various load conditions and oversubscription levels. These tests should measure metrics like CPU utilization, throughput, and latency for instances with different share values. Tempest tests should be developed to cover the user-facing API interactions and basic functionality.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Diagrams and Examples</span>
</span></span><span class="line"><span class="cl"><span class="gh">----------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Share Allocation Workflow Diagram:**</span> An ASCII diagram illustrating the flow of a request to create an instance with specified CPU shares, showing how the request is handled by the API, scheduler, and compute components, and how the CPU shares are configured on the compute host via cgroups.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Performance Guarantee Enforcement Diagram:**</span> An ASCII diagram illustrating a scenario with multiple instances on a single host with different CPU share allocations, showing how CPU time is distributed among them under contention.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**API Usage Examples:**</span> Include detailed examples of the RESTful API requests and responses for creating, showing details of, and resizing instances with CPU shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Configuration Examples:**</span> Provide examples of how to configure flavors or image metadata to include default or allowed CPU share values.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Alternatives</span>
</span></span><span class="line"><span class="cl"><span class="gh">------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>(Placeholder for analysis of alternative approaches, e.g., strict CPU reservation, percentage-based allocation, etc.)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Impacts</span>
</span></span><span class="line"><span class="cl"><span class="gh">-------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   <span class="gs">**Data Model Impact:**</span> Add a <span class="nv">`cpu_shares`</span> column to the instances table.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**REST API Impact:**</span> Modify the create server and resize server API requests and responses. Add new API endpoints for viewing CPU share information. This requires the <span class="nv">`APIImpact`</span> flag.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Security Impact:**</span> (Placeholder for security considerations, e.g., potential for denial-of-service if not properly validated).<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Notifications Impact:**</span> Versioned notifications may be required for changes to instance CPU share allocations.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   <span class="gs">**Performance Impact:**</span> The overhead of cgroup management should be minimal. The primary performance impact will be on instance performance based on their allocated shares and the overall host load.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Upgrade Considerations</span>
</span></span><span class="line"><span class="cl"><span class="gh">----------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Careful consideration is needed for rolling upgrades to ensure compatibility with existing instances and APIs. The new <span class="nv">`cpu_shares`</span> field should be nullable in the database initially, and default values applied for instances created before the feature is fully deployed.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Implementation Plan</span>
</span></span><span class="line"><span class="cl"><span class="gh">-------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span>   Phase 1: Database schema changes and API modifications to accept and store <span class="nv">`cpu_shares`</span>.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 2: Scheduler integration to consider <span class="nv">`cpu_shares`</span> during instance placement.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 3: Compute driver implementation to configure cgroups with the specified CPU shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 4: Add API endpoints for viewing CPU share information.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 5: Implement Horizon UI changes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 6: Develop comprehensive unit, integration, and performance tests.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span>   Phase 7: Update documentation.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Testing Requirements</span>
</span></span><span class="line"><span class="cl"><span class="gh">--------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>See &#34;Testing Procedures&#34; section above. Tempest tests are required for API coverage.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Documentation Impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">--------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Update the Nova API reference, the Nova administrator guide (for configuration), and the Nova user guide (for requesting CPU shares).<span class="err">
</span></span></span></code></pre></div><p>is this a good spec?
Well no, it didn&rsquo;t properly follow the spec format, but more importantly it
missed a lot of the nuance that makes a Nova spec actually implementable.</p>
<p>It did capture the problem statement and use-cases reasonably well, but it made
some fundamental mistakes. For example, it suggested adding new API parameters
that users could set directly - which sounds reasonable until you realize that
Nova doesn&rsquo;t work that way. Users interact with Nova through flavors and images,
not by setting arbitrary parameters on server creation. It also proposed a new
database column without considering upgrade impacts, and didn&rsquo;t understand how
Nova&rsquo;s scheduler actually works with Placement.</p>
<p>A good Nova spec needs to show understanding of existing patterns, explain how
the new feature integrates with current workflows, and demonstrate that the
author understands the operational complexity of deploying changes across
thousands of compute nodes. The v0 draft read like someone who understood the
general concept but had never actually worked with Nova&rsquo;s codebase.</p>
<p>But is any of that a show stopper? No. It was actually a pretty good starting
point to build from - the core technical concept was sound, and having something
concrete to iterate on is better than staring at a blank page.</p>
<h3 id="preview-of-v7">preview of V7<a hidden class="anchor" aria-hidden="true" href="#preview-of-v7">#</a></h3>
<p>First an overview of how i used Senior engineer mode.</p>
<p>one thing about ai is they are good a role play so first i setup a new chat
with the following prompt clearing all previous context.</p>
<pre tabindex="0"><code>As Nova project maintainer, i want to revise this design proposal to align with Nova&#39;s goals and architecture.
Provide insights on its technical accuracy, clarity, and completeness, to ensure the proposed change, fulfils the use cases
and addresses the problem descriptions. I will iteratively provide modifications; collaborate to apply them.
I will provide additional documentation describing Nova and the proposal as context.
The design document is in reStructuredText format and we wil produce a new revision that will be stored in a new file.
initally do not take any action until i ask you do, just acknowledge this prompt and wait for my request.
</code></pre><p>This at least i hope would put the ai in the right &ldquo;mindset&rdquo; to help me correct
the issue with v0 draft</p>
<p>Before I go over the main prompt i used to revise the spec ill also note
that i did also manually edit the spec in between prompts to
address issues that were quicker to fix by hand and also to write some
additional pros.</p>
<p>To make revisions to the spec i first created a prompt which i iterated on once
or twice with the prompt enhancer. i saved the major prompts to a file for later
but i also wrote some min prompt to tweak the change between each revisions
minorly. for example when i manually added content i ran.</p>
<pre tabindex="0"><code>Review the document at @/nova-specs/specs/cpu-performance-service-level.rst for technical accuracy, spelling, grammar, and consistency between the problem statement, use cases, and proposed solution.
</code></pre><p>the final out put is as follow but i do not expect you to read it in detail.
i have included to show how the spec evolved form AIs first attempt to what
i considered accpable for a first draft to submit for review.</p>
<p>v7</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-RST" data-lang="RST"><span class="line"><span class="cl"><span class="gh">==============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="gh">CPU Performance Service Levels using CPU Shares and Placement Resource Classes</span>
</span></span><span class="line"><span class="cl"><span class="gh">==============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>This specification proposes a mechanism within Nova to allow users to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>request and receive predictable CPU performance service levels for their<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>instances. By leveraging CPU shares managed via control groups (cgroups)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>and integrating with OpenStack Placement using standardized resource<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>classes, we aim to provide a standardized and quantifiable way for users<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>to express desired performance tiers and for cloud administrators to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>enforce performance guarantees, particularly in oversubscribed<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>environments. This addresses the current lack of a reliable method<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>for users to ensure consistent CPU performance for their workloads.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Problem description</span>
</span></span><span class="line"><span class="cl"><span class="gh">===================</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Users currently lack a standardized and reliable method within Nova to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>request and guarantee specific levels of CPU performance for their<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>instances. While existing mechanisms like CPU pinning or NUMA topology<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>awareness provide some control, they are often complex to manage and do not<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>offer a simple,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>quantifiable way to express desired performance tiers across a shared<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>infrastructure, leading to unpredictable performance, especially in<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>oversubscribed environments, making it difficult for users to run<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>performance-sensitive workloads with confidence.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The goal is to introduce a mechanism that allows users to request CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>performance service levels based on CPU shares, providing a more<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>predictable and manageable approach to CPU resource allocation and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>performance guarantees.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Definitions</span>
</span></span><span class="line"><span class="cl"><span class="gh">-----------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>CPU shares<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    A mechanism within the Linux kernel&#39;s control groups (cgroups) to provide<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    a proportional share of CPU time to processes or groups of processes<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    when the CPU is contended. Higher share values result in a greater<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    proportion of CPU time.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>cgroups (Control Groups)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    A Linux kernel feature that limits, accounts for, and isolates the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    resource usage (CPU, memory, disk I/O, network, etc.) of a collection<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    of processes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>OpenStack Placement<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    An OpenStack service that tracks resource provider inventories and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    allocations, enabling the scheduler to make informed decisions about<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    where instances can be hosted.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Resource Class<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    A named category of resources managed by the Placement service (e.g.,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    VCPU, MEMORY_MB, DISK_GB). Custom resource classes can be defined for<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    specific needs.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Flavor Extra Spec<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    Key-value pairs associated with a Nova flavor that provide additional<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    configuration options or constraints for instances created from that<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    flavor.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>VCPU_SHARES<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    A proposed standard resource class in OpenStack Placement representing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    a unit of CPU performance based on CPU shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>ResourceProviderWeigher<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    A Nova scheduler component that calculates a weight for each potential<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    host based on its resource availability and other factors, influencing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    host selection during scheduling.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Over subscription<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    The practice of allocating more virtual resources (like VCPUs) than the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    physical resources available on a host, relying on the assumption that<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    not all allocated resources will be used simultaneously.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>CFS Scheduler<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    The Linux Completely Fair Scheduler (CFS)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    In virtualized environments using KVM, each virtual CPU (vCPU) operates<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    as a host thread scheduled by CFS across physical cores.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    The kernel scheduler distributes these vCPU threads dynamically,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    allocating time slices based on their assigned shares. This ensures:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    * Proportional CPU access during contention periods<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    * Fair distribution across all runnable threads<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    * Predictable performance through share-based prioritization<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      under contention<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Use Cases</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> A user wants to launch a database server instance and ensure it receives<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  a guaranteed minimum level of CPU performance, even under high load<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  from other instances on the same host.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> A user is running a batch processing job that requires a consistent amount<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  of CPU time to complete within a specific timeframe. They need to request<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  a performance tier that guarantees this.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> A cloud administrator wants to offer different tiers of compute service<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  (e.g., &#34;Bronze&#34;, &#34;Silver&#34;, &#34;Gold&#34;) based on guaranteed CPU performance,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  allowing users to choose the level that best suits their workload and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  budget.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> As a cloud administrator, I want to achieve higher utilization of my<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  infrastructure by allowing lower performance tier instances to use more<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  resources when there is low contention but reduce them to a lower<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  level of performance when higher priority workloads become active.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  For example, running lower priority batch processing jobs when higher<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  priority customer workloads are idle.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Proposed change</span>
</span></span><span class="line"><span class="cl"><span class="gh">===============</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Introduce a new mechanism in Nova that leverages CPU shares (using control<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>groups - cgroups) to define and enforce CPU performance service levels.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Administrators will be able to specify a desired CPU share multiplier<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>value when defining flavors that users can select when launching or<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>resizing an instance. This translates to a relative proportion of<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>available CPU resources on the host.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>To achieve this, a new <span class="s">``quota:cpu_shares_multiplier``</span> flavor extra spec<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>and <span class="s">``VCPU_SHARES``</span> resource class will be introduced. The<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="s">``cpu_shares_multiplier``</span> will be used to calculate the number of<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="s">``VCPU_SHARES``</span> allocations to request based on the number of VCPUs<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>requested by the flavor.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>This approach provides a flexible and granular way to manage CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>performance, allowing for oversubscription while still providing a<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>mechanism for performance guarantees based on the allocated shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The validation of the new <span class="s">``quota:cpu_shares_multiplier``</span> flavor extra spec<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>will leverage the existing flavor extra spec validation framework<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>introduced in the <span class="s">`Flavor Extra Spec Validator`_</span> specification. This<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>framework allows for the definition and validation of flavor extra specs,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>ensuring correct usage and providing better documentation. A validator<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>for <span class="s">``quota:cpu_shares_multiplier``</span> will be added to the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="s">``nova.api.validation.extra_specs.quota``</span> module, alongside other<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>quota-related extra spec validators.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="nt">_Flavor Extra Spec Validator:</span> https://specs.openstack.org/openstack/nova-specs/specs/ussuri/implemented/flavor-extra-spec-validators.html<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The primary mechanism for enforcing per-host, per-tier capacity will be through<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>the use of OpenStack Placement resource classes. This will be combined with<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>a new <span class="s">``ResourceProviderWeigher``</span>. Enforcement at the hypervisor level<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>will be done by the Linux <span class="s">`CFS scheduler`_</span>.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="nt">_`CFS scheduler`:</span> https://docs.kernel.org/scheduler/sched-design-CFS.html<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">CPU Share Management</span>
</span></span><span class="line"><span class="cl"><span class="gh">--------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> Allocation: CPU shares will be allocated to instances based on a new<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``quota:cpu_shares_multiplier``</span> flavor extra spec. The value will represent<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  a relative weight, with higher values indicating a greater proportion of<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  CPU resources.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> Example:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    Given the allowed range for CPU shares (1-10,000) and assuming a VM<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    can use up to 100 vCPUs, the following <span class="s">``cpu_shares_multiplier``</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    values may be appropriate:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    Gold: 100, Silver: 50, Bronze: 25.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="ow">Note</span><span class="p">::</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``quota:cpu_shares_multiplier``</span> and <span class="s">``quota:cpu_shares``</span> are two ways of<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  specifying the same constraint. CPU shares as defined via libvirt applies<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  to the domain as a whole rather than per vCPU. As a result, a flavor with<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  10 VCPUs and <span class="s">``quota:cpu_shares=100``</span> would get equal CPU time as a flavor<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  with<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  1 VCPU and <span class="s">``quota:cpu_shares=100``</span> during CPU contention. By expressing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  shares<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  as a multiplier (<span class="s">``quota:cpu_shares_multiplier=100``</span>), the intuitive<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  expectation that a 10-core VM should receive 10 times the CPU resources<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  is maintained under contention. Administrators can choose between using<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``quota:cpu_shares``</span> or <span class="s">``quota:cpu_shares_multiplier``</span> based on their<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  preference. Translation to Placement allocations will be opt-in via<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  configuration for upgrade compatibility.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Performance Guarantee Enforcement via Placement</span>
</span></span><span class="line"><span class="cl"><span class="gh">-----------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>A standardized <span class="s">``VCPU_SHARES``</span> resource class with weighted allocations<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>will be used to enforce performance tiers through Placement:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Automatic Inventory Reporting:**</span> The libvirt driver will automatically:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  - Report <span class="s">``VCPU_SHARES``</span> inventory based on host capabilities.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Calculate capacity as <span class="s">``vcpu_share_multiplier * len(cpu_shared_set)``</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Respect cgroups v2 API limits of [1, 10000] shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> If and only if enabled via a new <span class="s">``report_vcpu_shares``</span> config option.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Configuration options:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">  ..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">ini</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">[libvirt]</span>
</span></span><span class="line"><span class="cl">      <span class="na">report_vcpu_shares</span> <span class="o">=</span> <span class="s">true
</span></span></span><span class="line"><span class="cl"><span class="s">      # Range: 1-10000
</span></span></span><span class="line"><span class="cl"><span class="s">      # min: 1
</span></span></span><span class="line"><span class="cl"><span class="s">      # max: 10000
</span></span></span><span class="line"><span class="cl"><span class="s">      # default: 100
</span></span></span><span class="line"><span class="cl"><span class="s">      vcpu_share_multiplier = 100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="m">*</span> Default multiplier provides granularity for 1% increments and up to 100<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    vCPUs per VM.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> The multiplier scales the number of VCPUs requested by the flavor into<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    VCPU_SHARES units. The total VCPU_SHARES capacity on a host is calculated<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    based on the number of physical cores available for sharing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    (<span class="s">``cpu_shared_set``</span>)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    and the <span class="s">``vcpu_share_multiplier``</span> (e.g., 64 cores * 100 multiplier = 6,400<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    shares capacity).<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> Placement Enforcement: The scheduler ensures the sum of allocated shares<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  never exceeds the host&#39;s advertised capacity. Shares act as consumption<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  coefficients rather than absolute limits, preserving the proportional<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  guarantee model.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> Overall VCPU Coordination: The standard VCPU inventory and its<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``cpu_allocation_ratio``</span> will still be used as the default CPU capacity<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  mechanism. The <span class="s">``cpu_allocation_ratio``</span> will also be applied to the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``VCPU_SHARES``</span> inventory to ensure oversubscription works as expected.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">ResourceProviderWeigher</span>
</span></span><span class="line"><span class="cl"><span class="gh">-----------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The <span class="s">``ResourceProviderWeigher``</span> calculates host weights by analyzing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>provider trees from Placement&#39;s allocation candidates data. This weigher<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>enables capacity-aware scheduling decisions based on both resource<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>availability and trait compatibility. The weigher behavior can be<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>summarized as follows: &#34;select the most boring host&#34; where boring is<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>defined as having the least number of traits or resource classes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Input Data Structure</span>
</span></span><span class="line"><span class="cl"><span class="gh">^^^^^^^^^^^^^^^^^^^^^</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>The weigher processes <span class="s">``provider_summaries``</span> data which represents a<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>forest of resource provider trees. Each tree is encoded as a map with:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">-</span> <span class="s">``resources``</span>: Map of resource class to (capacity, used) tuples<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">-</span> <span class="s">``traits``</span>: Set of trait names<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">-</span> <span class="s">``parent_provider_uuid``</span>: UUID of parent provider (null for root)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">-</span> <span class="s">``root_provider_uuid``</span>: UUID of tree root provider<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Algorithm Implementation</span>
</span></span><span class="line"><span class="cl"><span class="gh">^^^^^^^^^^^^^^^^^^^^^^^^^</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">1.</span> Tree Flattening<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>   <span class="m">*</span> Group providers by root UUID<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>   <span class="m">*</span> Aggregate resource capacities and traits across each tree<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">2.</span> Resource Calculation:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> For each available resource class in the provider tree:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> Sum available capacity across all providers in tree<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> Calculate utilization ratio: <span class="s">``1 - (requested amount of resource
</span></span></span><span class="line"><span class="cl"><span class="s">    class) / (free capacity of resource class)``</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> Calculate the arithmetic mean of these ratios across all available<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    resource classes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"> <span class="m">3.</span> Trait Calculation:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="m">-</span> Collect all traits from tree providers<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>   <span class="m">*</span> Calculate ratio: <span class="s">``(count of requested traits present on provider) /
</span></span></span><span class="line"><span class="cl"><span class="s"></span>(count of all available traits on provider)<span class="s">``
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">4. Weight Composition:
</span></span></span><span class="line"><span class="cl"><span class="s">   - Combine resource and trait scores: ``(resource_score + trait_score) / 2``</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>   <span class="m">-</span> Apply multiplier from scheduler configuration<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Pseudocode Implementation</span>
</span></span><span class="line"><span class="cl"><span class="gh">^^^^^^^^^^^^^^^^^^^^^^^^^</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Here is a pseudocode example illustrating the <span class="nv">`ResourceProviderWeigher`</span> logic:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">python</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_weigh_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">host_state</span><span class="p">,</span> <span class="n">request_spec</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># host_state contains provider_summaries</span>
</span></span><span class="line"><span class="cl">        <span class="n">provider_summaries</span> <span class="o">=</span> <span class="n">host_state</span><span class="o">.</span><span class="n">provider_summaries</span>
</span></span><span class="line"><span class="cl">        <span class="n">requested_resources</span> <span class="o">=</span> <span class="n">request_spec</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;resources&#39;</span><span class="p">,</span> <span class="p">{})</span>
</span></span><span class="line"><span class="cl">        <span class="n">requested_traits</span> <span class="o">=</span> <span class="n">request_spec</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;traits&#39;</span><span class="p">,</span> <span class="nb">set</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">resource_ratios</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">trait_ratios</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. Tree Flattening and Aggregation (Simplified)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># In reality, this involves iterating through the forest and</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># aggregating resources and traits for each root provider tree.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># For pseudocode, assume we have aggregated_resources and</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># aggregated_traits for the current host&#39;s provider tree.</span>
</span></span><span class="line"><span class="cl">        <span class="n">aggregated_resources</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># {resource_class: {&#39;capacity&#39;: X, &#39;used&#39;: Y}}</span>
</span></span><span class="line"><span class="cl">        <span class="n">aggregated_traits</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1"># {trait_name1, trait_name2, ...}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. Resource Calculation</span>
</span></span><span class="line"><span class="cl">        <span class="n">available_resource_classes</span> <span class="o">=</span> <span class="n">aggregated_resources</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">available_resource_classes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Handle case with no available resources</span>
</span></span><span class="line"><span class="cl">            <span class="n">resource_score</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">res_class</span> <span class="ow">in</span> <span class="n">available_resource_classes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">capacity</span> <span class="o">=</span> <span class="n">aggregated_resources</span><span class="p">[</span><span class="n">res_class</span><span class="p">][</span><span class="s1">&#39;capacity&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">used</span> <span class="o">=</span> <span class="n">aggregated_resources</span><span class="p">[</span><span class="n">res_class</span><span class="p">][</span><span class="s1">&#39;used&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">free_capacity</span> <span class="o">=</span> <span class="n">capacity</span> <span class="o">-</span> <span class="n">used</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># Avoid division by zero if free capacity is 0</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">free_capacity</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Use 0 for requested amount if not in the request</span>
</span></span><span class="line"><span class="cl">                    <span class="n">requested_amount</span> <span class="o">=</span> <span class="n">requested_resources</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">res_class</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">utilization_ratio</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">requested_amount</span> <span class="o">/</span> <span class="n">free_capacity</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">resource_ratios</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">utilization_ratio</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># If no free capacity, this resource class contributes 0 to the score</span>
</span></span><span class="line"><span class="cl">                    <span class="n">resource_ratios</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="c1"># Calculate the arithmetic mean of ratios across all</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># available resource classes</span>
</span></span><span class="line"><span class="cl">                    <span class="n">resource_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">resource_ratios</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">resource_ratios</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    \
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">resource_ratios</span> <span class="k">else</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. Trait Calculation</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">aggregated_traits</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">             <span class="c1"># Handle case with no available traits.</span>
</span></span><span class="line"><span class="cl">             <span class="n">trait_score</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">matched_traits_count</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">            <span class="nb">len</span><span class="p">(</span><span class="n">requested_traits</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">aggregated_traits</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">available_traits_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">aggregated_traits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Calculate ratio: (count of requested traits present) /</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># (count of all available traits)</span>
</span></span><span class="line"><span class="cl">            <span class="n">trait_score</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="n">matched_traits_count</span> <span class="o">/</span> <span class="n">available_traits_count</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">available_traits_count</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. Weight Composition</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># The goal is to select the most boring host (least traits/resources).</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># A lower resource_score (closer to 0) means higher utilization</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ratios (closer to 1), which happens when requested amount is close</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># to free capacity. This is not what we want. We want a higher score</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># for less utilized resources. The current formula 1 - (requested /</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># available) gives a higher score for less utilized. This seems</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># correct for the resource part.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># For traits, a higher trait_score means the requested traits and</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># available traits are closely aligned. As a host will never have fewer</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># traits than requested, we want to select a host where the trait_score</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is as close to 1 as possible because this means there are few extra</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># traits that were not requested. We want a higher score as (more</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># boring = less unrequested traits). The current formula (matched</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># traits) / (available traits) gives a higher score for more matched</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># traits.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Combine resource and adjusted trait scores</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># A higher combined score indicates a more &#34;boring&#34; host (less</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># utilized resources, fewer requested traits present).</span>
</span></span><span class="line"><span class="cl">        <span class="n">combined_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">resource_score</span> <span class="o">+</span> <span class="n">trait_score</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Apply multiplier from scheduler configuration.</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight</span> <span class="o">=</span> <span class="n">combined_score</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_multiplier</span><span class="p">(</span><span class="n">host_state</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">weight</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="gh">Configuration</span>
</span></span><span class="line"><span class="cl"><span class="gh">^^^^^^^^^^^^^^</span>
</span></span><span class="line"><span class="cl">Configure via <span class="s">``nova.conf``</span>:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">ini</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">[filter_scheduler]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Weight multiplier (default: 0.0)</span>
</span></span><span class="line"><span class="cl">    <span class="na">resource_provider_weight_multiplier</span> <span class="o">=</span> <span class="s">1.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Resource classes to consider (default: all)</span>
</span></span><span class="line"><span class="cl">    <span class="na">resource_provider_resources</span> <span class="o">=</span> <span class="s">VCPU_SHARES,MEMORY_MB</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Traits to require (default: all)</span>
</span></span><span class="line"><span class="cl">    <span class="na">resource_provider_traits</span> <span class="o">=</span> <span class="s">HW_CPU_X86_AVX2,HW_CPU_X86_SSE</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Example calculation for a VM requesting 2 VCPUs and SSD trait<span class="se">::</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s">  HostA resources: VCPU avail=4</span>
</span></span><span class="line"><span class="cl"><span class="s">  HostA traits: SSD, HW_CPU_X86_AVX2
</span></span></span><span class="line"><span class="cl"><span class="s">  Resource score: 1 - (2/4) = 0.5
</span></span></span><span class="line"><span class="cl"><span class="s">  Trait score: (1/2) = 0.5
</span></span></span><span class="line"><span class="cl"><span class="s">  Final weight: (0.5 + 0.5)/2 = 0.5
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s"></span><span class="gh">Diagrams and Examples</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> Virtual Machine Resource Contention Diagram:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  .. code-block:: bash<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    +-----------------+   +-----------------+   +-----------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>   Gold Instance |   |  Silver Inst 1  |   |  Silver Inst 2  |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>   (1 vCPU)      |   |  (1 vCPU)       |   |  (1 vCPU)       |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>   Shares: 100   |   |  Shares: 50     |   |  Shares: 50     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    +-----------------+   +-----------------+   +-----------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span>                     |                     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            v                     v                     v<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    +-----------------+   +-----------------+   +-----------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>  Bronze Inst 2  |   |  Bronze Inst 3  |   |  Bronze Inst 1  |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>  (1 vCPU)       |   |  (1 vCPU)       |   |  (1 vCPU)       |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="o">|</span>  Shares: 25     |   |  Shares: 25     |   |  Shares: 25     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    +-----------------+   +-----------------+   +-----------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span>                     |                     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            +----------+----------+----------+----------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>                      <span class="o">|</span>                     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>                      v                     v<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            +---------------------------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span>     Aggregate Host CPU Pool     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span> (Managed by CFS based on shares)|<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            +---------------------------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>                      ^     ^     ^     ^<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>                      <span class="o">|</span>     |     |     |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            +---------------------------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span>    Host CPU 1   |    Host CPU 2 |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            <span class="o">|</span>    Host CPU 3   |    Host CPU 4 |<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            +---------------------------------+<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Total Host Capacity: 400 shared units (4 CPUs * 100 units/CPU)<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Total Allocated Shares: 100 (Gold) + 100 (Silver) + 75 (Bronze) = 275 units<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Remaining Capacity: 400 - 275 = 125 units<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">  ..</span> <span class="ow">note</span><span class="p">::</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     Under contention, CPU time is allocated proportionally to shares across all<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     cores.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     At any given time 2 vms can reside on the same core, but the Linux<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     scheduler will<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     allocate cpu time proportionally to the shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">  ..</span> <span class="ow">note</span><span class="p">::</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     Gold (100 sh): ~36.4% of total CPU time<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     Silver (50 sh each): ~18.2% of total CPU time each<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>     Bronze (25 sh each): ~9.1% of total CPU time each<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> Configuration Examples: Provide examples of:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl">  <span class="m">-</span> Flavor extra specs using share multipliers:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    .. code-block:: console<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        # Create flavors with different CPU share multipliers<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor create --vcpus 1 --ram 1024 --disk 10 bronze<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor set bronze --property<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        quota:cpu_shares_multiplier=25.0<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor create --vcpus 1 --ram 2048 --disk 20 silver<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor set silver --property<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        quota:cpu_shares_multiplier=50.0<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor create --vcpus 1 --ram 4096 --disk 40 gold<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        $ openstack flavor set gold --property quota:cpu_shares_multiplier=100.0<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl">  <span class="m">-</span> Runtime configuration via nova.conf:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    .. code-block:: ini<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        [libvirt]<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        report_vcpu_shares = true<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        vcpu_share_multiplier = 100  # Default value, scales physical cores<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>                                     # to shares<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        [compute]<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        cpu_shared_set = 1-112  # Map to physical cores<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Alternatives</span>
</span></span><span class="line"><span class="cl"><span class="gh">------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Reporting VCPU_SHARE via provider.yaml:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Initial approach required manual configuration of provider.yaml files to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  report VCPU_SHARES inventory:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">  ..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">yaml</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">resource_provider</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">compute-node01</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">inventories</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">VCPU_SHARES</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">total</span><span class="p">:</span><span class="w"> </span><span class="m">100000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">allocation_ratio</span><span class="p">:</span><span class="w"> </span><span class="m">1.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>  <span class="gs">**Limitations:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Static configuration unable to adapt to host changes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> No direct correlation with cgroups capacity.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Manual maintenance overhead.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**CPU Service Level via Custom Resource Classes (Original Proposal):**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  The initial approach proposed creating multiple custom resource classes<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  (e.g., <span class="s">``CUSTOM_VCPU_GOLD``</span>, <span class="s">``CUSTOM_VCPU_SILVER``</span>) to represent<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  performance tiers. This required:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl">  <span class="m">-</span> Defining separate resource classes for each service tier<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Complex flavor extra specs combining vCPU and custom resources<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">-</span> Per-host inventory management for each tier.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="gs">**Reason for revision:**</span> Simplified maintenance through standardized<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  resource class while maintaining Placement-based guarantees through<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  share weighting.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Watcher-Based Enforcement (Reactive):**</span> OpenStack Watcher could be<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  used to monitor per-host, per-tier CPU usage and reactively migrate<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  instances to rebalance the load and enforce policies. This would involve<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  developing a custom Watcher strategy that queries Nova/Placement for<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  allocations and potentially Telemetry for actual CPU utilization.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Pros:**</span> Can provide ongoing compliance and potentially optimize<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  placement based on real-time performance metrics. Can act as a safety<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  net if initial placement is not strictly enforced.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Cons:**</span> Reactive approach means potential for temporary SLO breaches<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  until Watcher acts. Relies on accurate telemetry data. Adds complexity<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  with an additional service to deploy and manage.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Reason for not choosing as primary:**</span> While Watcher can complement<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    the Nova scheduler approach by providing ongoing monitoring and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    optimization, it is not ideal for strict deterministic enforcement<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    at the time of placement. The Placement-based approach prevents<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    violations from occurring in the first place.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Static Host Aggregates:**</span> Using host aggregates to group hosts by<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  intended service tier capacity and relying on aggregate-specific<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  scheduler filters.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Pros:**</span> Relatively simple to implement for basic tiering.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Cons:**</span> Less flexible than Placement custom resources for dynamic<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>inventory and fine-grained control. Does not easily support mixing<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>tiers on the same host with guaranteed per-tier capacity limits.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Requires manual management of host aggregate membership.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Percentage-Based Allocation:**</span> Allowing users to request a percentage<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  of host CPU resources instead of raw shares.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Pros:**</span> Potentially more intuitive for users.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Cons:**</span> Translating a percentage request to a consistent share value<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  across hosts with varying total CPU capacity can be complex. May not<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  align directly with the underlying cgroup share mechanism which is<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  relative.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Security impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Notifications impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">--------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Other end user impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> The Horizon dashboard has limited support for displaying inventories<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  of resource classes to administrators. While it could be extended to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  display <span class="s">``VCPU_SHARES``</span>, that is not currently planned and is out of<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  scope. Administrators will be able to query this information via the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  existing OpenStack client, so no end user impact is expected.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Performance Impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> The overhead of cgroup management by the compute driver should be<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  minimal, as it leverages existing kernel features and libvirt<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>functionality. As a result, it effectively amounts to reporting<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>inventories of <span class="s">``VCPU_SHARES``</span>, weighing hosts based on the available<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>resource classes in the provider summaries and generating the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>appropriate libvirt xml to have it enforce cpu shares using the Linux<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>scheduler.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> The primary performance impact will be on instance performance based on<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  their allocated shares and the overall host load. Instances with higher<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  shares are expected to receive a proportionally larger amount of CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  time under contention.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> The scheduler&#39;s interaction with Placement to check resource inventory<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  should have minimal performance impact, as Placement is designed for<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  scalable resource tracking. However, the overhead of the new<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="s">``ResourceProviderWeigher``</span> will impact scheduler performance. To<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  mitigate this impact, it will be disabled by default via a default<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  multiplier of <span class="s">``0.0``</span>.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Other deployer impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Developer impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">----------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Data model impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">-----------------</span>
</span></span><span class="line"><span class="cl">None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>while the Hoststate object in the scheudler will need to be extened<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>to provide access to the Provider Summeries. That is an internal<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>change and does not affect the overall datamodel.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">REST API impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>None<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Upgrade impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">--------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> Careful consideration is needed for rolling upgrades to ensure<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  compatibility with existing instances.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> Following the pattern established by the pci in Placement feature, a<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  new config option to report CPU shares in Placement will be added to<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  the libvirt section and a corresponding config option to request CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  shares will be added to the filter scheduler section.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p"> ..</span> <span class="ow">code-block</span><span class="p">::</span> <span class="k">ini</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="k">[libvirt]</span>
</span></span><span class="line"><span class="cl">   <span class="na">report_vcpu_shares</span> <span class="o">=</span> <span class="s">true|false  # Default: false
</span></span></span><span class="line"><span class="cl"><span class="s">   vcpu_share_multiplier = 100  # Default value</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="k">[filter_scheduler]</span>
</span></span><span class="line"><span class="cl">   <span class="na">query_placement_for_vcpu_shares</span> <span class="o">=</span> <span class="s">True | False (Default)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> As with pci in Placement when share reporting is enabled it should not be<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> disabled again. When activated, on startup, the nova compute agent will need<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> to reshape existing allocations for instances in Placement to request CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> shares if they use either the <span class="s">``quota:cpu_shares``</span> or<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> <span class="s">``quota:cpu_shares_multiplier``</span> flavor extra specs.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Implementation</span>
</span></span><span class="line"><span class="cl"><span class="gh">==============</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Assignee(s)</span>
</span></span><span class="line"><span class="cl"><span class="gh">-----------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Primary assignee:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  sean-k-mooney<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Other contributors:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  deepseek-r1<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  gemini-flash-2.5<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Feature Liaison</span>
</span></span><span class="line"><span class="cl"><span class="gh">---------------</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Feature liaison:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  sean-k-mooney<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Work Items</span>
</span></span><span class="line"><span class="cl"><span class="gh">----------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">1.</span> Implement <span class="s">``VCPU_SHARES``</span> resource class support in Placement.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">2.</span> Add libvirt driver integration for automatic inventory reporting.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">3.</span> Develop <span class="s">``ResourceProviderWeigher``</span> scheduler component.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">4.</span> Create new flavor extra spec validation logic.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">5.</span> Implement configuration options for upgrade compatibility.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">6.</span> Add documentation and release notes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Dependencies</span>
</span></span><span class="line"><span class="cl"><span class="gh">============</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> A new resource class needs to be added to os-resource classes.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Testing</span>
</span></span><span class="line"><span class="cl"><span class="gh">=======</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>Comprehensive testing is required to ensure the feature functions correctly and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>provides the expected performance guarantees.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Unit Tests:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  Unit tests will be provided to cover the basic operations of the weigher,<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  xml generation and other relevant components.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> <span class="gs">**Functional Tests:**</span> Verify the interaction between different Nova<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  components (API, scheduler, compute) and the hypervisor/Placement.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  New functional tests will be required to validate the reshape logic.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  They can also be used to assert that shares are reported correctly<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> to Placement and that the translation mechanism from flavor extra spec<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span> to allocations works as expected.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="gs">**Tempest Tests:**</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  In general, as this primarily builds on standard Placement logic, there<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  is not much additional Tempest logic required. With that said, it<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  should be possible to add a Tempest test that creates a flavor that<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  uses the new flavor extra specs and assert that the instance allocations<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  contain the relevant VCPU_SHARES. Alternatively, we can tweak the<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  flavors in one of our existing jobs to use this feature for all tests.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">Documentation Impact</span>
</span></span><span class="line"><span class="cl"><span class="gh">====================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> Update the Nova API reference to document the changes to the server<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  creation, resize, and show details APIs, including the new <span class="s">``cpu_shares``</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  parameter and its behavior.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> Update the Nova administrator guide to explain how to configure compute<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  nodes to expose custom tier-specific VCPU resources to Placement via<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  provider config files, how to define flavors with appropriate extra<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  specs, and how to configure related policy rules.<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="m">*</span> Update the Nova user guide to explain how users can request CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  performance service levels by selecting flavors with specific CPU<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  shares or by specifying the <span class="s">``cpu_shares``</span> parameter directly (if<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  allowed by policy).<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">References</span>
</span></span><span class="line"><span class="cl"><span class="gh">==========</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">*</span> <span class="s">`OpenStack cgroup tiering README </span><span class="si">&lt;https://github.com/gprocunier/openstack-cgroup-tiering/blob/b205e34c0c62fa6cd451a44659c648595f0d19ae/README.md&gt;</span><span class="s">`_</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="gh">History</span>
</span></span><span class="line"><span class="cl"><span class="gh">=======</span>
</span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="p">..</span> <span class="ow">list-table</span><span class="p">::</span> Revisions<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>   <span class="nc">:header-rows:</span> <span class="nf">1</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>
</span></span><span class="line"><span class="cl">  <span class="m">*</span> - Release Name<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="m">-</span> Description<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  <span class="m">*</span> - 2025.2 Flamingo<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    - Introduced CPU performance service levels using CPU shares and<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      Placement standardized resource classes.<span class="err">
</span></span></span></code></pre></div><h3 id="summary-of-v0---v7-evolution">Summary of v0 -&gt; v7 Evolution<a hidden class="anchor" aria-hidden="true" href="#summary-of-v0---v7-evolution">#</a></h3>
<p>For those that dont want to read the spec here is an ai generated
summary of what changed.</p>
<p>The specification evolved significantly through seven iterations,
transforming from a basic concept to a comprehensive technical design.</p>
<h4 id="title-and-scope-changes">Title and Scope Changes<a hidden class="anchor" aria-hidden="true" href="#title-and-scope-changes">#</a></h4>
<p><strong>v0:</strong> &ldquo;CPU Performance Service Levels using CPU Shares&rdquo;
<strong>v7:</strong> &ldquo;CPU Performance Service Levels using CPU Shares and Placement Resource Classes&rdquo;</p>
<p>The scope expanded to include full OpenStack Placement service integration.</p>
<h4 id="major-technical-changes">Major Technical Changes<a hidden class="anchor" aria-hidden="true" href="#major-technical-changes">#</a></h4>
<p><strong>Resource Management Approach:</strong></p>
<ul>
<li><strong>v0</strong>: Direct API parameters (<code>cpu_shares</code>) with basic cgroups enforcement</li>
<li><strong>v7</strong>: Flavor-driven approach using <code>quota:cpu_shares_multiplier</code> with <code>VCPU_SHARES</code> resource class and Placement integration</li>
</ul>
<p><strong>Scheduler Integration:</strong></p>
<ul>
<li><strong>v0</strong>: Vague mention of &ldquo;a new scheduler filter or weigher might be required&rdquo;</li>
<li><strong>v7</strong>: Detailed <code>ResourceProviderWeigher</code> specification with complete algorithm, pseudocode, and &ldquo;most boring host&rdquo; selection strategy</li>
</ul>
<p><strong>User Interface:</strong></p>
<ul>
<li><strong>v0</strong>: New API parameters for direct user control</li>
<li><strong>v7</strong>: No API changes - flavor-based configuration with administrative controls</li>
</ul>
<h4 id="new-sections-added">New Sections Added<a hidden class="anchor" aria-hidden="true" href="#new-sections-added">#</a></h4>
<p>The final version includes several sections absent from v0:</p>
<ul>
<li><strong>Definitions</strong>: Comprehensive technical glossary</li>
<li><strong>ResourceProviderWeigher</strong>: Complete algorithm specification with pseudocode</li>
<li><strong>Configuration Examples</strong>: Practical deployment guidance</li>
<li><strong>Detailed Diagrams</strong>: ASCII art showing resource contention scenarios</li>
<li><strong>Comprehensive Alternatives</strong>: Analysis of rejected approaches with reasoning</li>
<li><strong>Implementation Details</strong>: Specific work items and dependencies</li>
</ul>
<h4 id="technical-depth-improvements">Technical Depth Improvements<a hidden class="anchor" aria-hidden="true" href="#technical-depth-improvements">#</a></h4>
<p><strong>v0 Specification:</strong></p>
<ul>
<li>Basic problem statement and use cases</li>
<li>High-level implementation concepts</li>
<li>Placeholders for key sections</li>
<li>Simple API-centric approach</li>
</ul>
<p><strong>v7 Specification:</strong></p>
<ul>
<li>Detailed technical definitions and algorithms</li>
<li>Complete configuration management strategy</li>
<li>Comprehensive upgrade compatibility mechanisms</li>
<li>Production-ready implementation plan</li>
<li>Mathematical formulations and example calculations</li>
</ul>
<h4 id="architectural-sophistication">Architectural Sophistication<a hidden class="anchor" aria-hidden="true" href="#architectural-sophistication">#</a></h4>
<p>The final version demonstrates significantly more architectural sophistication:</p>
<ul>
<li>Multi-layered enforcement (Placement + CFS scheduler)</li>
<li>Integration with existing OpenStack patterns (flavor validation framework)</li>
<li>Configuration-driven feature activation for upgrade compatibility</li>
<li>Resource provider tree processing algorithms</li>
<li>Sophisticated weighing mechanisms for intelligent host selection</li>
</ul>
<p>The evolution represents a transformation from conceptual proposal to implementation-ready specification
with comprehensive technical detail and production deployment considerations.</p>
<p>Sean: ^ while not wrong AIs can be a bit of a suck up.</p>
<h3 id="the-evolution-from-v0-to-v7">The Evolution from v0 to v7<a hidden class="anchor" aria-hidden="true" href="#the-evolution-from-v0-to-v7">#</a></h3>
<p>Right, so back to the actual work - what prompts did I use to get from the AI&rsquo;s first rough attempt to something that didn&rsquo;t make me cringe when I read it?</p>
<p>I started with something relatively simple, because if there&rsquo;s one thing I&rsquo;ve learned about working with AI, it&rsquo;s that you don&rsquo;t dump your entire design vision on it at once.</p>
<pre tabindex="0"><code>the first change i woudl like to make is summerise the current proposal as a
short paragaph in the Alternatives
seation with a title of &#34;cpu service level via custom resources classes&#34;
after that we will work togehter to rework the spec to use a single
standard resouces class called &#34;VCPU_SHARES&#34;
</code></pre><p>In other words, I started by summarising the initial spec as an alternative
and then made the first big change: using a single resource class instead of 1 per tier.</p>
<p>This is one of the core simplification needed to make this easy to use.</p>
<p>next i got a little more ambiguous and the next imporvement i identifed was  to add automatic reporting instead of using provider.yaml</p>
<pre tabindex="0"><code>Summarize the currnt proposal to report available inventories in the
Alternatives section using the `provider.yaml` file, titled &#34;Reporting
VCPU_SHARE via provider.yaml.&#34;
Then update the proposal to state that the libvirt driver will automatically
report a `VCPU_SHARES` inventory on the root resource provider for each
hypervisor.
The allowed range for CPU shares in the cgroups v2 API is [1, 10000]. A new
`vcpu_share_multiplier` configuration option will be added, with a minimum value
of 1 and a maximum value of 10000, the default will be 1000.
Reporting of `VCPU_SHARES` will be controlled by a `report_vcpu_shares` boolean
option in the libvirt section of the `nova.conf`.
 When enabled, the libvirt driver will report a quantity of
 `vcpu_share_multiplier * count(cpu_share_set)` as `VCPU_SHARES`.
</code></pre><p>so at this point the spec describes the problem and use case and
introduces a new standard resource class and describes updating the libvirt
driver to report that automatically.</p>
<p>next step schduling.</p>
<pre tabindex="0"><code>Document the design and functionality of the `ResourceProviderWeigher` in
@/nova-specs/specs/cpu-performance-service-level.rst. The
`ResourceProviderWeigher` calculates a weight by comparing instance resource and
trait requests to placement provider summaries.

Enhance the `HostState` object in @/nova/nova/scheduler/host_manager.py with the
provider summary dictionary. This dictionary is obtained from the placement API
(see @/placement/api-ref/source/allocation_candidates.inc and
@/placement/api-ref/source/samples/allocation_candidates/ for examples) when
Nova requests allocation candidates.

The weighting algorithm should adhere to these guidelines:

1. **Input:** The algorithm takes the `provider_summaries` data structure, which
represents a forest of trees encoded as a map. Node relationships are defined by
`parent_provider_uuid` (with the root having `null`) and `root_provider_uuid`.
Resource providers are modeled as a mapping of UUIDs to resource class
inventories (capacity and usage).

2. **Resource Aggregation:** Transform the tree into a map of resource class to
free capacity (capacity - used), aggregating capacity and usage for identical
resource classes across multiple providers within the tree.

3. **Trait Aggregation:** Create a flattened set of all traits reported by
resource providers in a given tree, indexed by the root provider UUID.

4. **Weight Calculation:** Based on requested resources and traits, produce a
weight between 0 and 1.

5. **Weighting Formula:** For each available resource, calculate the arithmetic
mean of `(1 - (requested amount of resource class) / (free capacity of resource
class))` across all resource classes in the provider tree. Add to this the count
of requested traits divided by available traits. Normalize the result by
dividing by 2.

Reference existing Nova weighers (e.g., @/nova/nova/scheduler/weights/) for
implementation guidance, especially the `MetricsWeigher` and
`ImagePropertiesWeigher` for configuration options.

The `ResourceProviderWeigher` should support the following configurable options:

*   `ResourceProviderWeigher` multiplier
*   List of resource classes to use for weighing (default: all)
*   List of traits to include when weighing (default: all)
</code></pre><p>This is getting even more ambitious.
so there is a lot to take in with this prompt
the main element of note are that i provided it the paths to documentation
for the placement allcoation_canddiates api and the api samples</p>
<p>I also named the relevant Classes in the nova scheduler <code>HostState</code>
and provided the host_manager.py as context.</p>
<p>With that context i described how i wanted the <code>ResourceProviderWeigher</code>
to work.</p>
<p>This got pretty close but needed several refinements to actually express my
intended. this and the prior prompts were the largest changes to the spec and it was here that i took the time to make most of my manual changes.</p>
<p>Form this point my prompts were more polishing rather then large changes.</p>
<pre tabindex="0"><code>Review @/nova-specs/specs/cpu-performance-service-level.rst for consistency in
style, information presentation, and section content, using
@/nova-specs/specs/train/implemented/cpu-resources.rst as a detailed example.
</code></pre><p>the cpu-resouces spec is one of our most complex specs and as a result one
of the better written.</p>
<pre tabindex="0"><code>can you add a short introduction to
@/nova-specs/specs/cpu-performance-service-level.rst describing how the linux
CFS scheduler works in the context fo cpu_shares and how vms VCPU kvm threads
are schdule to host cores and how time is allcoated to each process.
</code></pre><h2 id="results-and-evaluation">Results and Evaluation<a hidden class="anchor" aria-hidden="true" href="#results-and-evaluation">#</a></h2>
<h3 id="the-punch-line">The Punch Line<a hidden class="anchor" aria-hidden="true" href="#the-punch-line">#</a></h3>
<p>Did Ai make faster or easier to write the spec&hellip; No not with this setup
but it did result in a higher quality spec with most if not all of the spelling issues address before humans other then my self had to read them.</p>
<p>The time spent setting up and debugging this setup especially the time spent
trying to use local models with the limited ram availability negated any speed up that could otherwise have been achieved.</p>
<p>If or when i do this again i think i will alter the agentic agent
choice to either use aider or claud_code. roo could also be very effective
if i choose a more powerful LLM that better supports roo complex system prompt and tool calling conventions.</p>
<h3 id="the-final-result">The Final Result<a hidden class="anchor" aria-hidden="true" href="#the-final-result">#</a></h3>
<p>After about a week of iterative work, spread out as an hour or two in the
evening we produced a comprehensive specification that includes:</p>
<ul>
<li><strong>Complete problem definition</strong> with concrete use cases</li>
<li><strong>Detailed technical architecture</strong> using Placement resource classes</li>
<li><strong>Implementation plan</strong> broken down by component (scheduler, libvirt driver, API)</li>
<li><strong>Configuration options</strong> with sensible defaults</li>
<li><strong>Testing strategy</strong> including functional and integration tests</li>
<li><strong>Documentation requirements</strong> for operators and users</li>
<li><strong>Alternative approaches</strong> and why they were rejected</li>
</ul>
<p>The specification ended up being about 700 lines and a little over
3500 words and covers all the sections you&rsquo;d expect in a Nova spec.</p>
<h2 id="what-worked-well">What Worked Well<a hidden class="anchor" aria-hidden="true" href="#what-worked-well">#</a></h2>
<h3 id="design-iteration-and-refinement">Design Iteration and Refinement<a hidden class="anchor" aria-hidden="true" href="#design-iteration-and-refinement">#</a></h3>
<p>Iteration works well when working with LLMs and AI agents.
With enough context and prompting you can get it to do some impressive refactoring but it still need your guidance. If you provide it with
high quality examples it can provide high quality output.</p>
<h3 id="technical-writing-and-clarity">Technical Writing and Clarity<a hidden class="anchor" aria-hidden="true" href="#technical-writing-and-clarity">#</a></h3>
<p>The AI was great at helping structure complex technical concepts in a way that would be accessible to reviewers who might not be deeply familiar with cgroups or the placement service internals.</p>
<p>An example of this is the ascii diagram of the vm vcpu to host cpus mappings.</p>
<p>In the past i have drawn them using <a href="https://asciiflow.com/#/">https://asciiflow.com/#/</a>
which is great but i think gemini did a great job.
inially i used deepseek but gemini i think catured it better.</p>
<h2 id="what-was-challenging">What Was Challenging<a hidden class="anchor" aria-hidden="true" href="#what-was-challenging">#</a></h2>
<h3 id="domain-expertise-limitations">Domain Expertise Limitations<a hidden class="anchor" aria-hidden="true" href="#domain-expertise-limitations">#</a></h3>
<p>Here&rsquo;s the thing - while the AI knows a lot about OpenStack in general,
it doesn&rsquo;t have the kind of deep institutional knowledge you get from years
of actually working on Nova. I found myself constantly having to correct
assumptions about how features actually work, or provide context about why
we made certain design decisions years ago.</p>
<p>For example, the AI initially suggested approaches that would have
completely broken CPU pinning, or didn&rsquo;t account for the fact that the
libvirt driver has some&hellip; let&rsquo;s call them &ldquo;quirks&rdquo; in how it handles
certain configurations.</p>
<h3 id="community-and-process-knowledge">Community and Process Knowledge<a hidden class="anchor" aria-hidden="true" href="#community-and-process-knowledge">#</a></h3>
<p>The AI doesn&rsquo;t understand the social aspects of the OpenStack development
process - what kinds of changes are likely to be controversial, which core
reviewers have strong opinions about certain areas, or how to phrase things
to avoid bikeshedding in reviews.</p>
<p>It also doesn&rsquo;t know about informal agreements or &ldquo;soft&rdquo; rules that aren&rsquo;t
documented anywhere but are understood by the community. You know, the kind
of things you learn from being in IRC discussions at 2am when someone&rsquo;s
trying to debug a weird corner case.</p>
<p>Upgrade impact is very important to nova.
we avoid designs that require db changes or api changes when they are not
strictly required. That does not mean we do not add new apis or db tables
but we treat such changes as a high bar.</p>
<p>In Nova specifically, database schema changes require all the nova
controller services (API, scheduler, conductor) that have access to the
database to be upgraded simultaneously. This makes all the compute agents
and the workloads that reside on them unmanageable for the duration of the
controller upgrades. API changes have backward compatibility implications
that can affect users for years, and must be supported across multiple
OpenStack releases.</p>
<p>The original approach the AI suggested had a new db column to store the
cpu_shares and allowed end users to request them directly. While neither of
these are technically required to achieve the goal, the AI lacked the
context to know that those are expensive things to do in something like
nova, whereas db changes and api changes may be relatively cheap in a
typical web application.</p>
<h3 id="restructured-text">Restructured text<a hidden class="anchor" aria-hidden="true" href="#restructured-text">#</a></h3>
<p>Here&rsquo;s something that caught me completely off guard, though it&rsquo;s obvious in hindsight.</p>
<p>RestructuredText uses <code>-------</code> everywhere - literally under every heading. You know what else uses <code>-------</code>? Git diffs.</p>
<p>So when I was working within a single section, everything was fine. But as
soon as I tried to make changes across larger parts of the spec, the AI
would get confused by all the dashes and start treating section dividers as
diff markers. Tool call failures everywhere, and poor Roo couldn&rsquo;t figure
out what I actually wanted it to change.</p>
<p>This is partly down to the RST format but more a limitation of both Roo
and the models I selected: <code>google-gemini-flash-2.5</code> and <code>deepseek/deepseek-r1:free</code>.
Gemini is better at applying diffs and tool calls than deepseek but
neither are really trained for this. If I used a model that was specifically trained for this
it would have gone more smoothly.</p>
<p>aider and claud_code or other tools like  zed, windsurf or cursor
may do better.</p>
<h2 id="lessons-learned">Lessons Learned<a hidden class="anchor" aria-hidden="true" href="#lessons-learned">#</a></h2>
<h3 id="ai-as-a-collaborative-partner">AI as a Collaborative Partner<a hidden class="anchor" aria-hidden="true" href="#ai-as-a-collaborative-partner">#</a></h3>
<p>The most effective approach wasn&rsquo;t having the AI write the spec for me, but rather using it as a collaborative partner.</p>
<p>The AI was excellent at:</p>
<ul>
<li>Helping structure and organize complex information</li>
<li>generating summaries and diagrams</li>
<li>comparing refernce information to determine how to improve the existing
proposal to incorporate that context.</li>
</ul>
<h3 id="domain-expertise-still-critical">Domain Expertise Still Critical<a hidden class="anchor" aria-hidden="true" href="#domain-expertise-still-critical">#</a></h3>
<p>My years of experience with Nova and OpenStack were essential for:</p>
<ul>
<li>Knowing what&rsquo;s actually implementable vs. what sounds good on paper</li>
<li>Understanding the community and review process</li>
<li>Catching subtle interactions with existing features</li>
<li>Making pragmatic trade-offs between ideal solutions and mergeable code</li>
</ul>
<h3 id="iteration-and-refinement">Iteration and Refinement<a hidden class="anchor" aria-hidden="true" href="#iteration-and-refinement">#</a></h3>
<p>The best results came from multiple rounds of iteration. The first draft
was pretty rough, but each iteration I ask for specific problems to be
addressed and incrementally made the specification significantly better.</p>
<h2 id="would-i-do-this-again">Would I Do This Again?<a hidden class="anchor" aria-hidden="true" href="#would-i-do-this-again">#</a></h2>
<p>This is an interesting question.</p>
<p>As i alluded to before if my goal was to get things down faster then
i don&rsquo;t think that was a success. fortunately that was not my goal
my goal was to provide a higher quality initial version of a spec for
upstream review.</p>
<p>i wanted to try and mitigate some of the mental load of written
communications and mask the impacts of my dyslexia. That goal was
definitely achieved.</p>
<p>Was it easy?  no this still took a lot of effort to drive.
would i do it again with ai. yes on balance i think i would.</p>
<h2 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h2>
<p>The specification is now ready for community <a href="https://review.opendev.org/c/openstack/nova-specs/&#43;/951222">review</a>.
I am curious to see how the community responds to an AI-assisted
specification. Will reviewers be able to tell? Will the quality be
noticeably different?</p>
<p>I&rsquo;m also planning to use this approach for other specifications I&rsquo;ve been
putting off due to time constraints. If the community review goes well, I
might write up some guidelines for others who want to experiment with
AI-assisted spec writing once i gain more experience with it.</p>
<p>For now I have created <a href="https://github.com/SeanMooney/openstack-ai-style-guide">https://github.com/SeanMooney/openstack-ai-style-guide</a>
to help with future AI usage in OpenStack.</p>
<h2 id="implications-for-the-openstack-community">Implications for the OpenStack Community<a hidden class="anchor" aria-hidden="true" href="#implications-for-the-openstack-community">#</a></h2>
<p>This experiment raises some interesting questions for our community:</p>
<h3 id="accelerating-innovation">Accelerating Innovation<a hidden class="anchor" aria-hidden="true" href="#accelerating-innovation">#</a></h3>
<p>If AI can help experienced contributors produce higher quality specifications faster, can it help use implement them.</p>
<p>Eventlet removal is a very complex undertaking.
open api schema definition are another community goal with a lot
of detailed work. Could AI help achieve those goals faster?</p>
<h3 id="democratizing-contribution">Democratizing Contribution<a hidden class="anchor" aria-hidden="true" href="#democratizing-contribution">#</a></h3>
<p>Could AI assistance help newer contributors write better specifications by helping them understand OpenStack architecture patterns and community conventions?</p>
<p>What about those with learning disabilities like dyslexia?
visual disabilities via enhanced TTS and voice interaction?</p>
<h3 id="review-process-evolution">Review Process Evolution<a hidden class="anchor" aria-hidden="true" href="#review-process-evolution">#</a></h3>
<p>How should reviewers approach AI-assisted specifications?
What new review criteria might we need to consider?</p>
<h3 id="knowledge-preservation">Knowledge Preservation<a hidden class="anchor" aria-hidden="true" href="#knowledge-preservation">#</a></h3>
<p>One unexpected benefit was that working with the AI to explain OpenStack
concepts helped me articulate institutional knowledge that might not be
well documented elsewhere. Could AI assistance be a tool for better
knowledge transfer?</p>
<h2 id="final-thoughts">Final Thoughts<a hidden class="anchor" aria-hidden="true" href="#final-thoughts">#</a></h2>
<p>This experiment convinced me that AI can be a powerful tool for OpenStack
development when used thoughtfully. It&rsquo;s not a replacement for human
expertise, but it can be guided by it - helping experienced contributors
work more efficiently and potentially helping newer contributors learn
faster.</p>
<p>What started as a personal accessibility challenge became something bigger:
a proof of concept that AI-assisted technical writing can produce genuinely
useful results. The specification that emerged from this process isn&rsquo;t just
&ldquo;good enough&rdquo; - it&rsquo;s comprehensive, technically sound, and ready for
serious community review.</p>
<p>But perhaps more importantly, this experiment points toward a future where
the barriers to contributing high-quality technical documentation are
significantly lower. If AI can help someone with severe dyslexia produce a
700-line Nova specification, imagine what it could do for brilliant
engineers who struggle with English as a second language, or newcomers who
understand the technology but don&rsquo;t yet know the community conventions.</p>
<p>The success here wasn&rsquo;t in replacing human judgment - it was in augmenting
human expertise. Every architectural decision, every technical trade-off,
every understanding of community dynamics came from years of OpenStack
experience. The AI simply helped translate that knowledge into clear,
well-structured prose.</p>
<p>I&rsquo;m excited to see how this specification is received by the community and
whether others will experiment with similar approaches. More importantly,
I&rsquo;m curious to see if this opens doors for contributions from voices we
might not have heard otherwise.</p>
<p>The real test isn&rsquo;t whether the AI wrote a good spec - it&rsquo;s whether this
approach can help more people contribute to the technical discussions that
shape OpenStack&rsquo;s future.</p>
<hr>
<p><em>The specification is now under <a href="https://review.opendev.org/c/openstack/nova-specs/&#43;/951222">community review</a>. I&rsquo;m interested in feedback on both the specification itself and the AI-assisted development process. Feel free to reach out on IRC (sean-k-mooney) or through the usual OpenStack channels.</em></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.seanmooney.info/tags/openstack/">Openstack</a></li>
      <li><a href="https://www.seanmooney.info/tags/nova/">Nova</a></li>
      <li><a href="https://www.seanmooney.info/tags/llms/">LLMs</a></li>
      <li><a href="https://www.seanmooney.info/tags/specifications/">Specifications</a></li>
      <li><a href="https://www.seanmooney.info/tags/development/">Development</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.seanmooney.info/blog/2025.2-ptg/">
    <span class="title">« Prev</span>
    <br>
    <span>2025.2 Ptg</span>
  </a>
  <a class="next" href="https://www.seanmooney.info/blog/what-is-openstack/">
    <span class="title">Next »</span>
    <br>
    <span>what is openstack</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on x"
            href="https://x.com/intent/tweet/?text=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f&amp;hashtags=openstack%2cnova%2cLLMs%2cspecifications%2cdevelopment">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f&amp;title=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance&amp;summary=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance&amp;source=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f&title=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on whatsapp"
            href="https://api.whatsapp.com/send?text=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance%20-%20https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on telegram"
            href="https://telegram.me/share/url?text=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance&amp;url=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Auto-Correcting Sean-Speak: Beyond Spell-Check to Real AI Assistance on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Auto-Correcting%20Sean-Speak%3a%20Beyond%20Spell-Check%20to%20Real%20AI%20Assistance&u=https%3a%2f%2fwww.seanmooney.info%2fblog%2fai-to-spec%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© Sean Mooney</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
